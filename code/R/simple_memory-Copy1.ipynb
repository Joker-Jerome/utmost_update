{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: Matrix\n",
      "\n",
      "Loading required package: foreach\n",
      "\n",
      "Loaded glmnet 2.0-13\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: loading expression files...INFO: loading genotype files..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading: /ysm-gpfs/project/wl382/GTEx_v8/genotype/cis_loc/chr21/ENSG00000184012.11/ENSG00000184012.11.bim\n",
      "\n",
      "Reading: /ysm-gpfs/project/wl382/GTEx_v8/genotype/cis_loc/chr21/ENSG00000184012.11/ENSG00000184012.11.fam\n",
      "\n",
      "Reading: /ysm-gpfs/project/wl382/GTEx_v8/genotype/cis_loc/chr21/ENSG00000184012.11/ENSG00000184012.11.bed\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "INFO: CV preparation"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "INFO: fold 1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "INFO: glmnet cv tissue5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "INFO: glmnet cv tissue10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "INFO: glmnet cv tissue15"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "INFO: glmnet cv tissue20"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "INFO: glmnet cv tissue25"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "INFO: glmnet cv tissue30"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "INFO: glmnet cv tissue35"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "INFO: glmnet cv tissue40"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "INFO: glmnet cv tissue45"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "starting train-validate-test\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "saving cross-validation results for evaluation and analysis\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "INFO: fold 2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "INFO: glmnet cv tissue5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "INFO: glmnet cv tissue10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "INFO: glmnet cv tissue15"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "INFO: glmnet cv tissue20"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "INFO: glmnet cv tissue25"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "INFO: glmnet cv tissue30"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "INFO: glmnet cv tissue35"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "INFO: glmnet cv tissue40"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "INFO: glmnet cv tissue45"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "starting train-validate-test\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "saving cross-validation results for evaluation and analysis\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "INFO: fold 3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "INFO: glmnet cv tissue5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "INFO: glmnet cv tissue10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "INFO: glmnet cv tissue15"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "INFO: glmnet cv tissue20"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "INFO: glmnet cv tissue25"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "INFO: glmnet cv tissue30"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "INFO: glmnet cv tissue35"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "INFO: glmnet cv tissue40"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "INFO: glmnet cv tissue45"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "starting train-validate-test\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "saving cross-validation results for evaluation and analysis\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "INFO: fold 4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "INFO: glmnet cv tissue5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "INFO: glmnet cv tissue10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "INFO: glmnet cv tissue15"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "INFO: glmnet cv tissue20"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "INFO: glmnet cv tissue25"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "INFO: glmnet cv tissue30"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "INFO: glmnet cv tissue35"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "INFO: glmnet cv tissue40"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "INFO: glmnet cv tissue45"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "starting train-validate-test\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "saving cross-validation results for evaluation and analysis\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "INFO: fold 5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "INFO: glmnet cv tissue5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "INFO: glmnet cv tissue10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "INFO: glmnet cv tissue15"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "INFO: glmnet cv tissue20"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "INFO: glmnet cv tissue25"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "INFO: glmnet cv tissue30"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "INFO: glmnet cv tissue35"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "INFO: glmnet cv tissue40"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "INFO: glmnet cv tissue45"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "starting train-validate-test\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "saving cross-validation results for evaluation and analysis\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error:  119.3398 114.6329 1.708057 15.75679 8.412752 115.7258 21.11208 26.43508 3.709641 0.6076277 0.8083031 2.444312 5.359959 4.389012 7.689671 1.052159 1.850138 1.229541 0.289239 169.1105 5.215024 95.01199 12.99727 2.534415 7.078416 135.454 115.3448 4.687944 2.494125 27.80487 0.6125086 12.68857 12.7498 1.900796 392.3966 3.403 0.380951 3.358635 2.752936 12.46359 24.51894 1.678472 890.5619 122.3215 0.8900094 82.96384 0.8348241 19.10221 350.6712 \n",
      "Testing error:  30.97815 29.66304 1.890809 5.64455 10.39465 9.712695 54.47921 12.44059 0.3998688 0.2114464 0.1669511 0.5783634 3.451111 4.836893 5.840885 0.4502649 0.3105615 1.506523 1.143664 47.93043 7.111697 111.6167 10.51324 3.090358 3.66992 18.48137 22.08741 2.368471 1.29136 30.81286 0.1855813 10.83344 3.321453 0.3335148 55.28606 5.240068 0.4895775 2.739135 2.560963 3.482242 3.486906 7.41522 120.456 21.82361 0.6109977 12.25211 3.127056 8.031119 223.5297 \n",
      "Training error:  0.2631911 0.2104692 3.615741e-05 0.0007667789 1.363791e-05 0.369849 1.652571e-07 5.109307e-07 2.348908e-06 2.579463e-07 3.875511e-06 1.312007e-05 5.297944e-06 5.716483e-07 6.703783e-07 1.659726e-06 6.898165e-07 4.232278e-07 4.894036e-07 0.04677339 0.000796398 239599.1 0.001690003 0.0005778082 0.00118041 0.1613442 0.03751542 0.001764751 0.0006368035 0.0003088649 1.921338e-06 0.01543964 27476.56 0.01445195 0.6490321 1.783213e-05 1.406404e-05 0.0002149795 1.17513e-06 0.05277776 0.1162366 5.810748e+29 0.0003250108 22733.35 3.814169e-05 0.1748239 7.440677e-05 2.604784e-07 2.614609 \n",
      "Testing error:  175.831 102.0507 11.04173 11.58148 58.41499 167.9759 55.77779 34.42434 5.904897 0.8486583 1.297107 3.349136 6.031756 9.639149 11.91013 1.699605 3.175357 1.993808 1.617026 214.8969 11.72875 6438389396 21.39661 6.541075 6.940057 154.9676 128.7506 5.035892 3.070556 25.08763 0.7857102 20.17074 57454097 2.009027 475.2438 6.713199 0.8430462 7.617983 5.599801 15.98517 27.36684 2.0605e+30 714.5742 5485518402 1.19564 137.4746 3.188112 22.13815 421.5679 \n",
      "total training time:  30.75207 \n",
      "lambda1= 0.01165389 ; lambda2= 0.01165389 ; avg tune err= 4.205102e+28 \n",
      "Training error:  119.3398 114.6329 1.708057 15.75679 8.412752 115.7258 21.11208 26.43508 3.709641 0.6076277 0.8083031 2.444312 5.359959 4.389012 7.689671 1.052159 1.850138 1.229541 0.289239 169.1105 5.215024 95.01199 12.99727 2.534415 7.078416 135.454 115.3448 4.687944 2.494125 27.80487 0.6125086 12.68857 12.7498 1.900796 392.3966 3.403 0.380951 3.358635 2.752936 12.46359 24.51894 1.678472 890.5619 122.3215 0.8900094 82.96384 0.8348241 19.10221 350.6712 \n",
      "Testing error:  30.97815 29.66304 1.890809 5.64455 10.39465 9.712695 54.47921 12.44059 0.3998688 0.2114464 0.1669511 0.5783634 3.451111 4.836893 5.840885 0.4502649 0.3105615 1.506523 1.143664 47.93043 7.111697 111.6167 10.51324 3.090358 3.66992 18.48137 22.08741 2.368471 1.29136 30.81286 0.1855813 10.83344 3.321453 0.3335148 55.28606 5.240068 0.4895775 2.739135 2.560963 3.482242 3.486906 7.41522 120.456 21.82361 0.6109977 12.25211 3.127056 8.031119 223.5297 \n",
      "Training error:  119.3398 114.6329 1.708057 15.75679 8.412752 115.7258 21.11208 26.43508 3.709641 0.6076277 0.8083031 2.444312 5.459433 4.389012 7.689671 1.052159 1.850138 1.229541 0.4338346 169.1105 5.215024 161.7591 12.99727 2.534415 7.078416 135.454 115.3448 4.751531 2.494125 126.9928 0.6125086 13.17567 12.7498 1.900796 392.3966 6.876697 0.3987369 3.358635 2.752936 12.46359 24.51894 6.771038 890.5619 122.3215 1.145183 82.96384 2.826967 19.10221 350.6712 \n",
      "Testing error:  30.97815 29.66304 1.890809 5.64455 10.39465 9.712695 54.47921 12.44059 0.3998688 0.2114464 0.1669511 0.5783634 3.445466 4.836893 5.840885 0.4502649 0.3105615 1.506523 1.155394 47.93043 7.111697 84.79978 10.51324 3.090358 3.66992 18.48137 22.08741 2.368665 1.29136 33.06717 0.1855813 10.82053 3.321453 0.3335148 55.28606 5.729017 0.488382 2.739135 2.560963 3.482242 3.486906 5.3941 120.456 21.82361 0.7863911 12.25211 2.881309 8.031119 223.5297 \n",
      "total training time:  30.7746 \n",
      "Training error:  119.3398 114.6329 1.708057 15.75679 8.412752 115.7258 21.11208 26.43508 3.709641 0.6076277 0.8083031 2.444312 5.359959 4.389012 7.689671 1.052159 1.850138 1.229541 0.289239 169.1105 5.215024 95.01199 12.99727 2.534415 7.078416 135.454 115.3448 4.687944 2.494125 27.80487 0.6125086 12.68857 12.7498 1.900796 392.3966 3.403 0.380951 3.358635 2.752936 12.46359 24.51894 1.678472 890.5619 122.3215 0.8900094 82.96384 0.8348241 19.10221 350.6712 \n",
      "Testing error:  30.97815 29.66304 1.890809 5.64455 10.39465 9.712695 54.47921 12.44059 0.3998688 0.2114464 0.1669511 0.5783634 3.451111 4.836893 5.840885 0.4502649 0.3105615 1.506523 1.143664 47.93043 7.111697 111.6167 10.51324 3.090358 3.66992 18.48137 22.08741 2.368471 1.29136 30.81286 0.1855813 10.83344 3.321453 0.3335148 55.28606 5.240068 0.4895775 2.739135 2.560963 3.482242 3.486906 7.41522 120.456 21.82361 0.6109977 12.25211 3.127056 8.031119 223.5297 \n",
      "Training error:  4.796672 3.872889 0.1808946 0.878645 0.2288312 4.241583 0.1129506 0.1244005 0.1521854 0.1225939 0.1982967 0.2621869 0.1713262 0.2005459 0.178477 0.2149587 0.1786602 0.08670296 0.09590913 1.91569 0.6120564 0.03425291 0.9001961 0.7181406 0.6589213 2.897474 1.965726 0.6949092 0.4785422 0.1744908 0.1709777 1.600781 0.2400615 0.8889411 6.128713 0.2226321 0.2325833 0.4991081 0.2264158 1.771982 2.965248 0.3216893 0.5266085 0.668057 0.315465 3.53799 0.1245408 0.09644168 11.40228 \n",
      "Testing error:  143.2963 92.94887 1.677442 6.83583 17.06865 140.8824 54.22769 25.40264 2.316793 0.2222023 0.1861891 1.114366 3.663288 6.615691 11.77319 0.6512534 1.013753 2.174009 1.223882 136.6307 9.923116 146.7663 14.40398 4.224186 4.636814 123.038 113.8668 3.733776 1.99478 29.15827 0.2464754 15.08283 9.648926 0.633694 429.7397 5.538408 0.697467 3.537632 3.430911 10.19436 18.32572 8.265173 636.4848 82.15427 0.5819985 92.3441 4.310901 9.941174 386.3766 \n",
      "total training time:  30.71931 \n",
      "lambda1= 19.10771 ; lambda2= 0.01165389 ; avg tune err= 57.53482 \n",
      "Training error:  119.3398 114.6329 1.708057 15.75679 8.412752 115.7258 21.11208 26.43508 3.709641 0.6076277 0.8083031 2.444312 5.359959 4.389012 7.689671 1.052159 1.850138 1.229541 0.289239 169.1105 5.215024 95.01199 12.99727 2.534415 7.078416 135.454 115.3448 4.687944 2.494125 27.80487 0.6125086 12.68857 12.7498 1.900796 392.3966 3.403 0.380951 3.358635 2.752936 12.46359 24.51894 1.678472 890.5619 122.3215 0.8900094 82.96384 0.8348241 19.10221 350.6712 \n",
      "Testing error:  30.97815 29.66304 1.890809 5.64455 10.39465 9.712695 54.47921 12.44059 0.3998688 0.2114464 0.1669511 0.5783634 3.451111 4.836893 5.840885 0.4502649 0.3105615 1.506523 1.143664 47.93043 7.111697 111.6167 10.51324 3.090358 3.66992 18.48137 22.08741 2.368471 1.29136 30.81286 0.1855813 10.83344 3.321453 0.3335148 55.28606 5.240068 0.4895775 2.739135 2.560963 3.482242 3.486906 7.41522 120.456 21.82361 0.6109977 12.25211 3.127056 8.031119 223.5297 \n",
      "Training error:  119.3398 114.6329 1.708057 15.75679 8.412752 115.7258 21.11208 26.43508 3.709641 0.6076277 0.8083031 2.444312 5.459433 4.389012 7.689671 1.052159 1.850138 1.229541 0.4338346 169.1105 5.215024 161.7591 12.99727 2.534415 7.078416 135.454 115.3448 4.751531 2.494125 126.9928 0.6125086 13.17567 12.7498 1.900796 392.3966 6.876697 0.3987369 3.358635 2.752936 12.46359 24.51894 6.771038 890.5619 122.3215 1.145183 82.96384 2.826967 19.10221 350.6712 \n",
      "Testing error:  30.97815 29.66304 1.890809 5.64455 10.39465 9.712695 54.47921 12.44059 0.3998688 0.2114464 0.1669511 0.5783634 3.445466 4.836893 5.840885 0.4502649 0.3105615 1.506523 1.155394 47.93043 7.111697 84.79978 10.51324 3.090358 3.66992 18.48137 22.08741 2.368665 1.29136 33.06717 0.1855813 10.82053 3.321453 0.3335148 55.28606 5.729017 0.488382 2.739135 2.560963 3.482242 3.486906 5.3941 120.456 21.82361 0.7863911 12.25211 2.881309 8.031119 223.5297 \n",
      "total training time:  30.78456 \n",
      "Training error:  119.3398 114.6329 1.708057 15.75679 8.412752 115.7258 21.11208 26.43508 3.709641 0.6076277 0.8083031 2.444312 5.359959 4.389012 7.689671 1.052159 1.850138 1.229541 0.289239 169.1105 5.215024 95.01199 12.99727 2.534415 7.078416 135.454 115.3448 4.687944 2.494125 27.80487 0.6125086 12.68857 12.7498 1.900796 392.3966 3.403 0.380951 3.358635 2.752936 12.46359 24.51894 1.678472 890.5619 122.3215 0.8900094 82.96384 0.8348241 19.10221 350.6712 \n",
      "Testing error:  30.97815 29.66304 1.890809 5.64455 10.39465 9.712695 54.47921 12.44059 0.3998688 0.2114464 0.1669511 0.5783634 3.451111 4.836893 5.840885 0.4502649 0.3105615 1.506523 1.143664 47.93043 7.111697 111.6167 10.51324 3.090358 3.66992 18.48137 22.08741 2.368471 1.29136 30.81286 0.1855813 10.83344 3.321453 0.3335148 55.28606 5.240068 0.4895775 2.739135 2.560963 3.482242 3.486906 7.41522 120.456 21.82361 0.6109977 12.25211 3.127056 8.031119 223.5297 \n",
      "Training error:  11.71706 9.75808 0.571236 2.365687 0.8033852 8.676199 0.4053838 0.4607702 0.5573806 0.3453377 0.5414097 0.7388579 0.6608766 0.7315636 0.6043105 0.5102293 0.5132517 0.3129376 0.2860398 4.46724 1.53538 0.07977164 2.267022 1.64564 1.8324 6.958722 4.294449 1.85838 1.237336 0.5318175 0.4862227 4.045512 0.8640228 1.663715 13.07489 0.6793895 0.3835869 1.300607 0.7814192 4.003491 5.842251 0.9461176 1.626916 1.668645 0.6320213 7.053759 0.3754549 0.389149 23.84377 \n",
      "Testing error:  116.1651 87.16481 1.621135 8.041694 15.50093 112.1764 59.33879 19.96059 1.177556 0.1303515 0.1535921 0.9593269 3.402468 5.113672 11.73498 0.4506594 0.4390639 1.791277 1.219526 129.7472 8.458507 124.1389 15.11621 3.155408 4.994764 99.21001 98.04327 2.751797 1.710031 35.44848 0.1873299 12.45572 11.05008 0.3660901 400.7867 5.354953 0.4955549 3.205037 2.850481 6.947202 13.24845 7.285401 500.8939 93.50153 0.53021 74.43716 3.79132 9.265708 370.117 \n",
      "total training time:  30.69827 \n",
      "lambda1= 38.20376 ; lambda2= 0.01165389 ; avg tune err= 50.73646 \n",
      "Training error:  119.3398 114.6329 1.708057 15.75679 8.412752 115.7258 21.11208 26.43508 3.709641 0.6076277 0.8083031 2.444312 5.359959 4.389012 7.689671 1.052159 1.850138 1.229541 0.289239 169.1105 5.215024 95.01199 12.99727 2.534415 7.078416 135.454 115.3448 4.687944 2.494125 27.80487 0.6125086 12.68857 12.7498 1.900796 392.3966 3.403 0.380951 3.358635 2.752936 12.46359 24.51894 1.678472 890.5619 122.3215 0.8900094 82.96384 0.8348241 19.10221 350.6712 \n",
      "Testing error:  30.97815 29.66304 1.890809 5.64455 10.39465 9.712695 54.47921 12.44059 0.3998688 0.2114464 0.1669511 0.5783634 3.451111 4.836893 5.840885 0.4502649 0.3105615 1.506523 1.143664 47.93043 7.111697 111.6167 10.51324 3.090358 3.66992 18.48137 22.08741 2.368471 1.29136 30.81286 0.1855813 10.83344 3.321453 0.3335148 55.28606 5.240068 0.4895775 2.739135 2.560963 3.482242 3.486906 7.41522 120.456 21.82361 0.6109977 12.25211 3.127056 8.031119 223.5297 \n",
      "Training error:  119.3398 114.6329 1.708057 15.75679 8.412752 115.7258 21.11208 26.43508 3.709641 0.6076277 0.8083031 2.444312 5.459433 4.389012 7.689671 1.052159 1.850138 1.229541 0.4338346 169.1105 5.215024 161.7591 12.99727 2.534415 7.078416 135.454 115.3448 4.751531 2.494125 126.9928 0.6125086 13.17567 12.7498 1.900796 392.3966 6.876697 0.3987369 3.358635 2.752936 12.46359 24.51894 6.771038 890.5619 122.3215 1.145183 82.96384 2.826967 19.10221 350.6712 \n",
      "Testing error:  30.97815 29.66304 1.890809 5.64455 10.39465 9.712695 54.47921 12.44059 0.3998688 0.2114464 0.1669511 0.5783634 3.445466 4.836893 5.840885 0.4502649 0.3105615 1.506523 1.155394 47.93043 7.111697 84.79978 10.51324 3.090358 3.66992 18.48137 22.08741 2.368665 1.29136 33.06717 0.1855813 10.82053 3.321453 0.3335148 55.28606 5.729017 0.488382 2.739135 2.560963 3.482242 3.486906 5.3941 120.456 21.82361 0.7863911 12.25211 2.881309 8.031119 223.5297 \n",
      "total training time:  30.65588 \n",
      "Training error:  119.3398 114.6329 1.708057 15.75679 8.412752 115.7258 21.11208 26.43508 3.709641 0.6076277 0.8083031 2.444312 5.359959 4.389012 7.689671 1.052159 1.850138 1.229541 0.289239 169.1105 5.215024 95.01199 12.99727 2.534415 7.078416 135.454 115.3448 4.687944 2.494125 27.80487 0.6125086 12.68857 12.7498 1.900796 392.3966 3.403 0.380951 3.358635 2.752936 12.46359 24.51894 1.678472 890.5619 122.3215 0.8900094 82.96384 0.8348241 19.10221 350.6712 \n",
      "Testing error:  30.97815 29.66304 1.890809 5.64455 10.39465 9.712695 54.47921 12.44059 0.3998688 0.2114464 0.1669511 0.5783634 3.451111 4.836893 5.840885 0.4502649 0.3105615 1.506523 1.143664 47.93043 7.111697 111.6167 10.51324 3.090358 3.66992 18.48137 22.08741 2.368471 1.29136 30.81286 0.1855813 10.83344 3.321453 0.3335148 55.28606 5.240068 0.4895775 2.739135 2.560963 3.482242 3.486906 7.41522 120.456 21.82361 0.6109977 12.25211 3.127056 8.031119 223.5297 \n",
      "Training error:  18.81197 15.07215 1.057919 4.25518 1.503993 13.68142 0.9331311 1.105913 1.158108 0.5994741 0.8083031 1.269549 1.367567 1.359356 1.113864 0.8375246 1.032202 0.6211437 0.4014967 7.496123 2.589948 0.2097834 4.084227 2.346552 3.163706 11.14928 7.06151 2.952007 1.969213 0.9778946 0.6125086 6.478387 1.472084 1.896345 19.81685 1.247732 0.3987369 2.102035 1.407679 6.394907 8.922189 1.94107 3.19819 3.085791 0.8216947 11.17342 0.7838328 0.7810526 37.63276 \n",
      "Testing error:  100.574 84.31435 1.747776 7.556656 14.74733 88.2715 54.31086 19.48606 0.5499238 0.2023706 0.1669511 0.7413788 2.738996 4.882089 9.757329 0.4344896 0.2911388 1.684314 1.167289 119.2925 7.679268 99.53364 13.56925 3.012107 4.568521 87.89081 90.69173 2.64038 1.430005 47.40805 0.1855813 11.96012 9.60666 0.3334127 381.0864 5.459161 0.488382 2.908409 2.238583 5.118546 9.069139 6.644803 426.0741 111.2874 0.5750366 59.58887 3.253625 11.97286 352.7675 \n",
      "total training time:  30.53477 \n",
      "lambda1= 57.29982 ; lambda2= 0.01165389 ; avg tune err= 46.36653 \n",
      "Training error:  119.3398 114.6329 1.708057 15.75679 8.412752 115.7258 21.11208 26.43508 3.709641 0.6076277 0.8083031 2.444312 5.359959 4.389012 7.689671 1.052159 1.850138 1.229541 0.289239 169.1105 5.215024 95.01199 12.99727 2.534415 7.078416 135.454 115.3448 4.687944 2.494125 27.80487 0.6125086 12.68857 12.7498 1.900796 392.3966 3.403 0.380951 3.358635 2.752936 12.46359 24.51894 1.678472 890.5619 122.3215 0.8900094 82.96384 0.8348241 19.10221 350.6712 \n",
      "Testing error:  30.97815 29.66304 1.890809 5.64455 10.39465 9.712695 54.47921 12.44059 0.3998688 0.2114464 0.1669511 0.5783634 3.451111 4.836893 5.840885 0.4502649 0.3105615 1.506523 1.143664 47.93043 7.111697 111.6167 10.51324 3.090358 3.66992 18.48137 22.08741 2.368471 1.29136 30.81286 0.1855813 10.83344 3.321453 0.3335148 55.28606 5.240068 0.4895775 2.739135 2.560963 3.482242 3.486906 7.41522 120.456 21.82361 0.6109977 12.25211 3.127056 8.031119 223.5297 \n",
      "Training error:  119.3398 114.6329 1.708057 15.75679 8.412752 115.7258 21.11208 26.43508 3.709641 0.6076277 0.8083031 2.444312 5.459433 4.389012 7.689671 1.052159 1.850138 1.229541 0.4338346 169.1105 5.215024 161.7591 12.99727 2.534415 7.078416 135.454 115.3448 4.751531 2.494125 126.9928 0.6125086 13.17567 12.7498 1.900796 392.3966 6.876697 0.3987369 3.358635 2.752936 12.46359 24.51894 6.771038 890.5619 122.3215 1.145183 82.96384 2.826967 19.10221 350.6712 \n",
      "Testing error:  30.97815 29.66304 1.890809 5.64455 10.39465 9.712695 54.47921 12.44059 0.3998688 0.2114464 0.1669511 0.5783634 3.445466 4.836893 5.840885 0.4502649 0.3105615 1.506523 1.155394 47.93043 7.111697 84.79978 10.51324 3.090358 3.66992 18.48137 22.08741 2.368665 1.29136 33.06717 0.1855813 10.82053 3.321453 0.3335148 55.28606 5.729017 0.488382 2.739135 2.560963 3.482242 3.486906 5.3941 120.456 21.82361 0.7863911 12.25211 2.881309 8.031119 223.5297 \n",
      "total training time:  30.63452 \n",
      "Training error:  119.3398 114.6329 1.708057 15.75679 8.412752 115.7258 21.11208 26.43508 3.709641 0.6076277 0.8083031 2.444312 5.359959 4.389012 7.689671 1.052159 1.850138 1.229541 0.289239 169.1105 5.215024 95.01199 12.99727 2.534415 7.078416 135.454 115.3448 4.687944 2.494125 27.80487 0.6125086 12.68857 12.7498 1.900796 392.3966 3.403 0.380951 3.358635 2.752936 12.46359 24.51894 1.678472 890.5619 122.3215 0.8900094 82.96384 0.8348241 19.10221 350.6712 \n",
      "Testing error:  30.97815 29.66304 1.890809 5.64455 10.39465 9.712695 54.47921 12.44059 0.3998688 0.2114464 0.1669511 0.5783634 3.451111 4.836893 5.840885 0.4502649 0.3105615 1.506523 1.143664 47.93043 7.111697 111.6167 10.51324 3.090358 3.66992 18.48137 22.08741 2.368471 1.29136 30.81286 0.1855813 10.83344 3.321453 0.3335148 55.28606 5.240068 0.4895775 2.739135 2.560963 3.482242 3.486906 7.41522 120.456 21.82361 0.6109977 12.25211 3.127056 8.031119 223.5297 \n",
      "Training error:  25.06077 20.74134 1.556941 6.352667 2.392384 18.67007 1.531842 1.917622 1.902956 0.6076277 0.8083031 1.875255 2.271084 1.992666 1.865536 1.052159 1.566264 0.9508616 0.4338346 10.662 3.581633 0.3105706 6.042813 2.52992 4.312356 15.97068 10.56329 3.783071 2.477387 1.550356 0.6125086 8.584066 2.124401 1.900796 25.56693 1.893187 0.3987369 2.702499 1.988489 8.491216 12.05452 3.011686 5.154247 4.908107 0.9130878 15.53697 1.304787 1.297349 51.48998 \n",
      "Testing error:  87.97898 77.79903 1.832372 6.829938 15.45967 69.70729 54.98627 21.47469 0.4760277 0.2114464 0.1669511 0.5896701 2.974028 5.122716 8.331798 0.4502649 0.2819878 1.643908 1.155394 107.3894 7.237121 84.42274 12.91708 3.085279 3.983439 81.41023 83.85226 2.513353 1.297699 58.52152 0.1855813 11.47843 7.453105 0.3335148 356.2483 5.594759 0.488382 2.784996 2.111831 3.931917 6.745407 6.412339 387.5786 112.1624 0.6011787 47.03217 2.674268 15.12888 334.4925 \n",
      "total training time:  30.67829 \n",
      "lambda1= 76.39587 ; lambda2= 0.01165389 ; avg tune err= 43.01104 \n",
      "Training error:  119.3398 114.6329 1.708057 15.75679 8.412752 115.7258 21.11208 26.43508 3.709641 0.6076277 0.8083031 2.444312 5.359959 4.389012 7.689671 1.052159 1.850138 1.229541 0.289239 169.1105 5.215024 95.01199 12.99727 2.534415 7.078416 135.454 115.3448 4.687944 2.494125 27.80487 0.6125086 12.68857 12.7498 1.900796 392.3966 3.403 0.380951 3.358635 2.752936 12.46359 24.51894 1.678472 890.5619 122.3215 0.8900094 82.96384 0.8348241 19.10221 350.6712 \n",
      "Testing error:  30.97815 29.66304 1.890809 5.64455 10.39465 9.712695 54.47921 12.44059 0.3998688 0.2114464 0.1669511 0.5783634 3.451111 4.836893 5.840885 0.4502649 0.3105615 1.506523 1.143664 47.93043 7.111697 111.6167 10.51324 3.090358 3.66992 18.48137 22.08741 2.368471 1.29136 30.81286 0.1855813 10.83344 3.321453 0.3335148 55.28606 5.240068 0.4895775 2.739135 2.560963 3.482242 3.486906 7.41522 120.456 21.82361 0.6109977 12.25211 3.127056 8.031119 223.5297 \n",
      "Training error:  119.3398 114.6329 1.708057 15.75679 8.412752 115.7258 21.11208 26.43508 3.709641 0.6076277 0.8083031 2.444312 5.459433 4.389012 7.689671 1.052159 1.850138 1.229541 0.4338346 169.1105 5.215024 161.7591 12.99727 2.534415 7.078416 135.454 115.3448 4.751531 2.494125 126.9928 0.6125086 13.17567 12.7498 1.900796 392.3966 6.876697 0.3987369 3.358635 2.752936 12.46359 24.51894 6.771038 890.5619 122.3215 1.145183 82.96384 2.826967 19.10221 350.6712 \n",
      "Testing error:  30.97815 29.66304 1.890809 5.64455 10.39465 9.712695 54.47921 12.44059 0.3998688 0.2114464 0.1669511 0.5783634 3.445466 4.836893 5.840885 0.4502649 0.3105615 1.506523 1.155394 47.93043 7.111697 84.79978 10.51324 3.090358 3.66992 18.48137 22.08741 2.368665 1.29136 33.06717 0.1855813 10.82053 3.321453 0.3335148 55.28606 5.729017 0.488382 2.739135 2.560963 3.482242 3.486906 5.3941 120.456 21.82361 0.7863911 12.25211 2.881309 8.031119 223.5297 \n",
      "total training time:  30.48505 \n",
      "Training error:  119.3398 114.6329 1.708057 15.75679 8.412752 115.7258 21.11208 26.43508 3.709641 0.6076277 0.8083031 2.444312 5.359959 4.389012 7.689671 1.052159 1.850138 1.229541 0.289239 169.1105 5.215024 95.01199 12.99727 2.534415 7.078416 135.454 115.3448 4.687944 2.494125 27.80487 0.6125086 12.68857 12.7498 1.900796 392.3966 3.403 0.380951 3.358635 2.752936 12.46359 24.51894 1.678472 890.5619 122.3215 0.8900094 82.96384 0.8348241 19.10221 350.6712 \n",
      "Testing error:  30.97815 29.66304 1.890809 5.64455 10.39465 9.712695 54.47921 12.44059 0.3998688 0.2114464 0.1669511 0.5783634 3.451111 4.836893 5.840885 0.4502649 0.3105615 1.506523 1.143664 47.93043 7.111697 111.6167 10.51324 3.090358 3.66992 18.48137 22.08741 2.368471 1.29136 30.81286 0.1855813 10.83344 3.321453 0.3335148 55.28606 5.240068 0.4895775 2.739135 2.560963 3.482242 3.486906 7.41522 120.456 21.82361 0.6109977 12.25211 3.127056 8.031119 223.5297 \n",
      "Training error:  25.06077 20.74134 1.556941 6.352667 2.392384 18.67007 1.531842 1.917622 1.902956 0.6076277 0.8083031 1.875255 2.271084 1.992666 1.865536 1.052159 1.566264 0.9508616 0.4338346 10.662 3.581633 0.3105706 6.042813 2.52992 4.312356 15.97068 10.56329 3.783071 2.477387 1.550356 0.6125086 8.584066 2.124401 1.900796 25.56693 1.893187 0.3987369 2.702499 1.988489 8.491216 12.05452 3.011686 5.154247 4.908107 0.9130878 15.53697 1.304787 1.297349 51.48998 \n",
      "Testing error:  87.97898 77.79903 1.832372 6.829938 15.45967 69.70729 54.98627 21.47469 0.4760277 0.2114464 0.1669511 0.5896701 2.974028 5.122716 8.331798 0.4502649 0.2819878 1.643908 1.155394 107.3894 7.237121 84.42274 12.91708 3.085279 3.983439 81.41023 83.85226 2.513353 1.297699 58.52152 0.1855813 11.47843 7.453105 0.3335148 356.2483 5.594759 0.488382 2.784996 2.111831 3.931917 6.745407 6.412339 387.5786 112.1624 0.6011787 47.03217 2.674268 15.12888 334.4925 \n",
      "total training time:  30.50799 \n",
      "Time difference of 19.99523 mins\n",
      "Training error:  124.8223 110.3077 1.883483 3.763209 8.118563 114.227 15.30457 18.7186 3.261099 0.5562415 0.8679712 2.413123 5.157031 4.164879 6.885867 0.9734866 1.615562 1.947656 0.9357094 164.5242 5.168911 295.8283 14.23351 3.337734 7.419827 112.4822 102.5964 8.763962 2.436706 79.42456 0.6542333 15.90259 11.87528 2.101873 391.5968 6.616889 0.5065581 17.70669 3.108761 11.51866 18.9272 7.451642 899.2863 18.51815 0.7691304 98.0103 1.392167 19.89778 288.0852 \n",
      "Testing error:  19.85596 23.63955 0.6160155 35.90172 4.391137 8.636189 27.10788 29.90024 0.6010925 0.0827707 0.1770268 0.3674347 2.664534 3.030622 4.89763 0.6814092 0.2451048 1.129974 0.4454563 21.01982 5.418442 107.1302 4.905399 1.591772 3.876459 106.6477 16.71267 3.599876 1.488676 16.88805 0.3008918 13.40872 8.326852 0.4349411 47.61137 5.485216 0.5969048 4.731752 1.202536 5.738261 16.47225 5.258426 181.1372 305.7091 0.6003362 12.36739 5.055204 12.46633 450.7253 \n",
      "Training error:  0.4048177 0.2899942 0.0013328 0.002484882 0.0001361673 0.4112807 5.509078e-06 2.108961e-05 0.0002933423 1.399052e-06 9.181761e-06 1.329222e-05 2.677513e-06 4.746797e-06 9.387875e-06 3.51037e-06 1.772986e-06 0.001589385 1.345626e-07 0.04485021 0.001428009 4.344816e+39 0.00393215 0.0004311197 0.002104799 0.07688616 0.04990173 0.003832316 0.0004066816 1.295641e-05 5.619981e-05 0.0241526 1.073457e+12 0.02453445 0.6280239 7.417783e-07 1.234216e-05 0.0007555169 3.726189e-06 0.05595742 0.08922523 98106.81 0.0001067205 0.0002683489 2.255249e-05 0.2556923 5.693801e-08 2.385174e-07 1.751102 \n",
      "Testing error:  100.6808 106.2417 1918.285 42.5527 197.1607 56.74362 42.73825 38.85068 9.42299 0.3861915 0.5531962 3.036631 10.47714 5.864814 15.43969 1.349294 0.633994 170.6236 3.124422 111.5735 12.37624 1.391995e+42 15.77454 5.010338 11.5594 171.4354 127.9508 10.23418 4.045278 132.9383 10.35095 23.88804 9.558123e+13 1.60761 257.0551 11.87577 1.252053 17.65601 2.078758 11.47396 29.18941 14476218 644.5895 321.7485 1.294825 69.04534 4.429002 37.77605 563.9296 \n",
      "total training time:  30.57562 \n",
      "lambda1= 0.01448908 ; lambda2= 0.01448908 ; avg tune err= 2.840806e+40 \n",
      "Training error:  124.8223 110.3077 1.883483 3.763209 8.118563 114.227 15.30457 18.7186 3.261099 0.5562415 0.8679712 2.413123 5.157031 4.164879 6.885867 0.9734866 1.615562 1.947656 0.9357094 164.5242 5.168911 295.8283 14.23351 3.337734 7.419827 112.4822 102.5964 8.763962 2.436706 79.42456 0.6542333 15.90259 11.87528 2.101873 391.5968 6.616889 0.5065581 17.70669 3.108761 11.51866 18.9272 7.451642 899.2863 18.51815 0.7691304 98.0103 1.392167 19.89778 288.0852 \n",
      "Testing error:  19.85596 23.63955 0.6160155 35.90172 4.391137 8.636189 27.10788 29.90024 0.6010925 0.0827707 0.1770268 0.3674347 2.664534 3.030622 4.89763 0.6814092 0.2451048 1.129974 0.4454563 21.01982 5.418442 107.1302 4.905399 1.591772 3.876459 106.6477 16.71267 3.599876 1.488676 16.88805 0.3008918 13.40872 8.326852 0.4349411 47.61137 5.485216 0.5969048 4.731752 1.202536 5.738261 16.47225 5.258426 181.1372 305.7091 0.6003362 12.36739 5.055204 12.46633 450.7253 \n",
      "Training error:  124.8223 110.3077 1.883483 5.867505 8.118563 114.227 17.61183 18.7186 3.261099 0.5562415 0.8679712 2.413123 5.157031 4.164879 6.885867 0.9734866 1.615562 2.050034 0.9357094 164.5242 5.168911 295.8283 14.23351 3.524038 7.419827 112.4822 117.8414 8.763962 2.436706 109.0466 0.6542333 16.01656 11.87528 2.101873 391.5968 6.778201 0.5065581 17.70669 3.108761 11.51866 20.70619 7.451642 899.2863 18.51815 1.098387 98.0103 1.392167 19.89778 288.0852 \n",
      "Testing error:  19.85596 23.63955 0.6160155 35.09043 4.391137 8.636189 27.81961 29.90024 0.6010925 0.0827707 0.1770268 0.3674347 2.664534 3.030622 4.89763 0.6814092 0.2451048 1.124524 0.4454563 21.01982 5.418442 107.1302 4.905399 1.615702 3.876459 106.6477 17.03444 3.599876 1.488676 17.90618 0.3008918 13.43641 8.326852 0.4349411 47.61137 5.544399 0.5969048 4.731752 1.202536 5.738261 16.44852 5.258426 181.1372 305.7091 0.7705434 12.36739 5.055204 12.46633 450.7253 \n",
      "total training time:  30.57011 \n",
      "Training error:  124.8223 110.3077 1.883483 3.763209 8.118563 114.227 15.30457 18.7186 3.261099 0.5562415 0.8679712 2.413123 5.157031 4.164879 6.885867 0.9734866 1.615562 1.947656 0.9357094 164.5242 5.168911 295.8283 14.23351 3.337734 7.419827 112.4822 102.5964 8.763962 2.436706 79.42456 0.6542333 15.90259 11.87528 2.101873 391.5968 6.616889 0.5065581 17.70669 3.108761 11.51866 18.9272 7.451642 899.2863 18.51815 0.7691304 98.0103 1.392167 19.89778 288.0852 \n",
      "Testing error:  19.85596 23.63955 0.6160155 35.90172 4.391137 8.636189 27.10788 29.90024 0.6010925 0.0827707 0.1770268 0.3674347 2.664534 3.030622 4.89763 0.6814092 0.2451048 1.129974 0.4454563 21.01982 5.418442 107.1302 4.905399 1.591772 3.876459 106.6477 16.71267 3.599876 1.488676 16.88805 0.3008918 13.40872 8.326852 0.4349411 47.61137 5.485216 0.5969048 4.731752 1.202536 5.738261 16.47225 5.258426 181.1372 305.7091 0.6003362 12.36739 5.055204 12.46633 450.7253 \n",
      "Training error:  3.142834 3.287541 0.1068607 0.5629138 0.1465258 3.521874 0.07697257 0.09824178 0.1206162 0.09075365 0.1387519 0.1612539 0.1088773 0.1302163 0.1219668 0.1328976 0.1265792 0.05497109 0.04179982 1.512659 0.5074814 6.411511e+35 0.5232612 0.4262239 0.4076795 1.225561 1.515431 0.6075648 0.3176784 0.05330128 0.07975082 1.13407 0.08294155 0.7272774 4.251215 0.09614636 0.1572385 0.2791749 0.1519236 1.279384 1.955422 0.06901476 0.18428 0.2828835 0.1755699 2.82187 0.04285533 0.04649912 7.268991 \n",
      "Testing error:  83.81389 88.24133 1.001654 39.3349 9.146461 42.33937 30.50622 40.68572 1.031183 0.2307346 0.3957512 1.717303 6.45751 5.68045 9.027903 0.7285044 0.359875 2.11737 0.6824548 94.37345 7.205229 2.054124e+38 5.881457 3.65621 7.894726 135.9896 83.17627 4.770375 2.813091 76.70661 0.3663859 22.72139 14.73412 0.7896483 240.7075 9.020638 0.8040269 11.66682 2.830236 7.254147 24.92819 12.4382 708.698 298.5556 0.791359 50.55778 4.002364 21.76228 477.333 \n",
      "total training time:  30.48847 \n",
      "lambda1= 14.16235 ; lambda2= 0.01448908 ; avg tune err= 4.192089e+36 \n",
      "Training error:  124.8223 110.3077 1.883483 3.763209 8.118563 114.227 15.30457 18.7186 3.261099 0.5562415 0.8679712 2.413123 5.157031 4.164879 6.885867 0.9734866 1.615562 1.947656 0.9357094 164.5242 5.168911 295.8283 14.23351 3.337734 7.419827 112.4822 102.5964 8.763962 2.436706 79.42456 0.6542333 15.90259 11.87528 2.101873 391.5968 6.616889 0.5065581 17.70669 3.108761 11.51866 18.9272 7.451642 899.2863 18.51815 0.7691304 98.0103 1.392167 19.89778 288.0852 \n",
      "Testing error:  19.85596 23.63955 0.6160155 35.90172 4.391137 8.636189 27.10788 29.90024 0.6010925 0.0827707 0.1770268 0.3674347 2.664534 3.030622 4.89763 0.6814092 0.2451048 1.129974 0.4454563 21.01982 5.418442 107.1302 4.905399 1.591772 3.876459 106.6477 16.71267 3.599876 1.488676 16.88805 0.3008918 13.40872 8.326852 0.4349411 47.61137 5.485216 0.5969048 4.731752 1.202536 5.738261 16.47225 5.258426 181.1372 305.7091 0.6003362 12.36739 5.055204 12.46633 450.7253 \n",
      "Training error:  124.8223 110.3077 1.883483 5.867505 8.118563 114.227 17.61183 18.7186 3.261099 0.5562415 0.8679712 2.413123 5.157031 4.164879 6.885867 0.9734866 1.615562 2.050034 0.9357094 164.5242 5.168911 295.8283 14.23351 3.524038 7.419827 112.4822 117.8414 8.763962 2.436706 109.0466 0.6542333 16.01656 11.87528 2.101873 391.5968 6.778201 0.5065581 17.70669 3.108761 11.51866 20.70619 7.451642 899.2863 18.51815 1.098387 98.0103 1.392167 19.89778 288.0852 \n",
      "Testing error:  19.85596 23.63955 0.6160155 35.09043 4.391137 8.636189 27.81961 29.90024 0.6010925 0.0827707 0.1770268 0.3674347 2.664534 3.030622 4.89763 0.6814092 0.2451048 1.124524 0.4454563 21.01982 5.418442 107.1302 4.905399 1.615702 3.876459 106.6477 17.03444 3.599876 1.488676 17.90618 0.3008918 13.43641 8.326852 0.4349411 47.61137 5.544399 0.5969048 4.731752 1.202536 5.738261 16.44852 5.258426 181.1372 305.7091 0.7705434 12.36739 5.055204 12.46633 450.7253 \n",
      "total training time:  30.66773 \n",
      "Training error:  124.8223 110.3077 1.883483 3.763209 8.118563 114.227 15.30457 18.7186 3.261099 0.5562415 0.8679712 2.413123 5.157031 4.164879 6.885867 0.9734866 1.615562 1.947656 0.9357094 164.5242 5.168911 295.8283 14.23351 3.337734 7.419827 112.4822 102.5964 8.763962 2.436706 79.42456 0.6542333 15.90259 11.87528 2.101873 391.5968 6.616889 0.5065581 17.70669 3.108761 11.51866 18.9272 7.451642 899.2863 18.51815 0.7691304 98.0103 1.392167 19.89778 288.0852 \n",
      "Testing error:  19.85596 23.63955 0.6160155 35.90172 4.391137 8.636189 27.10788 29.90024 0.6010925 0.0827707 0.1770268 0.3674347 2.664534 3.030622 4.89763 0.6814092 0.2451048 1.129974 0.4454563 21.01982 5.418442 107.1302 4.905399 1.591772 3.876459 106.6477 16.71267 3.599876 1.488676 16.88805 0.3008918 13.40872 8.326852 0.4349411 47.61137 5.485216 0.5969048 4.731752 1.202536 5.738261 16.47225 5.258426 181.1372 305.7091 0.6003362 12.36739 5.055204 12.46633 450.7253 \n",
      "Training error:  7.765194 8.417175 0.2967337 1.501191 0.4318118 6.912051 0.2542852 0.4197358 0.3929707 0.2575263 0.4014677 0.4151323 0.3613352 0.4271343 0.3520279 0.4043769 0.3996437 0.2079135 0.1722253 4.054599 1.401159 1.956331e+27 1.453432 1.096408 1.144571 3.224285 3.636365 1.771116 0.7710546 0.1456811 0.2270197 2.710719 0.3091407 1.410596 8.765374 0.3470946 0.423191 0.9205804 0.3778853 2.70821 4.191992 0.2562428 0.6872684 0.9302696 0.4326862 5.903468 0.1512255 0.2012267 15.10851 \n",
      "Testing error:  71.90532 70.06232 0.7479574 38.69463 8.190817 35.16994 32.77128 42.22512 1.232276 0.1168442 0.200937 1.664062 4.640002 5.190704 9.291199 0.6333003 0.2545794 2.565986 0.6044767 75.36707 6.563328 6.267703e+29 5.844885 2.951778 6.508912 128.8849 54.86933 4.973397 2.469009 68.24615 0.3562871 19.13313 13.16611 0.4999889 224.8874 7.469971 0.6165653 7.976281 2.942826 7.342425 21.10469 11.4997 688.2923 293.8201 0.5626072 40.43498 4.438216 20.98331 446.4022 \n",
      "total training time:  30.50808 \n",
      "lambda1= 28.31021 ; lambda2= 0.01448908 ; avg tune err= 1.279123e+28 \n",
      "Training error:  124.8223 110.3077 1.883483 3.763209 8.118563 114.227 15.30457 18.7186 3.261099 0.5562415 0.8679712 2.413123 5.157031 4.164879 6.885867 0.9734866 1.615562 1.947656 0.9357094 164.5242 5.168911 295.8283 14.23351 3.337734 7.419827 112.4822 102.5964 8.763962 2.436706 79.42456 0.6542333 15.90259 11.87528 2.101873 391.5968 6.616889 0.5065581 17.70669 3.108761 11.51866 18.9272 7.451642 899.2863 18.51815 0.7691304 98.0103 1.392167 19.89778 288.0852 \n",
      "Testing error:  19.85596 23.63955 0.6160155 35.90172 4.391137 8.636189 27.10788 29.90024 0.6010925 0.0827707 0.1770268 0.3674347 2.664534 3.030622 4.89763 0.6814092 0.2451048 1.129974 0.4454563 21.01982 5.418442 107.1302 4.905399 1.591772 3.876459 106.6477 16.71267 3.599876 1.488676 16.88805 0.3008918 13.40872 8.326852 0.4349411 47.61137 5.485216 0.5969048 4.731752 1.202536 5.738261 16.47225 5.258426 181.1372 305.7091 0.6003362 12.36739 5.055204 12.46633 450.7253 \n",
      "Training error:  124.8223 110.3077 1.883483 5.867505 8.118563 114.227 17.61183 18.7186 3.261099 0.5562415 0.8679712 2.413123 5.157031 4.164879 6.885867 0.9734866 1.615562 2.050034 0.9357094 164.5242 5.168911 295.8283 14.23351 3.524038 7.419827 112.4822 117.8414 8.763962 2.436706 109.0466 0.6542333 16.01656 11.87528 2.101873 391.5968 6.778201 0.5065581 17.70669 3.108761 11.51866 20.70619 7.451642 899.2863 18.51815 1.098387 98.0103 1.392167 19.89778 288.0852 \n",
      "Testing error:  19.85596 23.63955 0.6160155 35.09043 4.391137 8.636189 27.81961 29.90024 0.6010925 0.0827707 0.1770268 0.3674347 2.664534 3.030622 4.89763 0.6814092 0.2451048 1.124524 0.4454563 21.01982 5.418442 107.1302 4.905399 1.615702 3.876459 106.6477 17.03444 3.599876 1.488676 17.90618 0.3008918 13.43641 8.326852 0.4349411 47.61137 5.544399 0.5969048 4.731752 1.202536 5.738261 16.44852 5.258426 181.1372 305.7091 0.7705434 12.36739 5.055204 12.46633 450.7253 \n",
      "total training time:  30.75471 \n",
      "Training error:  124.8223 110.3077 1.883483 3.763209 8.118563 114.227 15.30457 18.7186 3.261099 0.5562415 0.8679712 2.413123 5.157031 4.164879 6.885867 0.9734866 1.615562 1.947656 0.9357094 164.5242 5.168911 295.8283 14.23351 3.337734 7.419827 112.4822 102.5964 8.763962 2.436706 79.42456 0.6542333 15.90259 11.87528 2.101873 391.5968 6.616889 0.5065581 17.70669 3.108761 11.51866 18.9272 7.451642 899.2863 18.51815 0.7691304 98.0103 1.392167 19.89778 288.0852 \n",
      "Testing error:  19.85596 23.63955 0.6160155 35.90172 4.391137 8.636189 27.10788 29.90024 0.6010925 0.0827707 0.1770268 0.3674347 2.664534 3.030622 4.89763 0.6814092 0.2451048 1.129974 0.4454563 21.01982 5.418442 107.1302 4.905399 1.591772 3.876459 106.6477 16.71267 3.599876 1.488676 16.88805 0.3008918 13.40872 8.326852 0.4349411 47.61137 5.485216 0.5969048 4.731752 1.202536 5.738261 16.47225 5.258426 181.1372 305.7091 0.6003362 12.36739 5.055204 12.46633 450.7253 \n",
      "Training error:  13.37575 14.16572 0.5550088 2.590493 0.7917257 10.97657 0.5588807 0.9200418 0.7727779 0.4836303 0.6903547 0.7847757 0.8257917 0.8676479 0.7045271 0.625581 0.7886645 0.4676912 0.4114659 7.21865 2.389805 0.1481637 2.695665 1.814022 2.126038 5.779133 5.938041 3.134214 1.351171 0.3011874 0.4308993 4.769449 0.6668068 1.920699 14.70027 0.6657697 0.5065581 1.897218 0.827985 4.38801 6.53837 0.5819564 1.602056 1.626852 0.6754045 9.732594 0.3395345 0.4548075 25.72221 \n",
      "Testing error:  69.75973 61.05107 0.746752 38.32224 7.928502 34.14472 31.46938 42.80392 1.149131 0.0886738 0.1656967 1.223772 4.144218 5.015465 8.673234 0.5808437 0.2057889 2.098126 0.5839094 58.40783 5.968445 433.7209 6.705986 2.357452 5.777994 128.7007 39.9544 4.58203 1.936259 63.56897 0.3675844 18.28402 12.29925 0.4372769 195.126 7.059569 0.5969048 7.853689 2.057999 6.749941 19.71208 9.626659 630.2215 296.6397 0.4996078 33.6919 4.820259 20.82687 441.7034 \n",
      "total training time:  30.73212 \n",
      "lambda1= 42.45808 ; lambda2= 0.01448908 ; avg tune err= 56.53899 \n",
      "Training error:  124.8223 110.3077 1.883483 3.763209 8.118563 114.227 15.30457 18.7186 3.261099 0.5562415 0.8679712 2.413123 5.157031 4.164879 6.885867 0.9734866 1.615562 1.947656 0.9357094 164.5242 5.168911 295.8283 14.23351 3.337734 7.419827 112.4822 102.5964 8.763962 2.436706 79.42456 0.6542333 15.90259 11.87528 2.101873 391.5968 6.616889 0.5065581 17.70669 3.108761 11.51866 18.9272 7.451642 899.2863 18.51815 0.7691304 98.0103 1.392167 19.89778 288.0852 \n",
      "Testing error:  19.85596 23.63955 0.6160155 35.90172 4.391137 8.636189 27.10788 29.90024 0.6010925 0.0827707 0.1770268 0.3674347 2.664534 3.030622 4.89763 0.6814092 0.2451048 1.129974 0.4454563 21.01982 5.418442 107.1302 4.905399 1.591772 3.876459 106.6477 16.71267 3.599876 1.488676 16.88805 0.3008918 13.40872 8.326852 0.4349411 47.61137 5.485216 0.5969048 4.731752 1.202536 5.738261 16.47225 5.258426 181.1372 305.7091 0.6003362 12.36739 5.055204 12.46633 450.7253 \n",
      "Training error:  124.8223 110.3077 1.883483 5.867505 8.118563 114.227 17.61183 18.7186 3.261099 0.5562415 0.8679712 2.413123 5.157031 4.164879 6.885867 0.9734866 1.615562 2.050034 0.9357094 164.5242 5.168911 295.8283 14.23351 3.524038 7.419827 112.4822 117.8414 8.763962 2.436706 109.0466 0.6542333 16.01656 11.87528 2.101873 391.5968 6.778201 0.5065581 17.70669 3.108761 11.51866 20.70619 7.451642 899.2863 18.51815 1.098387 98.0103 1.392167 19.89778 288.0852 \n",
      "Testing error:  19.85596 23.63955 0.6160155 35.09043 4.391137 8.636189 27.81961 29.90024 0.6010925 0.0827707 0.1770268 0.3674347 2.664534 3.030622 4.89763 0.6814092 0.2451048 1.124524 0.4454563 21.01982 5.418442 107.1302 4.905399 1.615702 3.876459 106.6477 17.03444 3.599876 1.488676 17.90618 0.3008918 13.43641 8.326852 0.4349411 47.61137 5.544399 0.5969048 4.731752 1.202536 5.738261 16.44852 5.258426 181.1372 305.7091 0.7705434 12.36739 5.055204 12.46633 450.7253 \n",
      "total training time:  30.68397 \n",
      "Training error:  124.8223 110.3077 1.883483 3.763209 8.118563 114.227 15.30457 18.7186 3.261099 0.5562415 0.8679712 2.413123 5.157031 4.164879 6.885867 0.9734866 1.615562 1.947656 0.9357094 164.5242 5.168911 295.8283 14.23351 3.337734 7.419827 112.4822 102.5964 8.763962 2.436706 79.42456 0.6542333 15.90259 11.87528 2.101873 391.5968 6.616889 0.5065581 17.70669 3.108761 11.51866 18.9272 7.451642 899.2863 18.51815 0.7691304 98.0103 1.392167 19.89778 288.0852 \n",
      "Testing error:  19.85596 23.63955 0.6160155 35.90172 4.391137 8.636189 27.10788 29.90024 0.6010925 0.0827707 0.1770268 0.3674347 2.664534 3.030622 4.89763 0.6814092 0.2451048 1.129974 0.4454563 21.01982 5.418442 107.1302 4.905399 1.591772 3.876459 106.6477 16.71267 3.599876 1.488676 16.88805 0.3008918 13.40872 8.326852 0.4349411 47.61137 5.485216 0.5969048 4.731752 1.202536 5.738261 16.47225 5.258426 181.1372 305.7091 0.6003362 12.36739 5.055204 12.46633 450.7253 \n",
      "Training error:  18.14825 19.00306 0.9082181 3.555064 1.271863 14.85284 0.9772133 1.442063 1.248555 0.5562415 0.8496984 1.196258 1.342996 1.387673 1.112066 0.821198 1.204851 0.8028859 0.6542376 9.783663 3.071943 0.3043292 3.870586 2.513267 2.957196 8.316562 8.403248 4.426504 1.877196 0.436539 0.5660045 6.562123 1.152545 2.08377 19.80644 1.108966 0.5065581 2.950475 1.389843 6.164168 8.54288 1.015902 2.869632 2.38667 0.775347 13.2193 0.5381978 0.7983868 35.58467 \n",
      "Testing error:  64.56698 56.46178 0.77725 37.93165 6.698426 32.34482 31.98622 43.94293 1.05186 0.0827707 0.1709162 0.8364521 4.178819 4.534195 8.242183 0.642618 0.2132727 1.474319 0.4823235 52.92636 5.893388 458.7469 7.421865 2.075749 5.237559 125.7036 31.58079 4.472809 1.688775 63.49856 0.3226456 16.89655 11.65688 0.4369025 181.7062 6.697773 0.5969048 8.284961 1.694706 6.65581 18.75269 7.994517 597.7861 296.1309 0.5272636 31.32128 4.992433 20.36117 445.9188 \n",
      "total training time:  30.70727 \n",
      "lambda1= 56.60594 ; lambda2= 0.01448908 ; avg tune err= 55.40001 \n",
      "Training error:  124.8223 110.3077 1.883483 3.763209 8.118563 114.227 15.30457 18.7186 3.261099 0.5562415 0.8679712 2.413123 5.157031 4.164879 6.885867 0.9734866 1.615562 1.947656 0.9357094 164.5242 5.168911 295.8283 14.23351 3.337734 7.419827 112.4822 102.5964 8.763962 2.436706 79.42456 0.6542333 15.90259 11.87528 2.101873 391.5968 6.616889 0.5065581 17.70669 3.108761 11.51866 18.9272 7.451642 899.2863 18.51815 0.7691304 98.0103 1.392167 19.89778 288.0852 \n",
      "Testing error:  19.85596 23.63955 0.6160155 35.90172 4.391137 8.636189 27.10788 29.90024 0.6010925 0.0827707 0.1770268 0.3674347 2.664534 3.030622 4.89763 0.6814092 0.2451048 1.129974 0.4454563 21.01982 5.418442 107.1302 4.905399 1.591772 3.876459 106.6477 16.71267 3.599876 1.488676 16.88805 0.3008918 13.40872 8.326852 0.4349411 47.61137 5.485216 0.5969048 4.731752 1.202536 5.738261 16.47225 5.258426 181.1372 305.7091 0.6003362 12.36739 5.055204 12.46633 450.7253 \n",
      "Training error:  124.8223 110.3077 1.883483 5.867505 8.118563 114.227 17.61183 18.7186 3.261099 0.5562415 0.8679712 2.413123 5.157031 4.164879 6.885867 0.9734866 1.615562 2.050034 0.9357094 164.5242 5.168911 295.8283 14.23351 3.524038 7.419827 112.4822 117.8414 8.763962 2.436706 109.0466 0.6542333 16.01656 11.87528 2.101873 391.5968 6.778201 0.5065581 17.70669 3.108761 11.51866 20.70619 7.451642 899.2863 18.51815 1.098387 98.0103 1.392167 19.89778 288.0852 \n",
      "Testing error:  19.85596 23.63955 0.6160155 35.09043 4.391137 8.636189 27.81961 29.90024 0.6010925 0.0827707 0.1770268 0.3674347 2.664534 3.030622 4.89763 0.6814092 0.2451048 1.124524 0.4454563 21.01982 5.418442 107.1302 4.905399 1.615702 3.876459 106.6477 17.03444 3.599876 1.488676 17.90618 0.3008918 13.43641 8.326852 0.4349411 47.61137 5.544399 0.5969048 4.731752 1.202536 5.738261 16.44852 5.258426 181.1372 305.7091 0.7705434 12.36739 5.055204 12.46633 450.7253 \n",
      "total training time:  30.5581 \n",
      "Training error:  124.8223 110.3077 1.883483 3.763209 8.118563 114.227 15.30457 18.7186 3.261099 0.5562415 0.8679712 2.413123 5.157031 4.164879 6.885867 0.9734866 1.615562 1.947656 0.9357094 164.5242 5.168911 295.8283 14.23351 3.337734 7.419827 112.4822 102.5964 8.763962 2.436706 79.42456 0.6542333 15.90259 11.87528 2.101873 391.5968 6.616889 0.5065581 17.70669 3.108761 11.51866 18.9272 7.451642 899.2863 18.51815 0.7691304 98.0103 1.392167 19.89778 288.0852 \n",
      "Testing error:  19.85596 23.63955 0.6160155 35.90172 4.391137 8.636189 27.10788 29.90024 0.6010925 0.0827707 0.1770268 0.3674347 2.664534 3.030622 4.89763 0.6814092 0.2451048 1.129974 0.4454563 21.01982 5.418442 107.1302 4.905399 1.591772 3.876459 106.6477 16.71267 3.599876 1.488676 16.88805 0.3008918 13.40872 8.326852 0.4349411 47.61137 5.485216 0.5969048 4.731752 1.202536 5.738261 16.47225 5.258426 181.1372 305.7091 0.6003362 12.36739 5.055204 12.46633 450.7253 \n",
      "Training error:  18.14825 19.00306 0.9082181 3.555064 1.271863 14.85284 0.9772133 1.442063 1.248555 0.5562415 0.8496984 1.196258 1.342996 1.387673 1.112066 0.821198 1.204851 0.8028859 0.6542376 9.783663 3.071943 0.3043292 3.870586 2.513267 2.957196 8.316562 8.403248 4.426504 1.877196 0.436539 0.5660045 6.562123 1.152545 2.08377 19.80644 1.108966 0.5065581 2.950475 1.389843 6.164168 8.54288 1.015902 2.869632 2.38667 0.775347 13.2193 0.5381978 0.7983868 35.58467 \n",
      "Testing error:  64.56698 56.46178 0.77725 37.93165 6.698426 32.34482 31.98622 43.94293 1.05186 0.0827707 0.1709162 0.8364521 4.178819 4.534195 8.242183 0.642618 0.2132727 1.474319 0.4823235 52.92636 5.893388 458.7469 7.421865 2.075749 5.237559 125.7036 31.58079 4.472809 1.688775 63.49856 0.3226456 16.89655 11.65688 0.4369025 181.7062 6.697773 0.5969048 8.284961 1.694706 6.65581 18.75269 7.994517 597.7861 296.1309 0.5272636 31.32128 4.992433 20.36117 445.9188 \n",
      "total training time:  30.55778 \n",
      "Time difference of 19.83639 mins\n",
      "Training error:  127.2699 113.5674 2.171565 6.014871 6.642752 120.9974 28.9218 13.31708 3.357528 0.6032143 0.9622837 1.493697 0.02397151 3.831629 6.976987 0.9488489 1.670368 2.23438 1.272966 176.0807 2.778081 274.5129 13.45948 3.259448 5.409701 112.8623 36.85887 7.815794 1.633402 0.1756187 0.2491912 14.97025 2.841694 2.135357 376.5289 3.839202 0.5975813 18.37605 1.709777 11.59655 20.9054 7.442914 892.9871 21.35226 0.1151227 101.615 1.733723 16.20486 286.221 \n",
      "Testing error:  35.82484 33.76299 1.183126 5.311249 13.28045 11.56558 22.20365 31.41538 0.2920659 0.09908222 0.1553323 3.28522 5.641932 5.50162 5.820739 0.5619862 0.5371546 1.028465 0.2455652 44.7037 5.983353 160.9262 8.050334 3.914125 10.04049 28.9921 145.172 5.87695 2.112559 41.26478 1.318108 14.36179 27.25109 0.3623535 83.30343 6.07548 0.2699139 2.056144 2.550981 4.678401 5.885488 5.508604 193.0659 13.02924 1.314045 10.7462 1.619512 22.99527 242.9372 \n",
      "Training error:  0.3231245 0.2315038 1.857369e-06 0.0008284655 9.095101e-07 0.1957264 2.516049e-05 4.456008e-06 2.692369e-06 1.190694e-06 2.109592e-06 4.379301e-06 0.001487024 1.102066e-06 3.898303e-06 2.785286e-06 5.929592e-07 4.870534e-07 1.755611e+12 0.02298912 0.0006875215 2.213044e+43 0.001294689 0.0825474 0.000518709 0.03723055 0.03109195 0.003175263 0.0005319645 0.00039351 2.507913e-07 0.03246194 706836.2 0.01760364 0.6090726 5.862631e-07 4.797833e-06 0.0004621959 55691294 0.03848652 0.05608975 101.0163 9.061912e-05 0.6493979 0.0002820574 0.2854006 9.863969e-08 2.216421e-07 1.108888 \n",
      "Testing error:  176.3318 212.6895 4.127027 10.017 23.04117 133.5241 6060.03 78.44964 6.690846 2.235178 1.243653 6.919687 5.762319 8.90398 34.11493 2.583355 2.667062 2.706381 1.115758e+19 360.9284 7.961694 2.883131e+46 18.217 26.6545 14.63557 202.0801 142.2863 8.727011 3.291088 37.51152 1.41182 25.75601 4420045156 2.691962 622.0878 14.03075 0.6700697 20.85902 17898109237 17.60083 27.80102 811265.3 834.6271 109.499 1.532164 88.10562 6.805997 56.13304 458.9187 \n",
      "total training time:  30.73411 \n",
      "lambda1= 0.01288027 ; lambda2= 0.01288027 ; avg tune err= 5.88394e+44 \n",
      "Training error:  127.2699 113.5674 2.171565 6.014871 6.642752 120.9974 28.9218 13.31708 3.357528 0.6032143 0.9622837 1.493697 0.02397151 3.831629 6.976987 0.9488489 1.670368 2.23438 1.272966 176.0807 2.778081 274.5129 13.45948 3.259448 5.409701 112.8623 36.85887 7.815794 1.633402 0.1756187 0.2491912 14.97025 2.841694 2.135357 376.5289 3.839202 0.5975813 18.37605 1.709777 11.59655 20.9054 7.442914 892.9871 21.35226 0.1151227 101.615 1.733723 16.20486 286.221 \n",
      "Testing error:  35.82484 33.76299 1.183126 5.311249 13.28045 11.56558 22.20365 31.41538 0.2920659 0.09908222 0.1553323 3.28522 5.641932 5.50162 5.820739 0.5619862 0.5371546 1.028465 0.2455652 44.7037 5.983353 160.9262 8.050334 3.914125 10.04049 28.9921 145.172 5.87695 2.112559 41.26478 1.318108 14.36179 27.25109 0.3623535 83.30343 6.07548 0.2699139 2.056144 2.550981 4.678401 5.885488 5.508604 193.0659 13.02924 1.314045 10.7462 1.619512 22.99527 242.9372 \n",
      "Training error:  127.2699 113.5674 2.171565 6.014871 6.642752 120.9974 28.9218 13.44795 3.357528 0.6032143 0.9622837 1.493697 5.230764 3.831629 6.976987 0.9488489 1.670368 2.23438 1.272966 176.0807 5.279947 274.5129 16.51823 3.259448 5.409701 112.8623 118.9388 7.815794 2.366977 107.6489 0.2561332 14.97025 2.841694 2.135357 376.5289 6.67679 0.5975813 18.37605 2.964214 11.59655 20.9054 7.442914 892.9871 21.35226 0.7855478 101.615 1.733723 16.20486 286.221 \n",
      "Testing error:  35.82484 33.76299 1.183126 5.311249 13.28045 11.56558 22.20365 31.43464 0.2920659 0.09908222 0.1553323 3.28522 3.350251 5.50162 5.820739 0.5619862 0.5371546 1.028465 0.2455652 44.7037 6.390044 160.9262 4.92637 3.914125 10.04049 28.9921 28.37554 5.87695 1.701723 13.33153 1.317564 14.36179 27.25109 0.3623535 83.30343 6.148311 0.2699139 2.056144 2.979621 4.678401 5.885488 5.508604 193.0659 13.02924 1.589792 10.7462 1.619512 22.99527 242.9372 \n",
      "total training time:  30.64659 \n",
      "Training error:  127.2699 113.5674 2.171565 6.014871 6.642752 120.9974 28.9218 13.31708 3.357528 0.6032143 0.9622837 1.493697 0.02397151 3.831629 6.976987 0.9488489 1.670368 2.23438 1.272966 176.0807 2.778081 274.5129 13.45948 3.259448 5.409701 112.8623 36.85887 7.815794 1.633402 0.1756187 0.2491912 14.97025 2.841694 2.135357 376.5289 3.839202 0.5975813 18.37605 1.709777 11.59655 20.9054 7.442914 892.9871 21.35226 0.1151227 101.615 1.733723 16.20486 286.221 \n",
      "Testing error:  35.82484 33.76299 1.183126 5.311249 13.28045 11.56558 22.20365 31.41538 0.2920659 0.09908222 0.1553323 3.28522 5.641932 5.50162 5.820739 0.5619862 0.5371546 1.028465 0.2455652 44.7037 5.983353 160.9262 8.050334 3.914125 10.04049 28.9921 145.172 5.87695 2.112559 41.26478 1.318108 14.36179 27.25109 0.3623535 83.30343 6.07548 0.2699139 2.056144 2.550981 4.678401 5.885488 5.508604 193.0659 13.02924 1.314045 10.7462 1.619512 22.99527 242.9372 \n",
      "Training error:  2.805298 2.623538 0.08627378 0.3982595 0.1020339 2.843481 0.1325549 0.1330172 0.1297752 0.07615759 0.1265323 0.1174623 0.2966358 0.07984861 0.1009443 0.1257444 0.08730499 0.06712347 0.04058023 1.000429 0.5133955 4.718545e+28 0.5036648 0.2412346 0.3711551 1.219455 1.060795 0.6023538 0.3789377 0.1240209 0.07222233 1.104487 0.08067791 0.7331553 3.805002 0.08096417 0.1467119 0.2961749 0.2862716 1.353981 1.680568 0.06509705 0.1761005 0.3736491 0.1680279 2.643624 0.04208604 0.05367424 6.030528 \n",
      "Testing error:  154.9268 168.8436 2.440038 8.863589 16.03559 113.2868 434.292 45.54162 2.799614 0.3966914 0.4814128 3.701282 4.811186 6.019312 12.913 1.124854 1.483937 1.629036 1.020197 306.784 7.73613 6.147271e+31 11.66865 6.843438 11.59764 196.0556 125.4857 8.656085 2.014054 50.27255 1.448582 25.12278 26.1716 0.9191965 588.905 7.10218 0.5699071 13.74618 3.474277 13.98325 19.53481 6.952 877.1914 35.38065 1.234614 70.35545 2.861623 32.29037 388.3352 \n",
      "total training time:  30.71923 \n",
      "lambda1= 14.1765 ; lambda2= 0.01288027 ; avg tune err= 1.254545e+30 \n",
      "Training error:  127.2699 113.5674 2.171565 6.014871 6.642752 120.9974 28.9218 13.31708 3.357528 0.6032143 0.9622837 1.493697 0.02397151 3.831629 6.976987 0.9488489 1.670368 2.23438 1.272966 176.0807 2.778081 274.5129 13.45948 3.259448 5.409701 112.8623 36.85887 7.815794 1.633402 0.1756187 0.2491912 14.97025 2.841694 2.135357 376.5289 3.839202 0.5975813 18.37605 1.709777 11.59655 20.9054 7.442914 892.9871 21.35226 0.1151227 101.615 1.733723 16.20486 286.221 \n",
      "Testing error:  35.82484 33.76299 1.183126 5.311249 13.28045 11.56558 22.20365 31.41538 0.2920659 0.09908222 0.1553323 3.28522 5.641932 5.50162 5.820739 0.5619862 0.5371546 1.028465 0.2455652 44.7037 5.983353 160.9262 8.050334 3.914125 10.04049 28.9921 145.172 5.87695 2.112559 41.26478 1.318108 14.36179 27.25109 0.3623535 83.30343 6.07548 0.2699139 2.056144 2.550981 4.678401 5.885488 5.508604 193.0659 13.02924 1.314045 10.7462 1.619512 22.99527 242.9372 \n",
      "Training error:  127.2699 113.5674 2.171565 6.014871 6.642752 120.9974 28.9218 13.44795 3.357528 0.6032143 0.9622837 1.493697 5.230764 3.831629 6.976987 0.9488489 1.670368 2.23438 1.272966 176.0807 5.279947 274.5129 16.51823 3.259448 5.409701 112.8623 118.9388 7.815794 2.366977 107.6489 0.2561332 14.97025 2.841694 2.135357 376.5289 6.67679 0.5975813 18.37605 2.964214 11.59655 20.9054 7.442914 892.9871 21.35226 0.7855478 101.615 1.733723 16.20486 286.221 \n",
      "Testing error:  35.82484 33.76299 1.183126 5.311249 13.28045 11.56558 22.20365 31.43464 0.2920659 0.09908222 0.1553323 3.28522 3.350251 5.50162 5.820739 0.5619862 0.5371546 1.028465 0.2455652 44.7037 6.390044 160.9262 4.92637 3.914125 10.04049 28.9921 28.37554 5.87695 1.701723 13.33153 1.317564 14.36179 27.25109 0.3623535 83.30343 6.148311 0.2699139 2.056144 2.979621 4.678401 5.885488 5.508604 193.0659 13.02924 1.589792 10.7462 1.619512 22.99527 242.9372 \n",
      "total training time:  30.84575 \n",
      "Training error:  127.2699 113.5674 2.171565 6.014871 6.642752 120.9974 28.9218 13.31708 3.357528 0.6032143 0.9622837 1.493697 0.02397151 3.831629 6.976987 0.9488489 1.670368 2.23438 1.272966 176.0807 2.778081 274.5129 13.45948 3.259448 5.409701 112.8623 36.85887 7.815794 1.633402 0.1756187 0.2491912 14.97025 2.841694 2.135357 376.5289 3.839202 0.5975813 18.37605 1.709777 11.59655 20.9054 7.442914 892.9871 21.35226 0.1151227 101.615 1.733723 16.20486 286.221 \n",
      "Testing error:  35.82484 33.76299 1.183126 5.311249 13.28045 11.56558 22.20365 31.41538 0.2920659 0.09908222 0.1553323 3.28522 5.641932 5.50162 5.820739 0.5619862 0.5371546 1.028465 0.2455652 44.7037 5.983353 160.9262 8.050334 3.914125 10.04049 28.9921 145.172 5.87695 2.112559 41.26478 1.318108 14.36179 27.25109 0.3623535 83.30343 6.07548 0.2699139 2.056144 2.550981 4.678401 5.885488 5.508604 193.0659 13.02924 1.314045 10.7462 1.619512 22.99527 242.9372 \n",
      "Training error:  6.774279 6.504052 0.2698568 1.115174 0.3083249 6.41091 0.2950355 0.4173879 0.4192236 0.2497061 0.3749157 0.3401318 0.8840525 0.2784049 0.3915132 0.3926592 0.3448422 0.2107756 0.1428851 2.810427 1.348805 9.726974e+27 1.449386 0.7058143 1.03949 3.227394 2.913356 1.589087 0.968214 0.4660867 0.2027589 2.810816 0.255907 1.469162 8.911053 0.3201602 0.3803077 0.9798259 0.80645 2.743261 3.88567 0.2819741 0.5402241 1.270159 0.3868615 6.40153 0.133836 0.2024509 13.08683 \n",
      "Testing error:  148.5327 150.5142 1.888704 8.341842 15.56799 99.38361 176.6007 46.79536 1.58058 0.1245668 0.2677074 3.031716 4.596201 6.283739 8.315412 0.6825156 0.8053404 1.955306 0.6352125 251.1596 6.610879 1.26722e+31 9.020244 5.773354 11.39671 186.4371 108.4961 7.977926 1.845872 52.82923 1.324091 22.68416 25.97725 0.4818183 551.3633 6.605433 0.3957186 10.27739 3.136982 8.758053 15.91895 6.281399 864.7178 31.69241 1.253728 71.53124 2.171307 22.8006 376.5846 \n",
      "total training time:  30.89473 \n",
      "lambda1= 28.34012 ; lambda2= 0.01288027 ; avg tune err= 2.586163e+29 \n",
      "Training error:  127.2699 113.5674 2.171565 6.014871 6.642752 120.9974 28.9218 13.31708 3.357528 0.6032143 0.9622837 1.493697 0.02397151 3.831629 6.976987 0.9488489 1.670368 2.23438 1.272966 176.0807 2.778081 274.5129 13.45948 3.259448 5.409701 112.8623 36.85887 7.815794 1.633402 0.1756187 0.2491912 14.97025 2.841694 2.135357 376.5289 3.839202 0.5975813 18.37605 1.709777 11.59655 20.9054 7.442914 892.9871 21.35226 0.1151227 101.615 1.733723 16.20486 286.221 \n",
      "Testing error:  35.82484 33.76299 1.183126 5.311249 13.28045 11.56558 22.20365 31.41538 0.2920659 0.09908222 0.1553323 3.28522 5.641932 5.50162 5.820739 0.5619862 0.5371546 1.028465 0.2455652 44.7037 5.983353 160.9262 8.050334 3.914125 10.04049 28.9921 145.172 5.87695 2.112559 41.26478 1.318108 14.36179 27.25109 0.3623535 83.30343 6.07548 0.2699139 2.056144 2.550981 4.678401 5.885488 5.508604 193.0659 13.02924 1.314045 10.7462 1.619512 22.99527 242.9372 \n",
      "Training error:  127.2699 113.5674 2.171565 6.014871 6.642752 120.9974 28.9218 13.44795 3.357528 0.6032143 0.9622837 1.493697 5.230764 3.831629 6.976987 0.9488489 1.670368 2.23438 1.272966 176.0807 5.279947 274.5129 16.51823 3.259448 5.409701 112.8623 118.9388 7.815794 2.366977 107.6489 0.2561332 14.97025 2.841694 2.135357 376.5289 6.67679 0.5975813 18.37605 2.964214 11.59655 20.9054 7.442914 892.9871 21.35226 0.7855478 101.615 1.733723 16.20486 286.221 \n",
      "Testing error:  35.82484 33.76299 1.183126 5.311249 13.28045 11.56558 22.20365 31.43464 0.2920659 0.09908222 0.1553323 3.28522 3.350251 5.50162 5.820739 0.5619862 0.5371546 1.028465 0.2455652 44.7037 6.390044 160.9262 4.92637 3.914125 10.04049 28.9921 28.37554 5.87695 1.701723 13.33153 1.317564 14.36179 27.25109 0.3623535 83.30343 6.148311 0.2699139 2.056144 2.979621 4.678401 5.885488 5.508604 193.0659 13.02924 1.589792 10.7462 1.619512 22.99527 242.9372 \n",
      "total training time:  30.69616 \n",
      "Training error:  127.2699 113.5674 2.171565 6.014871 6.642752 120.9974 28.9218 13.31708 3.357528 0.6032143 0.9622837 1.493697 0.02397151 3.831629 6.976987 0.9488489 1.670368 2.23438 1.272966 176.0807 2.778081 274.5129 13.45948 3.259448 5.409701 112.8623 36.85887 7.815794 1.633402 0.1756187 0.2491912 14.97025 2.841694 2.135357 376.5289 3.839202 0.5975813 18.37605 1.709777 11.59655 20.9054 7.442914 892.9871 21.35226 0.1151227 101.615 1.733723 16.20486 286.221 \n",
      "Testing error:  35.82484 33.76299 1.183126 5.311249 13.28045 11.56558 22.20365 31.41538 0.2920659 0.09908222 0.1553323 3.28522 5.641932 5.50162 5.820739 0.5619862 0.5371546 1.028465 0.2455652 44.7037 5.983353 160.9262 8.050334 3.914125 10.04049 28.9921 145.172 5.87695 2.112559 41.26478 1.318108 14.36179 27.25109 0.3623535 83.30343 6.07548 0.2699139 2.056144 2.550981 4.678401 5.885488 5.508604 193.0659 13.02924 1.314045 10.7462 1.619512 22.99527 242.9372 \n",
      "Training error:  11.23835 11.05379 0.5026266 1.861025 0.6542248 9.99175 0.6730272 0.9993083 0.7789006 0.4757614 0.6697582 0.6442359 1.681988 0.6698106 0.7437514 0.667592 0.683236 0.44671 0.3076633 5.70508 2.195711 1.069117e+28 2.774327 1.337531 1.608221 5.492011 5.245702 2.732808 1.556547 1.031431 0.2561332 4.395796 0.4878561 1.968325 14.55925 0.6645868 0.5606615 1.6938 1.291691 4.334881 5.905118 0.6871586 1.267332 2.177075 0.5474143 10.05487 0.2977302 0.4556793 22.16097 \n",
      "Testing error:  133.772 130.7526 1.604943 8.148001 14.58934 84.78637 53.35998 44.59087 0.9104333 0.1027805 0.1804943 2.921428 4.453448 6.263495 6.436434 0.5491139 0.724315 1.858078 0.4260138 222.2337 5.694836 1.392835e+31 7.860199 5.129734 10.47706 165.0982 92.83579 7.587612 1.793275 49.52649 1.317564 20.65654 26.74554 0.3918626 512.7591 6.095722 0.2935709 8.649464 2.844235 6.804644 13.2376 5.341643 840.7815 30.15504 1.32314 65.79871 2.223167 24.30906 380.3175 \n",
      "total training time:  30.66804 \n",
      "lambda1= 42.50373 ; lambda2= 0.01288027 ; avg tune err= 2.84252e+29 \n",
      "Training error:  127.2699 113.5674 2.171565 6.014871 6.642752 120.9974 28.9218 13.31708 3.357528 0.6032143 0.9622837 1.493697 0.02397151 3.831629 6.976987 0.9488489 1.670368 2.23438 1.272966 176.0807 2.778081 274.5129 13.45948 3.259448 5.409701 112.8623 36.85887 7.815794 1.633402 0.1756187 0.2491912 14.97025 2.841694 2.135357 376.5289 3.839202 0.5975813 18.37605 1.709777 11.59655 20.9054 7.442914 892.9871 21.35226 0.1151227 101.615 1.733723 16.20486 286.221 \n",
      "Testing error:  35.82484 33.76299 1.183126 5.311249 13.28045 11.56558 22.20365 31.41538 0.2920659 0.09908222 0.1553323 3.28522 5.641932 5.50162 5.820739 0.5619862 0.5371546 1.028465 0.2455652 44.7037 5.983353 160.9262 8.050334 3.914125 10.04049 28.9921 145.172 5.87695 2.112559 41.26478 1.318108 14.36179 27.25109 0.3623535 83.30343 6.07548 0.2699139 2.056144 2.550981 4.678401 5.885488 5.508604 193.0659 13.02924 1.314045 10.7462 1.619512 22.99527 242.9372 \n",
      "Training error:  127.2699 113.5674 2.171565 6.014871 6.642752 120.9974 28.9218 13.44795 3.357528 0.6032143 0.9622837 1.493697 5.230764 3.831629 6.976987 0.9488489 1.670368 2.23438 1.272966 176.0807 5.279947 274.5129 16.51823 3.259448 5.409701 112.8623 118.9388 7.815794 2.366977 107.6489 0.2561332 14.97025 2.841694 2.135357 376.5289 6.67679 0.5975813 18.37605 2.964214 11.59655 20.9054 7.442914 892.9871 21.35226 0.7855478 101.615 1.733723 16.20486 286.221 \n",
      "Testing error:  35.82484 33.76299 1.183126 5.311249 13.28045 11.56558 22.20365 31.43464 0.2920659 0.09908222 0.1553323 3.28522 3.350251 5.50162 5.820739 0.5619862 0.5371546 1.028465 0.2455652 44.7037 6.390044 160.9262 4.92637 3.914125 10.04049 28.9921 28.37554 5.87695 1.701723 13.33153 1.317564 14.36179 27.25109 0.3623535 83.30343 6.148311 0.2699139 2.056144 2.979621 4.678401 5.885488 5.508604 193.0659 13.02924 1.589792 10.7462 1.619512 22.99527 242.9372 \n",
      "total training time:  30.70794 \n",
      "Training error:  127.2699 113.5674 2.171565 6.014871 6.642752 120.9974 28.9218 13.31708 3.357528 0.6032143 0.9622837 1.493697 0.02397151 3.831629 6.976987 0.9488489 1.670368 2.23438 1.272966 176.0807 2.778081 274.5129 13.45948 3.259448 5.409701 112.8623 36.85887 7.815794 1.633402 0.1756187 0.2491912 14.97025 2.841694 2.135357 376.5289 3.839202 0.5975813 18.37605 1.709777 11.59655 20.9054 7.442914 892.9871 21.35226 0.1151227 101.615 1.733723 16.20486 286.221 \n",
      "Testing error:  35.82484 33.76299 1.183126 5.311249 13.28045 11.56558 22.20365 31.41538 0.2920659 0.09908222 0.1553323 3.28522 5.641932 5.50162 5.820739 0.5619862 0.5371546 1.028465 0.2455652 44.7037 5.983353 160.9262 8.050334 3.914125 10.04049 28.9921 145.172 5.87695 2.112559 41.26478 1.318108 14.36179 27.25109 0.3623535 83.30343 6.07548 0.2699139 2.056144 2.550981 4.678401 5.885488 5.508604 193.0659 13.02924 1.314045 10.7462 1.619512 22.99527 242.9372 \n",
      "Training error:  16.01031 15.68571 0.8273099 2.524762 1.182213 13.94475 0.8801549 1.700921 1.324976 0.5897476 0.8930913 1.000415 2.405953 1.140021 1.169068 0.8720988 1.07965 0.7195027 0.5561298 8.655331 2.945121 1.204503e+26 3.959661 1.933625 2.304997 8.18352 7.841464 3.961488 2.051993 1.773761 0.2561332 6.197898 0.812206 2.13091 20.71473 1.105618 0.5975813 2.66504 1.808636 6.207864 7.918012 1.216314 2.303545 3.30076 0.6280058 13.35627 0.5057484 0.8066491 32.73527 \n",
      "Testing error:  113.5322 112.8739 1.261438 7.742176 14.80543 74.6321 35.71962 41.83054 0.6086676 0.09835915 0.1573907 3.138214 4.475133 6.437407 5.480769 0.556398 0.6072228 1.585983 0.2987768 203.6873 5.525593 1.569214e+29 7.220981 4.683225 9.872507 138.3276 82.97965 6.955893 1.750872 44.8668 1.317564 19.93406 26.84798 0.3616399 462.2362 5.684939 0.2699139 6.927186 2.737256 6.068458 11.08182 5.7287 791.3803 27.02563 1.411534 58.79829 2.17604 24.25432 381.633 \n",
      "total training time:  30.67018 \n",
      "lambda1= 56.66735 ; lambda2= 0.01288027 ; avg tune err= 3.202478e+27 \n",
      "Training error:  127.2699 113.5674 2.171565 6.014871 6.642752 120.9974 28.9218 13.31708 3.357528 0.6032143 0.9622837 1.493697 0.02397151 3.831629 6.976987 0.9488489 1.670368 2.23438 1.272966 176.0807 2.778081 274.5129 13.45948 3.259448 5.409701 112.8623 36.85887 7.815794 1.633402 0.1756187 0.2491912 14.97025 2.841694 2.135357 376.5289 3.839202 0.5975813 18.37605 1.709777 11.59655 20.9054 7.442914 892.9871 21.35226 0.1151227 101.615 1.733723 16.20486 286.221 \n",
      "Testing error:  35.82484 33.76299 1.183126 5.311249 13.28045 11.56558 22.20365 31.41538 0.2920659 0.09908222 0.1553323 3.28522 5.641932 5.50162 5.820739 0.5619862 0.5371546 1.028465 0.2455652 44.7037 5.983353 160.9262 8.050334 3.914125 10.04049 28.9921 145.172 5.87695 2.112559 41.26478 1.318108 14.36179 27.25109 0.3623535 83.30343 6.07548 0.2699139 2.056144 2.550981 4.678401 5.885488 5.508604 193.0659 13.02924 1.314045 10.7462 1.619512 22.99527 242.9372 \n",
      "Training error:  127.2699 113.5674 2.171565 6.014871 6.642752 120.9974 28.9218 13.44795 3.357528 0.6032143 0.9622837 1.493697 5.230764 3.831629 6.976987 0.9488489 1.670368 2.23438 1.272966 176.0807 5.279947 274.5129 16.51823 3.259448 5.409701 112.8623 118.9388 7.815794 2.366977 107.6489 0.2561332 14.97025 2.841694 2.135357 376.5289 6.67679 0.5975813 18.37605 2.964214 11.59655 20.9054 7.442914 892.9871 21.35226 0.7855478 101.615 1.733723 16.20486 286.221 \n",
      "Testing error:  35.82484 33.76299 1.183126 5.311249 13.28045 11.56558 22.20365 31.43464 0.2920659 0.09908222 0.1553323 3.28522 3.350251 5.50162 5.820739 0.5619862 0.5371546 1.028465 0.2455652 44.7037 6.390044 160.9262 4.92637 3.914125 10.04049 28.9921 28.37554 5.87695 1.701723 13.33153 1.317564 14.36179 27.25109 0.3623535 83.30343 6.148311 0.2699139 2.056144 2.979621 4.678401 5.885488 5.508604 193.0659 13.02924 1.589792 10.7462 1.619512 22.99527 242.9372 \n",
      "total training time:  30.67188 \n",
      "Training error:  127.2699 113.5674 2.171565 6.014871 6.642752 120.9974 28.9218 13.31708 3.357528 0.6032143 0.9622837 1.493697 0.02397151 3.831629 6.976987 0.9488489 1.670368 2.23438 1.272966 176.0807 2.778081 274.5129 13.45948 3.259448 5.409701 112.8623 36.85887 7.815794 1.633402 0.1756187 0.2491912 14.97025 2.841694 2.135357 376.5289 3.839202 0.5975813 18.37605 1.709777 11.59655 20.9054 7.442914 892.9871 21.35226 0.1151227 101.615 1.733723 16.20486 286.221 \n",
      "Testing error:  35.82484 33.76299 1.183126 5.311249 13.28045 11.56558 22.20365 31.41538 0.2920659 0.09908222 0.1553323 3.28522 5.641932 5.50162 5.820739 0.5619862 0.5371546 1.028465 0.2455652 44.7037 5.983353 160.9262 8.050334 3.914125 10.04049 28.9921 145.172 5.87695 2.112559 41.26478 1.318108 14.36179 27.25109 0.3623535 83.30343 6.07548 0.2699139 2.056144 2.550981 4.678401 5.885488 5.508604 193.0659 13.02924 1.314045 10.7462 1.619512 22.99527 242.9372 \n",
      "Training error:  16.01031 15.68571 0.8273099 2.524762 1.182213 13.94475 0.8801549 1.700921 1.324976 0.5897476 0.8930913 1.000415 2.405953 1.140021 1.169068 0.8720988 1.07965 0.7195027 0.5561298 8.655331 2.945121 1.204503e+26 3.959661 1.933625 2.304997 8.18352 7.841464 3.961488 2.051993 1.773761 0.2561332 6.197898 0.812206 2.13091 20.71473 1.105618 0.5975813 2.66504 1.808636 6.207864 7.918012 1.216314 2.303545 3.30076 0.6280058 13.35627 0.5057484 0.8066491 32.73527 \n",
      "Testing error:  113.5322 112.8739 1.261438 7.742176 14.80543 74.6321 35.71962 41.83054 0.6086676 0.09835915 0.1573907 3.138214 4.475133 6.437407 5.480769 0.556398 0.6072228 1.585983 0.2987768 203.6873 5.525593 1.569214e+29 7.220981 4.683225 9.872507 138.3276 82.97965 6.955893 1.750872 44.8668 1.317564 19.93406 26.84798 0.3616399 462.2362 5.684939 0.2699139 6.927186 2.737256 6.068458 11.08182 5.7287 791.3803 27.02563 1.411534 58.79829 2.17604 24.25432 381.633 \n",
      "total training time:  30.67469 \n",
      "Time difference of 19.36943 mins\n",
      "Training error:  32.47384 26.31916 1.07959 16.22826 5.769298 9.902972 33.30432 13.35569 0.3103787 0.1594352 0.2933791 0.4367801 2.844662 3.421289 1.963668 0.5461246 0.2369287 2.024403 1.223118 29.4907 1.877106 0.6132215 7.90287 3.047561 4.29399 48.40723 19.49457 7.539764 1.365264 21.76735 0.2705567 15.57849 2.728027 0.6230284 47.50109 5.433645 0.487314 17.99567 1.916921 4.064531 8.282792 2.639869 169.8939 112.0607 0.3068364 15.46764 3.445135 1.808678 315.1736 \n",
      "Testing error:  297.8405 280.1525 4.135434 7.151198 6.390767 331.2819 15.71518 20.34807 8.85815 1.3731 2.156478 2.972342 9.767823 4.3931 13.48113 1.880652 4.295175 1.664474 0.6215873 441.5587 3.852687 290.7337 29.32061 2.08691 7.254257 255.2916 291.8851 4.589224 4.351107 327.4659 0.2783936 11.89913 2.945565 4.745387 1087.951 8.9003 0.484244 3.346638 3.936791 28.78494 53.13035 11.32082 2350.805 29.5637 0.5979249 263.7807 0.9589293 22.96768 371.9627 \n",
      "Training error:  0.08205591 0.06712015 2.599238e-06 0.001231451 1.943595e-06 0.02412418 2.6813e-07 1.51104e-05 1.289727e-05 2.227102e-06 1.008789e-06 6.918925e-07 5.330721e-07 1.051762e-06 0.001211772 6.153214e-07 2.165498e-07 2.177361e-07 1.888707e+17 0.004561201 0.0003487197 1.18313e+30 0.0004791571 0.0002961936 0.0005558466 0.01626043 0.006384213 0.003686052 0.0002302441 2.268455e-08 366149.4 0.01425884 5.262365e+20 0.008909371 0.14404 1.615926e-07 0.0007376612 0.0005442326 1.136151e-06 0.01759412 0.03602398 0.0001177429 9.614229e-05 0.0005462616 9.253223e-06 0.1088507 5.782692e-08 0.01240361 1.477354 \n",
      "Testing error:  277.7271 326.8953 6.342475 22.7526 12.52588 335.0889 215.6147 34.16493 8.490979 1.743506 2.230594 3.177955 12.03681 8.582695 18.36285 2.707857 4.772966 3.607689 1.463507e+18 540.8696 5.572414 1.527726e+34 34.52803 7.631939 11.50287 295.4245 328.6103 15.13747 5.748772 821.3343 5.405562e+12 23.35274 2.211561e+24 5.494904 1378.483 16.96767 0.6316344 26.06287 6.873915 33.57324 62.87302 37427.88 2628.807 78.86691 1.000713 301.804 3.37696 26.00106 606.4526 \n",
      "total training time:  31.41354 \n",
      "lambda1= 0.009171611 ; lambda2= 0.009171611 ; avg tune err= 3.117809e+32 \n",
      "Training error:  32.47384 26.31916 1.07959 16.22826 5.769298 9.902972 33.30432 13.35569 0.3103787 0.1594352 0.2933791 0.4367801 2.844662 3.421289 1.963668 0.5461246 0.2369287 2.024403 1.223118 29.4907 1.877106 0.6132215 7.90287 3.047561 4.29399 48.40723 19.49457 7.539764 1.365264 21.76735 0.2705567 15.57849 2.728027 0.6230284 47.50109 5.433645 0.487314 17.99567 1.916921 4.064531 8.282792 2.639869 169.8939 112.0607 0.3068364 15.46764 3.445135 1.808678 315.1736 \n",
      "Testing error:  297.8405 280.1525 4.135434 7.151198 6.390767 331.2819 15.71518 20.34807 8.85815 1.3731 2.156478 2.972342 9.767823 4.3931 13.48113 1.880652 4.295175 1.664474 0.6215873 441.5587 3.852687 290.7337 29.32061 2.08691 7.254257 255.2916 291.8851 4.589224 4.351107 327.4659 0.2783936 11.89913 2.945565 4.745387 1087.951 8.9003 0.484244 3.346638 3.936791 28.78494 53.13035 11.32082 2350.805 29.5637 0.5979249 263.7807 0.9589293 22.96768 371.9627 \n",
      "Training error:  32.34599 26.72339 1.051242 16.11606 5.887033 10.14789 32.85206 14.57747 0.4204714 0.191732 0.291069 0.4279686 2.83705 3.412275 4.440294 0.5362996 0.2334863 2.001873 1.20837 29.55538 5.886208 170.9717 7.88817 3.013762 4.225712 48.14193 19.4241 7.490203 1.353178 21.23557 0.2657919 15.46633 5.181957 0.6221134 47.69865 5.404872 0.687551 17.96327 2.189518 4.048183 8.37401 5.723677 163.9516 109.9958 0.7371326 26.40809 3.420549 11.41478 311.8582 \n",
      "Testing error:  297.6943 280.0736 4.097915 7.134489 6.309387 330.6094 15.55285 19.41305 8.895462 1.385647 2.156191 3.01025 9.781404 4.403275 12.31404 1.8842 4.313655 1.653099 0.6628295 441.0027 3.616992 235.9565 29.27232 2.0992 7.392445 254.6437 291.4534 4.588526 4.363169 323.8928 0.2805978 11.95092 2.034692 4.74251 1087.862 8.959407 0.3420473 3.375821 3.902391 28.72947 52.95638 9.665539 2336.472 31.15179 0.9267565 250.8248 0.9756106 23.52011 374.4294 \n",
      "total training time:  30.64396 \n",
      "lambda1= 0.009171611 ; lambda2= 7.222073 ; avg tune err= 139.6475 \n",
      "Training error:  32.47384 26.31916 1.07959 16.22826 5.769298 9.902972 33.30432 13.35569 0.3103787 0.1594352 0.2933791 0.4367801 2.844662 3.421289 1.963668 0.5461246 0.2369287 2.024403 1.223118 29.4907 1.877106 0.6132215 7.90287 3.047561 4.29399 48.40723 19.49457 7.539764 1.365264 21.76735 0.2705567 15.57849 2.728027 0.6230284 47.50109 5.433645 0.487314 17.99567 1.916921 4.064531 8.282792 2.639869 169.8939 112.0607 0.3068364 15.46764 3.445135 1.808678 315.1736 \n",
      "Testing error:  297.8405 280.1525 4.135434 7.151198 6.390767 331.2819 15.71518 20.34807 8.85815 1.3731 2.156478 2.972342 9.767823 4.3931 13.48113 1.880652 4.295175 1.664474 0.6215873 441.5587 3.852687 290.7337 29.32061 2.08691 7.254257 255.2916 291.8851 4.589224 4.351107 327.4659 0.2783936 11.89913 2.945565 4.745387 1087.951 8.9003 0.484244 3.346638 3.936791 28.78494 53.13035 11.32082 2350.805 29.5637 0.5979249 263.7807 0.9589293 22.96768 371.9627 \n",
      "Training error:  32.47384 26.78776 1.07959 16.22826 5.961284 10.22873 33.30432 15.0672 0.4218059 0.1941081 0.2933791 0.4367801 2.844662 3.421289 4.481242 0.5461246 0.2369287 2.024403 1.223118 29.70551 5.920099 246.0615 7.90287 3.047561 4.29399 48.40723 19.49457 7.539764 1.365264 21.76735 0.2705567 15.57849 5.27766 0.6230284 47.72941 5.433645 0.6895684 17.99567 2.245171 4.064531 8.401569 5.825545 169.8939 112.0607 0.7381695 26.52767 3.445135 11.42924 315.1736 \n",
      "Testing error:  297.8405 279.8118 4.135434 7.151198 6.339304 330.9759 15.71518 19.89446 8.898257 1.388213 2.156478 2.972342 9.767823 4.3931 12.35155 1.880652 4.295175 1.664474 0.6215873 442.1474 3.606019 206.0452 29.32061 2.08691 7.254257 255.2916 291.8851 4.589224 4.351107 327.4659 0.2783936 11.89913 2.083631 4.745387 1087.976 8.9003 0.3433187 3.346638 3.739164 28.78494 53.07832 9.505595 2350.805 29.5637 0.9272332 251.292 0.9589293 23.48992 371.9627 \n",
      "total training time:  30.66161 \n",
      "Training error:  32.47384 26.31916 1.07959 16.22826 5.769298 9.902972 33.30432 13.35569 0.3103787 0.1594352 0.2933791 0.4367801 2.844662 3.421289 1.963668 0.5461246 0.2369287 2.024403 1.223118 29.4907 1.877106 0.6132215 7.90287 3.047561 4.29399 48.40723 19.49457 7.539764 1.365264 21.76735 0.2705567 15.57849 2.728027 0.6230284 47.50109 5.433645 0.487314 17.99567 1.916921 4.064531 8.282792 2.639869 169.8939 112.0607 0.3068364 15.46764 3.445135 1.808678 315.1736 \n",
      "Testing error:  297.8405 280.1525 4.135434 7.151198 6.390767 331.2819 15.71518 20.34807 8.85815 1.3731 2.156478 2.972342 9.767823 4.3931 13.48113 1.880652 4.295175 1.664474 0.6215873 441.5587 3.852687 290.7337 29.32061 2.08691 7.254257 255.2916 291.8851 4.589224 4.351107 327.4659 0.2783936 11.89913 2.945565 4.745387 1087.951 8.9003 0.484244 3.346638 3.936791 28.78494 53.13035 11.32082 2350.805 29.5637 0.5979249 263.7807 0.9589293 22.96768 371.9627 \n",
      "Training error:  0.919459 0.865049 0.05331733 0.161687 0.03617683 0.5592443 0.02874564 0.04045826 0.03551536 0.0285442 0.03434395 0.03627682 0.03365924 0.04044239 0.05936424 0.03658493 0.020943 0.01893587 0.0122551 0.2942441 0.1370036 0.0274657 0.1704364 0.1398675 0.1731065 0.4944555 0.360673 0.2602564 0.1523276 0.008469625 0.03131596 0.3872121 0.03553383 0.2432741 0.9724007 0.0273485 0.07714598 0.1257428 0.04536863 0.4910392 0.5856434 0.03074372 0.09605364 0.1652333 0.07288073 1.036791 0.01467986 0.06308532 3.360692 \n",
      "Testing error:  285.9244 307.6977 4.162478 21.8051 10.99805 316.9091 95.27532 23.26096 8.627582 1.37724 2.076864 3.021806 11.99168 6.504488 14.6443 1.557898 3.98356 2.501002 1.226981 507.1988 4.761185 312.2568 34.52411 3.73671 10.80549 289.7128 296.7734 10.72771 4.364503 460.6693 0.3335008 23.33141 3.104193 4.71463 1333.389 14.31536 0.7180544 19.5223 5.303296 27.87463 55.14382 13.01916 2588.502 63.3208 0.4602765 275.2129 1.750331 25.40988 551.1546 \n",
      "total training time:  30.87959 \n",
      "lambda1= 7.222073 ; lambda2= 0.009171611 ; avg tune err= 164.6052 \n",
      "Training error:  32.47384 26.31916 1.07959 16.22826 5.769298 9.902972 33.30432 13.35569 0.3103787 0.1594352 0.2933791 0.4367801 2.844662 3.421289 1.963668 0.5461246 0.2369287 2.024403 1.223118 29.4907 1.877106 0.6132215 7.90287 3.047561 4.29399 48.40723 19.49457 7.539764 1.365264 21.76735 0.2705567 15.57849 2.728027 0.6230284 47.50109 5.433645 0.487314 17.99567 1.916921 4.064531 8.282792 2.639869 169.8939 112.0607 0.3068364 15.46764 3.445135 1.808678 315.1736 \n",
      "Testing error:  297.8405 280.1525 4.135434 7.151198 6.390767 331.2819 15.71518 20.34807 8.85815 1.3731 2.156478 2.972342 9.767823 4.3931 13.48113 1.880652 4.295175 1.664474 0.6215873 441.5587 3.852687 290.7337 29.32061 2.08691 7.254257 255.2916 291.8851 4.589224 4.351107 327.4659 0.2783936 11.89913 2.945565 4.745387 1087.951 8.9003 0.484244 3.346638 3.936791 28.78494 53.13035 11.32082 2350.805 29.5637 0.5979249 263.7807 0.9589293 22.96768 371.9627 \n",
      "Training error:  32.36089 26.73233 1.056678 16.13064 5.89703 10.15714 32.89148 14.61583 0.4215462 0.1932659 0.292618 0.43093 2.840206 3.414946 4.447356 0.5392287 0.2352197 2.00557 1.211493 29.56896 5.892983 172.8137 7.892 3.019434 4.233355 48.16386 19.42992 7.497324 1.356615 21.28373 0.2674876 15.47624 5.193796 0.6229865 47.70557 5.410357 0.6892753 17.96967 2.198323 4.052298 8.379478 5.734832 164.2601 110.1025 0.73769 26.4232 3.423131 11.41928 311.9906 \n",
      "Testing error:  297.7052 280.0458 4.104548 7.13707 6.312291 330.6649 15.55949 19.44536 8.896891 1.385826 2.154781 2.995833 9.773812 4.400208 12.32381 1.883582 4.304848 1.655446 0.6526685 441.1272 3.614507 234.0895 29.28124 2.095927 7.371377 254.6995 291.5122 4.589184 4.359617 324.2283 0.279803 11.94326 2.038123 4.745241 1087.931 8.951771 0.3431384 3.369664 3.872224 28.74558 52.98415 9.64942 2337.29 31.06208 0.9270002 250.8916 0.970739 23.51459 374.3615 \n",
      "total training time:  30.6983 \n",
      "lambda1= 7.222073 ; lambda2= 7.222073 ; avg tune err= 139.6376 \n",
      "Training error:  32.47384 26.31916 1.07959 16.22826 5.769298 9.902972 33.30432 13.35569 0.3103787 0.1594352 0.2933791 0.4367801 2.844662 3.421289 1.963668 0.5461246 0.2369287 2.024403 1.223118 29.4907 1.877106 0.6132215 7.90287 3.047561 4.29399 48.40723 19.49457 7.539764 1.365264 21.76735 0.2705567 15.57849 2.728027 0.6230284 47.50109 5.433645 0.487314 17.99567 1.916921 4.064531 8.282792 2.639869 169.8939 112.0607 0.3068364 15.46764 3.445135 1.808678 315.1736 \n",
      "Testing error:  297.8405 280.1525 4.135434 7.151198 6.390767 331.2819 15.71518 20.34807 8.85815 1.3731 2.156478 2.972342 9.767823 4.3931 13.48113 1.880652 4.295175 1.664474 0.6215873 441.5587 3.852687 290.7337 29.32061 2.08691 7.254257 255.2916 291.8851 4.589224 4.351107 327.4659 0.2783936 11.89913 2.945565 4.745387 1087.951 8.9003 0.484244 3.346638 3.936791 28.78494 53.13035 11.32082 2350.805 29.5637 0.5979249 263.7807 0.9589293 22.96768 371.9627 \n",
      "Training error:  32.47384 26.78776 1.07959 16.22826 5.961284 10.22873 33.30432 15.0672 0.4218059 0.1941081 0.2933791 0.4367801 2.844662 3.421289 4.481242 0.5461246 0.2369287 2.024403 1.223118 29.70551 5.920099 246.0615 7.90287 3.047561 4.29399 48.40723 19.49457 7.539764 1.365264 21.76735 0.2705567 15.57849 5.27766 0.6230284 47.72941 5.433645 0.6895684 17.99567 2.245171 4.064531 8.401569 5.825545 169.8939 112.0607 0.7381695 26.52767 3.445135 11.42924 315.1736 \n",
      "Testing error:  297.8405 279.8118 4.135434 7.151198 6.339304 330.9759 15.71518 19.89446 8.898257 1.388213 2.156478 2.972342 9.767823 4.3931 12.35155 1.880652 4.295175 1.664474 0.6215873 442.1474 3.606019 206.0452 29.32061 2.08691 7.254257 255.2916 291.8851 4.589224 4.351107 327.4659 0.2783936 11.89913 2.083631 4.745387 1087.976 8.9003 0.3433187 3.346638 3.739164 28.78494 53.07832 9.505595 2350.805 29.5637 0.9272332 251.292 0.9589293 23.48992 371.9627 \n",
      "total training time:  30.90751 \n",
      "Training error:  32.47384 26.31916 1.07959 16.22826 5.769298 9.902972 33.30432 13.35569 0.3103787 0.1594352 0.2933791 0.4367801 2.844662 3.421289 1.963668 0.5461246 0.2369287 2.024403 1.223118 29.4907 1.877106 0.6132215 7.90287 3.047561 4.29399 48.40723 19.49457 7.539764 1.365264 21.76735 0.2705567 15.57849 2.728027 0.6230284 47.50109 5.433645 0.487314 17.99567 1.916921 4.064531 8.282792 2.639869 169.8939 112.0607 0.3068364 15.46764 3.445135 1.808678 315.1736 \n",
      "Testing error:  297.8405 280.1525 4.135434 7.151198 6.390767 331.2819 15.71518 20.34807 8.85815 1.3731 2.156478 2.972342 9.767823 4.3931 13.48113 1.880652 4.295175 1.664474 0.6215873 441.5587 3.852687 290.7337 29.32061 2.08691 7.254257 255.2916 291.8851 4.589224 4.351107 327.4659 0.2783936 11.89913 2.945565 4.745387 1087.951 8.9003 0.484244 3.346638 3.936791 28.78494 53.13035 11.32082 2350.805 29.5637 0.5979249 263.7807 0.9589293 22.96768 371.9627 \n",
      "Training error:  2.314676 1.904917 0.1434987 0.4627061 0.1168181 1.335434 0.09482762 0.1402474 0.1182989 0.09818416 0.1002094 0.1035774 0.09212548 0.1090587 0.1802686 0.12007 0.07335289 0.0775049 0.04361655 0.8401499 0.3573279 0.1063009 0.4237644 0.3455952 0.4042634 1.161868 0.9680124 0.6801823 0.4245793 0.02498756 0.09305682 1.030965 0.1201888 0.4440397 2.110569 0.08420929 0.2114557 0.3551383 0.1333654 1.020361 1.337808 0.1049133 0.2921589 0.4804645 0.1894145 2.270875 0.05881857 0.1936373 5.765299 \n",
      "Testing error:  286.16 301.9545 3.920295 20.11896 11.91398 316.835 81.95522 22.69061 8.577446 1.380844 2.197195 3.119922 11.84443 4.729033 13.37393 1.666279 4.013346 2.565224 1.490978 488.3644 5.157933 317.8967 35.25841 3.071196 10.17938 285.3128 285.725 10.09923 4.249976 352.2449 0.3307209 22.53939 2.489096 4.75535 1316.445 13.27087 0.560224 15.83607 4.981454 27.95121 54.99807 13.09332 2584.932 58.41909 0.4836104 254.8382 1.582407 25.00307 516.6668 \n",
      "total training time:  30.6548 \n",
      "lambda1= 14.43497 ; lambda2= 0.009171611 ; avg tune err= 159.5356 \n",
      "Training error:  32.47384 26.31916 1.07959 16.22826 5.769298 9.902972 33.30432 13.35569 0.3103787 0.1594352 0.2933791 0.4367801 2.844662 3.421289 1.963668 0.5461246 0.2369287 2.024403 1.223118 29.4907 1.877106 0.6132215 7.90287 3.047561 4.29399 48.40723 19.49457 7.539764 1.365264 21.76735 0.2705567 15.57849 2.728027 0.6230284 47.50109 5.433645 0.487314 17.99567 1.916921 4.064531 8.282792 2.639869 169.8939 112.0607 0.3068364 15.46764 3.445135 1.808678 315.1736 \n",
      "Testing error:  297.8405 280.1525 4.135434 7.151198 6.390767 331.2819 15.71518 20.34807 8.85815 1.3731 2.156478 2.972342 9.767823 4.3931 13.48113 1.880652 4.295175 1.664474 0.6215873 441.5587 3.852687 290.7337 29.32061 2.08691 7.254257 255.2916 291.8851 4.589224 4.351107 327.4659 0.2783936 11.89913 2.945565 4.745387 1087.951 8.9003 0.484244 3.346638 3.936791 28.78494 53.13035 11.32082 2350.805 29.5637 0.5979249 263.7807 0.9589293 22.96768 371.9627 \n",
      "Training error:  32.37523 26.7411 1.062141 16.14418 5.906804 10.16629 32.92913 14.65256 0.4218059 0.1941081 0.2933791 0.4339979 2.842895 3.417404 4.454041 0.5420795 0.2367666 2.009437 1.214614 29.58225 5.899646 174.6015 7.895457 3.024998 4.240878 48.18393 19.43595 7.504171 1.360179 21.32926 0.2691462 15.4859 5.205373 0.6230284 47.71188 5.415909 0.6895684 17.97594 2.206915 4.056432 8.384952 5.745838 164.5573 110.2054 0.7381695 26.43781 3.426154 11.42384 312.118 \n",
      "Testing error:  297.7143 280.0183 4.111329 7.138576 6.315547 330.7168 15.56628 19.47689 8.898257 1.388213 2.156478 2.982698 9.767336 4.398002 12.33334 1.88208 4.29595 1.657561 0.6418675 441.2443 3.612698 232.3167 29.29541 2.093381 7.353232 254.7469 291.5572 4.588931 4.356302 324.5455 0.2792811 11.93721 2.042168 4.745387 1087.958 8.943458 0.3433187 3.364001 3.843976 28.76066 53.01041 9.633563 2338.067 30.97752 0.9272332 250.9545 0.9680377 23.51014 374.2965 \n",
      "total training time:  30.93589 \n",
      "lambda1= 14.43497 ; lambda2= 7.222073 ; avg tune err= 139.6272 \n",
      "Training error:  32.47384 26.31916 1.07959 16.22826 5.769298 9.902972 33.30432 13.35569 0.3103787 0.1594352 0.2933791 0.4367801 2.844662 3.421289 1.963668 0.5461246 0.2369287 2.024403 1.223118 29.4907 1.877106 0.6132215 7.90287 3.047561 4.29399 48.40723 19.49457 7.539764 1.365264 21.76735 0.2705567 15.57849 2.728027 0.6230284 47.50109 5.433645 0.487314 17.99567 1.916921 4.064531 8.282792 2.639869 169.8939 112.0607 0.3068364 15.46764 3.445135 1.808678 315.1736 \n",
      "Testing error:  297.8405 280.1525 4.135434 7.151198 6.390767 331.2819 15.71518 20.34807 8.85815 1.3731 2.156478 2.972342 9.767823 4.3931 13.48113 1.880652 4.295175 1.664474 0.6215873 441.5587 3.852687 290.7337 29.32061 2.08691 7.254257 255.2916 291.8851 4.589224 4.351107 327.4659 0.2783936 11.89913 2.945565 4.745387 1087.951 8.9003 0.484244 3.346638 3.936791 28.78494 53.13035 11.32082 2350.805 29.5637 0.5979249 263.7807 0.9589293 22.96768 371.9627 \n",
      "Training error:  32.47384 26.78776 1.07959 16.22826 5.961284 10.22873 33.30432 15.0672 0.4218059 0.1941081 0.2933791 0.4367801 2.844662 3.421289 4.481242 0.5461246 0.2369287 2.024403 1.223118 29.70551 5.920099 246.0615 7.90287 3.047561 4.29399 48.40723 19.49457 7.539764 1.365264 21.76735 0.2705567 15.57849 5.27766 0.6230284 47.72941 5.433645 0.6895684 17.99567 2.245171 4.064531 8.401569 5.825545 169.8939 112.0607 0.7381695 26.52767 3.445135 11.42924 315.1736 \n",
      "Testing error:  297.8405 279.8118 4.135434 7.151198 6.339304 330.9759 15.71518 19.89446 8.898257 1.388213 2.156478 2.972342 9.767823 4.3931 12.35155 1.880652 4.295175 1.664474 0.6215873 442.1474 3.606019 206.0452 29.32061 2.08691 7.254257 255.2916 291.8851 4.589224 4.351107 327.4659 0.2783936 11.89913 2.083631 4.745387 1087.976 8.9003 0.3433187 3.346638 3.739164 28.78494 53.07832 9.505595 2350.805 29.5637 0.9272332 251.292 0.9589293 23.48992 371.9627 \n",
      "total training time:  30.65504 \n",
      "Training error:  32.47384 26.31916 1.07959 16.22826 5.769298 9.902972 33.30432 13.35569 0.3103787 0.1594352 0.2933791 0.4367801 2.844662 3.421289 1.963668 0.5461246 0.2369287 2.024403 1.223118 29.4907 1.877106 0.6132215 7.90287 3.047561 4.29399 48.40723 19.49457 7.539764 1.365264 21.76735 0.2705567 15.57849 2.728027 0.6230284 47.50109 5.433645 0.487314 17.99567 1.916921 4.064531 8.282792 2.639869 169.8939 112.0607 0.3068364 15.46764 3.445135 1.808678 315.1736 \n",
      "Testing error:  297.8405 280.1525 4.135434 7.151198 6.390767 331.2819 15.71518 20.34807 8.85815 1.3731 2.156478 2.972342 9.767823 4.3931 13.48113 1.880652 4.295175 1.664474 0.6215873 441.5587 3.852687 290.7337 29.32061 2.08691 7.254257 255.2916 291.8851 4.589224 4.351107 327.4659 0.2783936 11.89913 2.945565 4.745387 1087.951 8.9003 0.484244 3.346638 3.936791 28.78494 53.13035 11.32082 2350.805 29.5637 0.5979249 263.7807 0.9589293 22.96768 371.9627 \n",
      "Training error:  3.732651 3.124161 0.2674135 0.887908 0.2166747 2.157354 0.1997994 0.2984221 0.2063166 0.1570462 0.1792426 0.1811846 0.195769 0.214939 0.3418819 0.2276176 0.135607 0.1622176 0.09104791 1.431907 0.6780796 0.2117101 0.7747578 0.6267414 0.6583266 2.086232 1.729217 1.154164 0.6932257 0.05714307 0.1709035 1.729348 0.2315564 0.5656983 3.562638 0.1844916 0.3405861 0.6633478 0.2572422 1.502019 2.082955 0.2177867 0.5616872 0.894787 0.3163911 3.678962 0.1268164 0.3648401 8.767021 \n",
      "Testing error:  285.0363 301.3744 3.813129 18.91723 11.67503 323.9201 66.50052 21.07158 8.741847 1.375192 2.203034 3.138051 11.54478 4.209134 13.12929 1.841145 4.171296 2.173312 1.400523 478.1584 4.968247 318.9852 34.65629 2.755159 9.665516 277.5707 285.2345 9.186906 4.42761 354.0638 0.3114043 20.72143 2.717076 4.747407 1294.54 11.82219 0.4559525 14.13261 4.605231 28.89484 53.04114 13.66861 2588.075 54.12653 0.5078916 246.7125 1.454354 24.44326 488.4881 \n",
      "total training time:  30.86725 \n",
      "lambda1= 21.64787 ; lambda2= 0.009171611 ; avg tune err= 157.5382 \n",
      "Training error:  32.47384 26.31916 1.07959 16.22826 5.769298 9.902972 33.30432 13.35569 0.3103787 0.1594352 0.2933791 0.4367801 2.844662 3.421289 1.963668 0.5461246 0.2369287 2.024403 1.223118 29.4907 1.877106 0.6132215 7.90287 3.047561 4.29399 48.40723 19.49457 7.539764 1.365264 21.76735 0.2705567 15.57849 2.728027 0.6230284 47.50109 5.433645 0.487314 17.99567 1.916921 4.064531 8.282792 2.639869 169.8939 112.0607 0.3068364 15.46764 3.445135 1.808678 315.1736 \n",
      "Testing error:  297.8405 280.1525 4.135434 7.151198 6.390767 331.2819 15.71518 20.34807 8.85815 1.3731 2.156478 2.972342 9.767823 4.3931 13.48113 1.880652 4.295175 1.664474 0.6215873 441.5587 3.852687 290.7337 29.32061 2.08691 7.254257 255.2916 291.8851 4.589224 4.351107 327.4659 0.2783936 11.89913 2.945565 4.745387 1087.951 8.9003 0.484244 3.346638 3.936791 28.78494 53.13035 11.32082 2350.805 29.5637 0.5979249 263.7807 0.9589293 22.96768 371.9627 \n",
      "Training error:  32.38903 26.74968 1.067599 16.15667 5.916342 10.17533 32.96516 14.68783 0.4218059 0.1941081 0.2933791 0.4363236 2.843862 3.419031 4.460306 0.545025 0.2369287 2.013444 1.217778 29.59526 5.906176 176.3481 7.898614 3.030308 4.248405 48.20279 19.44219 7.510888 1.363831 21.37245 0.2705567 15.4955 5.216676 0.6230284 47.71773 5.421482 0.6895684 17.98209 2.215271 4.060566 8.390399 5.756668 164.8447 110.3049 0.7381695 26.45192 3.429439 11.42745 312.2412 \n",
      "Testing error:  297.722 279.9911 4.118218 7.139761 6.319091 330.7653 15.57326 19.50776 8.898257 1.388213 2.156478 2.973884 9.767591 4.396135 12.34154 1.880963 4.295175 1.659807 0.6326029 441.3546 3.611483 230.6273 29.30797 2.091774 7.336337 254.7943 291.6007 4.588601 4.353185 324.8468 0.2783936 11.93172 2.046733 4.745387 1087.965 8.934494 0.3433187 3.358787 3.817468 28.77504 53.03521 9.617934 2338.812 30.89748 0.9272332 251.0138 0.9660911 23.50455 374.2342 \n",
      "total training time:  30.7417 \n",
      "lambda1= 21.64787 ; lambda2= 7.222073 ; avg tune err= 139.6172 \n",
      "Training error:  32.47384 26.31916 1.07959 16.22826 5.769298 9.902972 33.30432 13.35569 0.3103787 0.1594352 0.2933791 0.4367801 2.844662 3.421289 1.963668 0.5461246 0.2369287 2.024403 1.223118 29.4907 1.877106 0.6132215 7.90287 3.047561 4.29399 48.40723 19.49457 7.539764 1.365264 21.76735 0.2705567 15.57849 2.728027 0.6230284 47.50109 5.433645 0.487314 17.99567 1.916921 4.064531 8.282792 2.639869 169.8939 112.0607 0.3068364 15.46764 3.445135 1.808678 315.1736 \n",
      "Testing error:  297.8405 280.1525 4.135434 7.151198 6.390767 331.2819 15.71518 20.34807 8.85815 1.3731 2.156478 2.972342 9.767823 4.3931 13.48113 1.880652 4.295175 1.664474 0.6215873 441.5587 3.852687 290.7337 29.32061 2.08691 7.254257 255.2916 291.8851 4.589224 4.351107 327.4659 0.2783936 11.89913 2.945565 4.745387 1087.951 8.9003 0.484244 3.346638 3.936791 28.78494 53.13035 11.32082 2350.805 29.5637 0.5979249 263.7807 0.9589293 22.96768 371.9627 \n",
      "Training error:  32.47384 26.78776 1.07959 16.22826 5.961284 10.22873 33.30432 15.0672 0.4218059 0.1941081 0.2933791 0.4367801 2.844662 3.421289 4.481242 0.5461246 0.2369287 2.024403 1.223118 29.70551 5.920099 246.0615 7.90287 3.047561 4.29399 48.40723 19.49457 7.539764 1.365264 21.76735 0.2705567 15.57849 5.27766 0.6230284 47.72941 5.433645 0.6895684 17.99567 2.245171 4.064531 8.401569 5.825545 169.8939 112.0607 0.7381695 26.52767 3.445135 11.42924 315.1736 \n",
      "Testing error:  297.8405 279.8118 4.135434 7.151198 6.339304 330.9759 15.71518 19.89446 8.898257 1.388213 2.156478 2.972342 9.767823 4.3931 12.35155 1.880652 4.295175 1.664474 0.6215873 442.1474 3.606019 206.0452 29.32061 2.08691 7.254257 255.2916 291.8851 4.589224 4.351107 327.4659 0.2783936 11.89913 2.083631 4.745387 1087.976 8.9003 0.3433187 3.346638 3.739164 28.78494 53.07832 9.505595 2350.805 29.5637 0.9272332 251.292 0.9589293 23.48992 371.9627 \n",
      "total training time:  30.78414 \n",
      "Training error:  32.47384 26.31916 1.07959 16.22826 5.769298 9.902972 33.30432 13.35569 0.3103787 0.1594352 0.2933791 0.4367801 2.844662 3.421289 1.963668 0.5461246 0.2369287 2.024403 1.223118 29.4907 1.877106 0.6132215 7.90287 3.047561 4.29399 48.40723 19.49457 7.539764 1.365264 21.76735 0.2705567 15.57849 2.728027 0.6230284 47.50109 5.433645 0.487314 17.99567 1.916921 4.064531 8.282792 2.639869 169.8939 112.0607 0.3068364 15.46764 3.445135 1.808678 315.1736 \n",
      "Testing error:  297.8405 280.1525 4.135434 7.151198 6.390767 331.2819 15.71518 20.34807 8.85815 1.3731 2.156478 2.972342 9.767823 4.3931 13.48113 1.880652 4.295175 1.664474 0.6215873 441.5587 3.852687 290.7337 29.32061 2.08691 7.254257 255.2916 291.8851 4.589224 4.351107 327.4659 0.2783936 11.89913 2.945565 4.745387 1087.951 8.9003 0.484244 3.346638 3.936791 28.78494 53.13035 11.32082 2350.805 29.5637 0.5979249 263.7807 0.9589293 22.96768 371.9627 \n",
      "Training error:  5.156528 4.264524 0.4122538 1.357019 0.3604342 3.04385 0.3503634 0.4908736 0.2618204 0.1855234 0.2579057 0.2688085 0.3184097 0.3508836 0.5343124 0.3346192 0.1848619 0.2785972 0.1618294 1.984666 1.042203 0.3064293 1.171644 0.9336475 0.9450735 3.101158 2.57134 1.715476 0.9058532 0.1124848 0.243771 2.501088 0.3649394 0.6226852 5.267836 0.326901 0.4735432 1.098802 0.3963601 1.981602 2.921695 0.3677554 0.8901551 1.422751 0.4298092 5.121396 0.2181799 0.5920541 12.6124 \n",
      "Testing error:  286.0131 296.2914 3.761611 17.6946 10.88848 326.9685 48.09627 20.2978 8.747369 1.392049 2.176325 3.105932 11.47285 4.244614 13.30742 1.871271 4.268693 1.835085 1.283167 469.2281 4.783879 318.1075 33.90943 2.473428 9.216923 269.4076 284.9157 8.168816 4.42615 355.4766 0.2894164 19.7588 2.997035 4.7448 1259.938 10.53615 0.3928101 12.22826 4.536383 28.97643 53.48225 13.56464 2573.386 53.81638 0.5598623 246.6212 1.527674 22.81899 471.6811 \n",
      "total training time:  30.91519 \n",
      "lambda1= 28.86078 ; lambda2= 0.009171611 ; avg tune err= 155.2181 \n",
      "Training error:  32.47384 26.31916 1.07959 16.22826 5.769298 9.902972 33.30432 13.35569 0.3103787 0.1594352 0.2933791 0.4367801 2.844662 3.421289 1.963668 0.5461246 0.2369287 2.024403 1.223118 29.4907 1.877106 0.6132215 7.90287 3.047561 4.29399 48.40723 19.49457 7.539764 1.365264 21.76735 0.2705567 15.57849 2.728027 0.6230284 47.50109 5.433645 0.487314 17.99567 1.916921 4.064531 8.282792 2.639869 169.8939 112.0607 0.3068364 15.46764 3.445135 1.808678 315.1736 \n",
      "Testing error:  297.8405 280.1525 4.135434 7.151198 6.390767 331.2819 15.71518 20.34807 8.85815 1.3731 2.156478 2.972342 9.767823 4.3931 13.48113 1.880652 4.295175 1.664474 0.6215873 441.5587 3.852687 290.7337 29.32061 2.08691 7.254257 255.2916 291.8851 4.589224 4.351107 327.4659 0.2783936 11.89913 2.945565 4.745387 1087.951 8.9003 0.484244 3.346638 3.936791 28.78494 53.13035 11.32082 2350.805 29.5637 0.5979249 263.7807 0.9589293 22.96768 371.9627 \n",
      "Training error:  32.40229 26.75805 1.073021 16.16822 5.92562 10.18422 32.99962 14.72168 0.4218059 0.1941081 0.2933791 0.4367801 2.844662 3.420215 4.465866 0.5461246 0.2369287 2.017213 1.220427 29.60798 5.912543 178.0572 7.901458 3.035616 4.255905 48.22063 19.44859 7.517521 1.365264 21.41339 0.2705567 15.50503 5.227684 0.6230284 47.72324 5.426816 0.6895684 17.98808 2.223367 4.063804 8.39579 5.767286 165.1231 110.401 0.7381695 26.46552 3.432899 11.42845 312.3604 \n",
      "Testing error:  297.7281 279.9642 4.12518 7.141045 6.322852 330.8106 15.58033 19.53795 8.898257 1.388213 2.156478 2.972342 9.767823 4.394513 12.34601 1.880652 4.295175 1.661695 0.6269609 441.4586 3.610797 229.0122 29.31896 2.090288 7.3206 254.8389 291.6429 4.588467 4.351107 325.133 0.2783936 11.92674 2.051735 4.745387 1087.975 8.921807 0.3433187 3.353981 3.792559 28.78343 53.05862 9.602477 2339.525 30.82151 0.9272332 251.0696 0.9642316 23.49629 374.1742 \n",
      "total training time:  30.60618 \n",
      "lambda1= 28.86078 ; lambda2= 7.222073 ; avg tune err= 139.6076 \n",
      "Training error:  32.47384 26.31916 1.07959 16.22826 5.769298 9.902972 33.30432 13.35569 0.3103787 0.1594352 0.2933791 0.4367801 2.844662 3.421289 1.963668 0.5461246 0.2369287 2.024403 1.223118 29.4907 1.877106 0.6132215 7.90287 3.047561 4.29399 48.40723 19.49457 7.539764 1.365264 21.76735 0.2705567 15.57849 2.728027 0.6230284 47.50109 5.433645 0.487314 17.99567 1.916921 4.064531 8.282792 2.639869 169.8939 112.0607 0.3068364 15.46764 3.445135 1.808678 315.1736 \n",
      "Testing error:  297.8405 280.1525 4.135434 7.151198 6.390767 331.2819 15.71518 20.34807 8.85815 1.3731 2.156478 2.972342 9.767823 4.3931 13.48113 1.880652 4.295175 1.664474 0.6215873 441.5587 3.852687 290.7337 29.32061 2.08691 7.254257 255.2916 291.8851 4.589224 4.351107 327.4659 0.2783936 11.89913 2.945565 4.745387 1087.951 8.9003 0.484244 3.346638 3.936791 28.78494 53.13035 11.32082 2350.805 29.5637 0.5979249 263.7807 0.9589293 22.96768 371.9627 \n",
      "Training error:  32.47384 26.78776 1.07959 16.22826 5.961284 10.22873 33.30432 15.0672 0.4218059 0.1941081 0.2933791 0.4367801 2.844662 3.421289 4.481242 0.5461246 0.2369287 2.024403 1.223118 29.70551 5.920099 246.0615 7.90287 3.047561 4.29399 48.40723 19.49457 7.539764 1.365264 21.76735 0.2705567 15.57849 5.27766 0.6230284 47.72941 5.433645 0.6895684 17.99567 2.245171 4.064531 8.401569 5.825545 169.8939 112.0607 0.7381695 26.52767 3.445135 11.42924 315.1736 \n",
      "Testing error:  297.8405 279.8118 4.135434 7.151198 6.339304 330.9759 15.71518 19.89446 8.898257 1.388213 2.156478 2.972342 9.767823 4.3931 12.35155 1.880652 4.295175 1.664474 0.6215873 442.1474 3.606019 206.0452 29.32061 2.08691 7.254257 255.2916 291.8851 4.589224 4.351107 327.4659 0.2783936 11.89913 2.083631 4.745387 1087.976 8.9003 0.3433187 3.346638 3.739164 28.78494 53.07832 9.505595 2350.805 29.5637 0.9272332 251.292 0.9589293 23.48992 371.9627 \n",
      "total training time:  31.21448 \n",
      "Training error:  32.47384 26.31916 1.07959 16.22826 5.769298 9.902972 33.30432 13.35569 0.3103787 0.1594352 0.2933791 0.4367801 2.844662 3.421289 1.963668 0.5461246 0.2369287 2.024403 1.223118 29.4907 1.877106 0.6132215 7.90287 3.047561 4.29399 48.40723 19.49457 7.539764 1.365264 21.76735 0.2705567 15.57849 2.728027 0.6230284 47.50109 5.433645 0.487314 17.99567 1.916921 4.064531 8.282792 2.639869 169.8939 112.0607 0.3068364 15.46764 3.445135 1.808678 315.1736 \n",
      "Testing error:  297.8405 280.1525 4.135434 7.151198 6.390767 331.2819 15.71518 20.34807 8.85815 1.3731 2.156478 2.972342 9.767823 4.3931 13.48113 1.880652 4.295175 1.664474 0.6215873 441.5587 3.852687 290.7337 29.32061 2.08691 7.254257 255.2916 291.8851 4.589224 4.351107 327.4659 0.2783936 11.89913 2.945565 4.745387 1087.951 8.9003 0.484244 3.346638 3.936791 28.78494 53.13035 11.32082 2350.805 29.5637 0.5979249 263.7807 0.9589293 22.96768 371.9627 \n",
      "Training error:  32.40229 26.75805 1.073021 16.16822 5.92562 10.18422 32.99962 14.72168 0.4218059 0.1941081 0.2933791 0.4367801 2.844662 3.420215 4.465866 0.5461246 0.2369287 2.017213 1.220427 29.60798 5.912543 178.0572 7.901458 3.035616 4.255905 48.22063 19.44859 7.517521 1.365264 21.41339 0.2705567 15.50503 5.227684 0.6230284 47.72324 5.426816 0.6895684 17.98808 2.223367 4.063804 8.39579 5.767286 165.1231 110.401 0.7381695 26.46552 3.432899 11.42845 312.3604 \n",
      "Testing error:  297.7281 279.9642 4.12518 7.141045 6.322852 330.8106 15.58033 19.53795 8.898257 1.388213 2.156478 2.972342 9.767823 4.394513 12.34601 1.880652 4.295175 1.661695 0.6269609 441.4586 3.610797 229.0122 29.31896 2.090288 7.3206 254.8389 291.6429 4.588467 4.351107 325.133 0.2783936 11.92674 2.051735 4.745387 1087.975 8.921807 0.3433187 3.353981 3.792559 28.78343 53.05862 9.602477 2339.525 30.82151 0.9272332 251.0696 0.9642316 23.49629 374.1742 \n",
      "total training time:  30.72507 \n",
      "Time difference of 21.94832 mins\n",
      "Training error:  28.87882 29.34328 1.165463 15.60776 9.503404 9.374623 36.70494 22.76706 0.3316709 0.06738794 0.1582307 1.558934 3.172465 4.509295 2.389083 0.5474183 0.39375 1.118146 0.5682803 37.95562 2.266948 74.19962 6.667793 2.837609 5.936465 50.75302 22.48083 4.053685 1.496289 22.59818 0.6289365 12.89844 14.70348 0.3765403 61.25714 3.104439 0.4260634 3.161811 2.078396 4.479111 8.885034 5.38151 164.8159 113.9272 0.6440923 11.01404 3.476769 13.75879 233.9451 \n",
      "Testing error:  45.10205 26.99607 0.9237649 5.137148 4.005755 12.46454 15.3704 9.123989 0.3548048 0.2373786 0.4826093 0.3237565 2.484095 2.878527 4.181255 0.5426196 0.1755712 3.212176 2.108643 22.76908 4.411979 529.8244 8.431799 4.571341 5.187921 14.62888 19.84265 15.56979 1.307713 14.56396 0.2791189 22.56317 3.582288 1.081277 40.79616 6.65415 0.8858423 52.59917 2.607569 2.939735 4.239197 7.10117 207.0197 12.46486 0.5287144 54.20948 1.6421 12.82905 291.6601 \n",
      "Training error:  0.1055778 0.06884251 1.179721e-06 0.001477977 6.585077e-07 0.03435919 6.938533e-08 3.326748e-07 6.618363e-07 7.85005e-06 6.96219e-07 1.749074e-06 2.367755e-07 3.504287e-07 0.00131705 1.371152e-06 1.326496e-07 5.939001e-05 1.937852e-08 0.01438912 0.002101927 3.59854e+13 0.0008285725 0.000377423 0.001392793 0.030421 0.01209624 0.002273253 0.0002229572 4.511871e-09 2224481 0.01313586 107.6917 0.004011741 0.1460991 4.126445e-05 2.767519e-06 0.0001943998 3.275875e-06 0.02971863 0.04507871 2.476607e-06 9.898534e-05 0.001781411 5.00117e-05 0.03658674 1.72536e-08 2.47219e-08 1.419216 \n",
      "Testing error:  72.87634 45.53799 2.158111 19.1076 11.53571 21.88002 40.58096 29.47136 0.5346068 0.2992032 0.696753 1.992142 4.424501 7.357302 6.648559 0.8821853 0.9681323 3.414246 1.964475 69.48643 6.287466 3.042857e+17 11.68058 6.477945 12.55448 55.51054 32.94385 17.34507 2.988428 68.43296 1.899706e+13 38.53189 4735004 1.451567 86.83743 14.19711 1.670209 52.16244 4.937041 6.555967 10.24988 44.7174 448.5799 304.3347 0.9892149 64.84788 6.247062 53.56362 415.3361 \n",
      "total training time:  31.45217 \n",
      "lambda1= 0.005309585 ; lambda2= 0.005309585 ; avg tune err= 6.2103e+15 \n",
      "Training error:  28.87882 29.34328 1.165463 15.60776 9.503404 9.374623 36.70494 22.76706 0.3316709 0.06738794 0.1582307 1.558934 3.172465 4.509295 2.389083 0.5474183 0.39375 1.118146 0.5682803 37.95562 2.266948 74.19962 6.667793 2.837609 5.936465 50.75302 22.48083 4.053685 1.496289 22.59818 0.6289365 12.89844 14.70348 0.3765403 61.25714 3.104439 0.4260634 3.161811 2.078396 4.479111 8.885034 5.38151 164.8159 113.9272 0.6440923 11.01404 3.476769 13.75879 233.9451 \n",
      "Testing error:  45.10205 26.99607 0.9237649 5.137148 4.005755 12.46454 15.3704 9.123989 0.3548048 0.2373786 0.4826093 0.3237565 2.484095 2.878527 4.181255 0.5426196 0.1755712 3.212176 2.108643 22.76908 4.411979 529.8244 8.431799 4.571341 5.187921 14.62888 19.84265 15.56979 1.307713 14.56396 0.2791189 22.56317 3.582288 1.081277 40.79616 6.65415 0.8858423 52.59917 2.607569 2.939735 4.239197 7.10117 207.0197 12.46486 0.5287144 54.20948 1.6421 12.82905 291.6601 \n",
      "Training error:  29.13034 29.34306 1.16189 15.60677 9.502675 10.01771 36.24096 22.67349 0.4099484 0.1355253 0.1650247 1.556958 3.166113 4.485586 5.501348 0.5486738 0.3937055 1.212347 0.5659686 37.85858 6.332598 94.58899 6.658401 2.829795 5.932258 50.71076 22.39471 4.033389 1.498702 22.31905 0.6238423 12.87066 14.70226 0.3764265 61.12094 5.780112 0.4393791 3.161015 2.367542 4.628759 8.879242 5.373612 162.6268 113.4576 1.092295 11.76996 3.464369 13.73573 307.2866 \n",
      "Testing error:  45.20216 27.00142 0.9180991 5.130471 4.005502 12.28137 15.60855 9.104392 0.3233003 0.2324673 0.4840468 0.3286047 2.475851 2.870957 3.070886 0.5410424 0.1757265 3.219756 2.111724 22.99663 5.2426 527.6481 8.440977 4.549674 5.1799 14.52083 19.92754 15.58066 1.318677 14.40511 0.2788616 22.52897 3.576106 1.080708 40.6501 5.071941 0.88163 52.61259 2.693575 3.021221 4.228026 7.039523 205.7062 12.63659 0.6656699 54.4323 1.655272 12.85876 257.7288 \n",
      "total training time:  30.63172 \n",
      "lambda1= 0.005309585 ; lambda2= 7.01691 ; avg tune err= 29.88253 \n",
      "Training error:  28.87882 29.34328 1.165463 15.60776 9.503404 9.374623 36.70494 22.76706 0.3316709 0.06738794 0.1582307 1.558934 3.172465 4.509295 2.389083 0.5474183 0.39375 1.118146 0.5682803 37.95562 2.266948 74.19962 6.667793 2.837609 5.936465 50.75302 22.48083 4.053685 1.496289 22.59818 0.6289365 12.89844 14.70348 0.3765403 61.25714 3.104439 0.4260634 3.161811 2.078396 4.479111 8.885034 5.38151 164.8159 113.9272 0.6440923 11.01404 3.476769 13.75879 233.9451 \n",
      "Testing error:  45.10205 26.99607 0.9237649 5.137148 4.005755 12.46454 15.3704 9.123989 0.3548048 0.2373786 0.4826093 0.3237565 2.484095 2.878527 4.181255 0.5426196 0.1755712 3.212176 2.108643 22.76908 4.411979 529.8244 8.431799 4.571341 5.187921 14.62888 19.84265 15.56979 1.307713 14.56396 0.2791189 22.56317 3.582288 1.081277 40.79616 6.65415 0.8858423 52.59917 2.607569 2.939735 4.239197 7.10117 207.0197 12.46486 0.5287144 54.20948 1.6421 12.82905 291.6601 \n",
      "Training error:  29.15941 29.34328 1.165463 15.60776 9.503404 10.02295 36.70494 22.76706 0.41078 0.1360115 0.1651318 1.558934 3.172465 4.509295 5.535968 0.5527823 0.39375 1.218313 0.5682803 37.95562 6.344876 117.142 6.667793 2.837609 5.936465 50.75302 22.48083 4.053685 1.509187 22.59818 0.6289365 12.89844 14.70348 0.3765403 61.25714 5.792808 0.4394528 3.161811 2.374106 4.640997 8.885034 5.38151 164.8159 113.9272 1.094448 11.7707 3.476769 13.75879 307.2869 \n",
      "Testing error:  45.30163 26.99607 0.9237649 5.137148 4.005755 12.26202 15.3704 9.123989 0.3240297 0.2326207 0.483991 0.3237565 2.484095 2.878527 3.070917 0.5447485 0.1755712 3.227447 2.108643 22.76908 5.235187 526.0967 8.431799 4.571341 5.187921 14.62888 19.84265 15.56979 1.308216 14.56396 0.2791189 22.56317 3.582288 1.081277 40.79616 5.053894 0.8813741 52.59917 2.683452 3.008624 4.239197 7.10117 207.0197 12.46486 0.6628285 54.4253 1.6421 12.82905 257.7247 \n",
      "total training time:  31.00423 \n",
      "Training error:  28.87882 29.34328 1.165463 15.60776 9.503404 9.374623 36.70494 22.76706 0.3316709 0.06738794 0.1582307 1.558934 3.172465 4.509295 2.389083 0.5474183 0.39375 1.118146 0.5682803 37.95562 2.266948 74.19962 6.667793 2.837609 5.936465 50.75302 22.48083 4.053685 1.496289 22.59818 0.6289365 12.89844 14.70348 0.3765403 61.25714 3.104439 0.4260634 3.161811 2.078396 4.479111 8.885034 5.38151 164.8159 113.9272 0.6440923 11.01404 3.476769 13.75879 233.9451 \n",
      "Testing error:  45.10205 26.99607 0.9237649 5.137148 4.005755 12.46454 15.3704 9.123989 0.3548048 0.2373786 0.4826093 0.3237565 2.484095 2.878527 4.181255 0.5426196 0.1755712 3.212176 2.108643 22.76908 4.411979 529.8244 8.431799 4.571341 5.187921 14.62888 19.84265 15.56979 1.307713 14.56396 0.2791189 22.56317 3.582288 1.081277 40.79616 6.65415 0.8858423 52.59917 2.607569 2.939735 4.239197 7.10117 207.0197 12.46486 0.5287144 54.20948 1.6421 12.82905 291.6601 \n",
      "Training error:  0.892556 0.7273165 0.03895922 0.1911449 0.0342236 0.5299017 0.01979654 0.03310852 0.04114694 0.02252901 0.0321055 0.03982376 0.03431342 0.03596821 0.05217009 0.04887399 0.02425028 0.01829228 0.01233366 0.3870923 0.1732597 0.006476841 0.1846068 0.1726839 0.1814129 0.5743379 0.4088737 0.1993157 0.1285988 0.005589793 0.0368834 0.3649435 0.03304993 0.1950513 0.9093642 0.03741602 0.0439569 0.1141811 0.04445783 0.4927019 0.5992787 0.02748246 0.07226602 0.1372363 0.08828364 0.5688618 0.01216571 0.01239806 4.390871 \n",
      "Testing error:  63.01023 41.36678 1.919126 13.83763 7.345189 18.9133 38.03004 24.56594 0.4589198 0.2700237 0.4753729 1.284178 2.806988 6.397493 5.722336 0.6623908 0.40653 3.203038 2.662209 54.18173 5.321193 605.7242 8.198311 5.712366 10.0295 43.33363 31.69728 17.21185 2.023618 38.98782 0.4424809 29.6346 16.99358 1.126468 69.43274 10.66937 1.043013 49.86773 3.81289 4.571634 8.7067 10.06314 461.1928 81.80327 0.6496583 65.0142 3.59368 31.01235 381.1595 \n",
      "total training time:  30.71159 \n",
      "lambda1= 7.01691 ; lambda2= 0.005309585 ; avg tune err= 46.66427 \n",
      "Training error:  28.87882 29.34328 1.165463 15.60776 9.503404 9.374623 36.70494 22.76706 0.3316709 0.06738794 0.1582307 1.558934 3.172465 4.509295 2.389083 0.5474183 0.39375 1.118146 0.5682803 37.95562 2.266948 74.19962 6.667793 2.837609 5.936465 50.75302 22.48083 4.053685 1.496289 22.59818 0.6289365 12.89844 14.70348 0.3765403 61.25714 3.104439 0.4260634 3.161811 2.078396 4.479111 8.885034 5.38151 164.8159 113.9272 0.6440923 11.01404 3.476769 13.75879 233.9451 \n",
      "Testing error:  45.10205 26.99607 0.9237649 5.137148 4.005755 12.46454 15.3704 9.123989 0.3548048 0.2373786 0.4826093 0.3237565 2.484095 2.878527 4.181255 0.5426196 0.1755712 3.212176 2.108643 22.76908 4.411979 529.8244 8.431799 4.571341 5.187921 14.62888 19.84265 15.56979 1.307713 14.56396 0.2791189 22.56317 3.582288 1.081277 40.79616 6.65415 0.8858423 52.59917 2.607569 2.939735 4.239197 7.10117 207.0197 12.46486 0.5287144 54.20948 1.6421 12.82905 291.6601 \n",
      "Training error:  29.13474 29.34328 1.163156 15.60741 9.503221 10.01931 36.27351 22.68336 0.4105915 0.1360048 0.1651318 1.557893 3.167885 4.489498 5.506407 0.5500806 0.39375 1.214105 0.5669728 37.86856 6.335106 95.49614 6.660595 2.831765 5.933628 50.71628 22.40378 4.036917 1.501057 22.34154 0.6254164 12.87494 14.70297 0.3765403 61.13376 5.782901 0.4394528 3.161595 2.369306 4.631305 8.880907 5.375573 162.7491 113.4908 1.093288 11.77042 3.466973 13.73955 307.2868 \n",
      "Testing error:  45.21785 26.99607 0.9201252 5.134865 4.005683 12.27512 15.58794 9.106095 0.3238606 0.2326183 0.483991 0.3261911 2.478146 2.871996 3.0705 0.5422844 0.1755712 3.222032 2.110264 22.97085 5.240818 527.5314 8.438529 4.555312 5.182578 14.53562 19.91721 15.57842 1.316052 14.41739 0.278845 22.53424 3.5798 1.081277 40.66377 5.067687 0.8813741 52.60265 2.690534 3.018259 4.231308 7.055658 205.7743 12.62069 0.6642638 54.42779 1.652236 12.85335 257.7262 \n",
      "total training time:  30.82154 \n",
      "lambda1= 7.01691 ; lambda2= 7.01691 ; avg tune err= 29.88101 \n",
      "Training error:  28.87882 29.34328 1.165463 15.60776 9.503404 9.374623 36.70494 22.76706 0.3316709 0.06738794 0.1582307 1.558934 3.172465 4.509295 2.389083 0.5474183 0.39375 1.118146 0.5682803 37.95562 2.266948 74.19962 6.667793 2.837609 5.936465 50.75302 22.48083 4.053685 1.496289 22.59818 0.6289365 12.89844 14.70348 0.3765403 61.25714 3.104439 0.4260634 3.161811 2.078396 4.479111 8.885034 5.38151 164.8159 113.9272 0.6440923 11.01404 3.476769 13.75879 233.9451 \n",
      "Testing error:  45.10205 26.99607 0.9237649 5.137148 4.005755 12.46454 15.3704 9.123989 0.3548048 0.2373786 0.4826093 0.3237565 2.484095 2.878527 4.181255 0.5426196 0.1755712 3.212176 2.108643 22.76908 4.411979 529.8244 8.431799 4.571341 5.187921 14.62888 19.84265 15.56979 1.307713 14.56396 0.2791189 22.56317 3.582288 1.081277 40.79616 6.65415 0.8858423 52.59917 2.607569 2.939735 4.239197 7.10117 207.0197 12.46486 0.5287144 54.20948 1.6421 12.82905 291.6601 \n",
      "Training error:  29.15941 29.34328 1.165463 15.60776 9.503404 10.02295 36.70494 22.76706 0.41078 0.1360115 0.1651318 1.558934 3.172465 4.509295 5.535968 0.5527823 0.39375 1.218313 0.5682803 37.95562 6.344876 117.142 6.667793 2.837609 5.936465 50.75302 22.48083 4.053685 1.509187 22.59818 0.6289365 12.89844 14.70348 0.3765403 61.25714 5.792808 0.4394528 3.161811 2.374106 4.640997 8.885034 5.38151 164.8159 113.9272 1.094448 11.7707 3.476769 13.75879 307.2869 \n",
      "Testing error:  45.30163 26.99607 0.9237649 5.137148 4.005755 12.26202 15.3704 9.123989 0.3240297 0.2326207 0.483991 0.3237565 2.484095 2.878527 3.070917 0.5447485 0.1755712 3.227447 2.108643 22.76908 5.235187 526.0967 8.431799 4.571341 5.187921 14.62888 19.84265 15.56979 1.308216 14.56396 0.2791189 22.56317 3.582288 1.081277 40.79616 5.053894 0.8813741 52.59917 2.683452 3.008624 4.239197 7.10117 207.0197 12.46486 0.6628285 54.4253 1.6421 12.82905 257.7247 \n",
      "total training time:  31.06395 \n",
      "Training error:  28.87882 29.34328 1.165463 15.60776 9.503404 9.374623 36.70494 22.76706 0.3316709 0.06738794 0.1582307 1.558934 3.172465 4.509295 2.389083 0.5474183 0.39375 1.118146 0.5682803 37.95562 2.266948 74.19962 6.667793 2.837609 5.936465 50.75302 22.48083 4.053685 1.496289 22.59818 0.6289365 12.89844 14.70348 0.3765403 61.25714 3.104439 0.4260634 3.161811 2.078396 4.479111 8.885034 5.38151 164.8159 113.9272 0.6440923 11.01404 3.476769 13.75879 233.9451 \n",
      "Testing error:  45.10205 26.99607 0.9237649 5.137148 4.005755 12.46454 15.3704 9.123989 0.3548048 0.2373786 0.4826093 0.3237565 2.484095 2.878527 4.181255 0.5426196 0.1755712 3.212176 2.108643 22.76908 4.411979 529.8244 8.431799 4.571341 5.187921 14.62888 19.84265 15.56979 1.307713 14.56396 0.2791189 22.56317 3.582288 1.081277 40.79616 6.65415 0.8858423 52.59917 2.607569 2.939735 4.239197 7.10117 207.0197 12.46486 0.5287144 54.20948 1.6421 12.82905 291.6601 \n",
      "Training error:  2.047764 1.678778 0.0944627 0.5026666 0.1134088 1.124479 0.06962481 0.1169721 0.112975 0.06547328 0.08357191 0.1148505 0.1287238 0.127102 0.1557482 0.1322558 0.07592144 0.07254425 0.05607135 0.9903824 0.4901721 0.01909163 0.5060081 0.4888228 0.4569095 1.360896 1.088313 0.4337189 0.3347256 0.02277397 0.1070838 1.020866 0.1021136 0.3248271 2.072866 0.1093226 0.1500059 0.342876 0.1172743 1.0277 1.264184 0.1118488 0.2104947 0.4169187 0.2168263 1.36987 0.0485062 0.04091682 7.900007 \n",
      "Testing error:  61.86904 38.05618 1.788286 11.89399 6.390235 18.9326 37.7936 23.2492 0.36818 0.2492793 0.456734 1.156363 2.943481 6.531235 5.219811 0.5949238 0.2692837 3.093813 2.699529 48.3621 4.298637 555.8293 8.910099 5.663329 8.543592 36.70729 30.54438 17.32408 1.719277 24.0849 0.4303552 28.54065 14.65114 1.06558 58.66646 8.849298 1.007185 49.35734 3.165948 4.708213 8.885815 9.430082 442.5552 57.32275 0.5818703 63.45498 3.875603 23.96306 366.4485 \n",
      "total training time:  30.67419 \n",
      "lambda1= 14.02851 ; lambda2= 0.005309585 ; avg tune err= 43.1123 \n",
      "Training error:  28.87882 29.34328 1.165463 15.60776 9.503404 9.374623 36.70494 22.76706 0.3316709 0.06738794 0.1582307 1.558934 3.172465 4.509295 2.389083 0.5474183 0.39375 1.118146 0.5682803 37.95562 2.266948 74.19962 6.667793 2.837609 5.936465 50.75302 22.48083 4.053685 1.496289 22.59818 0.6289365 12.89844 14.70348 0.3765403 61.25714 3.104439 0.4260634 3.161811 2.078396 4.479111 8.885034 5.38151 164.8159 113.9272 0.6440923 11.01404 3.476769 13.75879 233.9451 \n",
      "Testing error:  45.10205 26.99607 0.9237649 5.137148 4.005755 12.46454 15.3704 9.123989 0.3548048 0.2373786 0.4826093 0.3237565 2.484095 2.878527 4.181255 0.5426196 0.1755712 3.212176 2.108643 22.76908 4.411979 529.8244 8.431799 4.571341 5.187921 14.62888 19.84265 15.56979 1.307713 14.56396 0.2791189 22.56317 3.582288 1.081277 40.79616 6.65415 0.8858423 52.59917 2.607569 2.939735 4.239197 7.10117 207.0197 12.46486 0.5287144 54.20948 1.6421 12.82905 291.6601 \n",
      "Training error:  29.13888 29.34328 1.164373 15.60776 9.503404 10.02082 36.30426 22.69261 0.41078 0.1360115 0.1651318 1.558791 3.169562 4.493169 5.511146 0.5514173 0.39375 1.215768 0.5679433 37.87794 6.33749 96.35988 6.662674 2.833635 5.934947 50.72148 22.41232 4.040235 1.503283 22.36269 0.6269059 12.87897 14.70348 0.3765403 61.14581 5.785513 0.4394528 3.161811 2.370995 4.633708 8.882491 5.377443 162.8652 113.5221 1.09425 11.7707 3.469429 13.74313 307.2869 \n",
      "Testing error:  45.23238 26.99607 0.922052 5.137148 4.005755 12.26948 15.56909 9.107779 0.3240297 0.2326207 0.483991 0.3240754 2.480322 2.873054 3.07027 0.5434915 0.1755712 3.224178 2.109034 22.94718 5.239265 527.4253 8.436395 4.560547 5.185094 14.54936 19.90783 15.57646 1.31373 14.42904 0.2789123 22.5392 3.582288 1.081277 40.67663 5.063861 0.8813741 52.59917 2.687859 3.015643 4.234374 7.070487 205.8396 12.60626 0.6630585 54.4253 1.649515 12.84847 257.7247 \n",
      "total training time:  30.92221 \n",
      "lambda1= 14.02851 ; lambda2= 7.01691 ; avg tune err= 29.87997 \n",
      "Training error:  28.87882 29.34328 1.165463 15.60776 9.503404 9.374623 36.70494 22.76706 0.3316709 0.06738794 0.1582307 1.558934 3.172465 4.509295 2.389083 0.5474183 0.39375 1.118146 0.5682803 37.95562 2.266948 74.19962 6.667793 2.837609 5.936465 50.75302 22.48083 4.053685 1.496289 22.59818 0.6289365 12.89844 14.70348 0.3765403 61.25714 3.104439 0.4260634 3.161811 2.078396 4.479111 8.885034 5.38151 164.8159 113.9272 0.6440923 11.01404 3.476769 13.75879 233.9451 \n",
      "Testing error:  45.10205 26.99607 0.9237649 5.137148 4.005755 12.46454 15.3704 9.123989 0.3548048 0.2373786 0.4826093 0.3237565 2.484095 2.878527 4.181255 0.5426196 0.1755712 3.212176 2.108643 22.76908 4.411979 529.8244 8.431799 4.571341 5.187921 14.62888 19.84265 15.56979 1.307713 14.56396 0.2791189 22.56317 3.582288 1.081277 40.79616 6.65415 0.8858423 52.59917 2.607569 2.939735 4.239197 7.10117 207.0197 12.46486 0.5287144 54.20948 1.6421 12.82905 291.6601 \n",
      "Training error:  29.15941 29.34328 1.165463 15.60776 9.503404 10.02295 36.70494 22.76706 0.41078 0.1360115 0.1651318 1.558934 3.172465 4.509295 5.535968 0.5527823 0.39375 1.218313 0.5682803 37.95562 6.344876 117.142 6.667793 2.837609 5.936465 50.75302 22.48083 4.053685 1.509187 22.59818 0.6289365 12.89844 14.70348 0.3765403 61.25714 5.792808 0.4394528 3.161811 2.374106 4.640997 8.885034 5.38151 164.8159 113.9272 1.094448 11.7707 3.476769 13.75879 307.2869 \n",
      "Testing error:  45.30163 26.99607 0.9237649 5.137148 4.005755 12.26202 15.3704 9.123989 0.3240297 0.2326207 0.483991 0.3237565 2.484095 2.878527 3.070917 0.5447485 0.1755712 3.227447 2.108643 22.76908 5.235187 526.0967 8.431799 4.571341 5.187921 14.62888 19.84265 15.56979 1.308216 14.56396 0.2791189 22.56317 3.582288 1.081277 40.79616 5.053894 0.8813741 52.59917 2.683452 3.008624 4.239197 7.10117 207.0197 12.46486 0.6628285 54.4253 1.6421 12.82905 257.7247 \n",
      "total training time:  30.67008 \n",
      "Training error:  28.87882 29.34328 1.165463 15.60776 9.503404 9.374623 36.70494 22.76706 0.3316709 0.06738794 0.1582307 1.558934 3.172465 4.509295 2.389083 0.5474183 0.39375 1.118146 0.5682803 37.95562 2.266948 74.19962 6.667793 2.837609 5.936465 50.75302 22.48083 4.053685 1.496289 22.59818 0.6289365 12.89844 14.70348 0.3765403 61.25714 3.104439 0.4260634 3.161811 2.078396 4.479111 8.885034 5.38151 164.8159 113.9272 0.6440923 11.01404 3.476769 13.75879 233.9451 \n",
      "Testing error:  45.10205 26.99607 0.9237649 5.137148 4.005755 12.46454 15.3704 9.123989 0.3548048 0.2373786 0.4826093 0.3237565 2.484095 2.878527 4.181255 0.5426196 0.1755712 3.212176 2.108643 22.76908 4.411979 529.8244 8.431799 4.571341 5.187921 14.62888 19.84265 15.56979 1.307713 14.56396 0.2791189 22.56317 3.582288 1.081277 40.79616 6.65415 0.8858423 52.59917 2.607569 2.939735 4.239197 7.10117 207.0197 12.46486 0.5287144 54.20948 1.6421 12.82905 291.6601 \n",
      "Training error:  3.441588 2.875785 0.163907 0.8469185 0.2513515 1.876788 0.1524345 0.2545541 0.1824536 0.1113671 0.142375 0.2179197 0.249439 0.2703796 0.3145336 0.2315447 0.1634652 0.1476181 0.1293177 1.818425 0.874546 0.04511505 0.8726164 0.786216 0.8250991 2.247678 1.892781 0.7498106 0.5731105 0.04888801 0.1885065 1.790838 0.1852376 0.3674269 3.46028 0.2032587 0.2442334 0.6257821 0.2339292 1.632482 1.888455 0.2347703 0.4170583 0.7545242 0.3829884 2.213323 0.1043638 0.09200091 11.26778 \n",
      "Testing error:  59.57293 37.4999 1.707203 10.94594 6.559423 17.86484 38.45839 22.76451 0.3196815 0.2314548 0.4693688 1.055566 3.057888 6.445346 4.367483 0.5340047 0.1954162 3.038502 2.650662 47.07244 4.192508 500.5175 9.395118 5.806962 7.709173 33.26448 28.94938 17.06309 1.539168 18.87534 0.3756238 28.69986 11.92752 1.077171 55.39483 8.971226 0.9360946 48.6258 3.037673 4.312412 8.06695 10.18464 415.7263 56.21893 0.5303568 63.0347 3.548878 20.96444 364.0733 \n",
      "total training time:  31.10133 \n",
      "lambda1= 21.04011 ; lambda2= 0.005309585 ; avg tune err= 40.77205 \n",
      "Training error:  28.87882 29.34328 1.165463 15.60776 9.503404 9.374623 36.70494 22.76706 0.3316709 0.06738794 0.1582307 1.558934 3.172465 4.509295 2.389083 0.5474183 0.39375 1.118146 0.5682803 37.95562 2.266948 74.19962 6.667793 2.837609 5.936465 50.75302 22.48083 4.053685 1.496289 22.59818 0.6289365 12.89844 14.70348 0.3765403 61.25714 3.104439 0.4260634 3.161811 2.078396 4.479111 8.885034 5.38151 164.8159 113.9272 0.6440923 11.01404 3.476769 13.75879 233.9451 \n",
      "Testing error:  45.10205 26.99607 0.9237649 5.137148 4.005755 12.46454 15.3704 9.123989 0.3548048 0.2373786 0.4826093 0.3237565 2.484095 2.878527 4.181255 0.5426196 0.1755712 3.212176 2.108643 22.76908 4.411979 529.8244 8.431799 4.571341 5.187921 14.62888 19.84265 15.56979 1.307713 14.56396 0.2791189 22.56317 3.582288 1.081277 40.79616 6.65415 0.8858423 52.59917 2.607569 2.939735 4.239197 7.10117 207.0197 12.46486 0.5287144 54.20948 1.6421 12.82905 291.6601 \n",
      "Training error:  29.14276 29.34328 1.165463 15.60776 9.503404 10.02226 36.3332 22.70125 0.41078 0.1360115 0.1651318 1.558934 3.17114 4.496596 5.515561 0.5526776 0.39375 1.217332 0.5682803 37.88671 6.339742 97.2031 6.664632 2.8354 5.936208 50.72636 22.42031 4.043337 1.505372 22.38246 0.6283056 12.88274 14.70348 0.3765403 61.15708 5.787943 0.4394528 3.161811 2.372602 4.635963 8.883989 5.379212 162.975 113.5515 1.094448 11.7707 3.471733 13.74648 307.2869 \n",
      "Testing error:  45.24579 26.99607 0.9237649 5.137148 4.005755 12.2644 15.55188 9.109421 0.3240297 0.2326207 0.483991 0.3237565 2.482372 2.874108 3.070166 0.5446514 0.1755712 3.226189 2.108643 22.9255 5.237911 527.3264 8.434533 4.565392 5.187446 14.56208 19.89932 15.57475 1.311677 14.44 0.2790414 22.54384 3.582288 1.081277 40.68867 5.060427 0.8813741 52.59917 2.685506 3.013332 4.237227 7.084085 205.9019 12.59323 0.6628285 54.4253 1.647077 12.84405 257.7247 \n",
      "total training time:  30.95182 \n",
      "lambda1= 21.04011 ; lambda2= 7.01691 ; avg tune err= 29.8792 \n",
      "Training error:  28.87882 29.34328 1.165463 15.60776 9.503404 9.374623 36.70494 22.76706 0.3316709 0.06738794 0.1582307 1.558934 3.172465 4.509295 2.389083 0.5474183 0.39375 1.118146 0.5682803 37.95562 2.266948 74.19962 6.667793 2.837609 5.936465 50.75302 22.48083 4.053685 1.496289 22.59818 0.6289365 12.89844 14.70348 0.3765403 61.25714 3.104439 0.4260634 3.161811 2.078396 4.479111 8.885034 5.38151 164.8159 113.9272 0.6440923 11.01404 3.476769 13.75879 233.9451 \n",
      "Testing error:  45.10205 26.99607 0.9237649 5.137148 4.005755 12.46454 15.3704 9.123989 0.3548048 0.2373786 0.4826093 0.3237565 2.484095 2.878527 4.181255 0.5426196 0.1755712 3.212176 2.108643 22.76908 4.411979 529.8244 8.431799 4.571341 5.187921 14.62888 19.84265 15.56979 1.307713 14.56396 0.2791189 22.56317 3.582288 1.081277 40.79616 6.65415 0.8858423 52.59917 2.607569 2.939735 4.239197 7.10117 207.0197 12.46486 0.5287144 54.20948 1.6421 12.82905 291.6601 \n",
      "Training error:  29.15941 29.34328 1.165463 15.60776 9.503404 10.02295 36.70494 22.76706 0.41078 0.1360115 0.1651318 1.558934 3.172465 4.509295 5.535968 0.5527823 0.39375 1.218313 0.5682803 37.95562 6.344876 117.142 6.667793 2.837609 5.936465 50.75302 22.48083 4.053685 1.509187 22.59818 0.6289365 12.89844 14.70348 0.3765403 61.25714 5.792808 0.4394528 3.161811 2.374106 4.640997 8.885034 5.38151 164.8159 113.9272 1.094448 11.7707 3.476769 13.75879 307.2869 \n",
      "Testing error:  45.30163 26.99607 0.9237649 5.137148 4.005755 12.26202 15.3704 9.123989 0.3240297 0.2326207 0.483991 0.3237565 2.484095 2.878527 3.070917 0.5447485 0.1755712 3.227447 2.108643 22.76908 5.235187 526.0967 8.431799 4.571341 5.187921 14.62888 19.84265 15.56979 1.308216 14.56396 0.2791189 22.56317 3.582288 1.081277 40.79616 5.053894 0.8813741 52.59917 2.683452 3.008624 4.239197 7.10117 207.0197 12.46486 0.6628285 54.4253 1.6421 12.82905 257.7247 \n",
      "total training time:  30.78066 \n",
      "Training error:  28.87882 29.34328 1.165463 15.60776 9.503404 9.374623 36.70494 22.76706 0.3316709 0.06738794 0.1582307 1.558934 3.172465 4.509295 2.389083 0.5474183 0.39375 1.118146 0.5682803 37.95562 2.266948 74.19962 6.667793 2.837609 5.936465 50.75302 22.48083 4.053685 1.496289 22.59818 0.6289365 12.89844 14.70348 0.3765403 61.25714 3.104439 0.4260634 3.161811 2.078396 4.479111 8.885034 5.38151 164.8159 113.9272 0.6440923 11.01404 3.476769 13.75879 233.9451 \n",
      "Testing error:  45.10205 26.99607 0.9237649 5.137148 4.005755 12.46454 15.3704 9.123989 0.3548048 0.2373786 0.4826093 0.3237565 2.484095 2.878527 4.181255 0.5426196 0.1755712 3.212176 2.108643 22.76908 4.411979 529.8244 8.431799 4.571341 5.187921 14.62888 19.84265 15.56979 1.307713 14.56396 0.2791189 22.56317 3.582288 1.081277 40.79616 6.65415 0.8858423 52.59917 2.607569 2.939735 4.239197 7.10117 207.0197 12.46486 0.5287144 54.20948 1.6421 12.82905 291.6601 \n",
      "Training error:  5.031575 4.105339 0.2673424 1.246365 0.4378495 2.744245 0.2597691 0.4236287 0.2737393 0.1360115 0.1651318 0.3753327 0.3740516 0.4183725 0.5093335 0.349607 0.2596231 0.2389311 0.2198672 2.777657 1.328461 0.06362788 1.239312 1.068903 1.248332 3.287745 2.80167 1.108062 0.8285163 0.08644919 0.2812355 2.669138 0.3399088 0.3765403 5.037551 0.3395159 0.3230095 0.959115 0.4170932 2.298551 2.485476 0.4065734 0.6592491 1.253718 0.5383978 3.045408 0.1802068 0.1829785 15.19272 \n",
      "Testing error:  57.5241 37.60551 1.526513 9.845574 6.43491 17.21893 39.79871 23.31359 0.315467 0.2326207 0.483991 0.9514234 2.736469 5.280034 3.944777 0.5484159 0.1837759 3.184208 2.518399 44.52166 4.145076 492.3545 9.573429 5.629626 7.020527 31.74419 29.27415 16.56267 1.438022 17.29463 0.3112719 28.4301 9.007613 1.081277 55.11815 8.934587 0.8877981 48.52535 2.856028 3.876661 7.539784 9.603484 392.7461 54.25966 0.504019 61.9706 3.042529 21.20692 368.2748 \n",
      "total training time:  30.75125 \n",
      "lambda1= 28.05171 ; lambda2= 0.005309585 ; avg tune err= 39.82413 \n",
      "Training error:  28.87882 29.34328 1.165463 15.60776 9.503404 9.374623 36.70494 22.76706 0.3316709 0.06738794 0.1582307 1.558934 3.172465 4.509295 2.389083 0.5474183 0.39375 1.118146 0.5682803 37.95562 2.266948 74.19962 6.667793 2.837609 5.936465 50.75302 22.48083 4.053685 1.496289 22.59818 0.6289365 12.89844 14.70348 0.3765403 61.25714 3.104439 0.4260634 3.161811 2.078396 4.479111 8.885034 5.38151 164.8159 113.9272 0.6440923 11.01404 3.476769 13.75879 233.9451 \n",
      "Testing error:  45.10205 26.99607 0.9237649 5.137148 4.005755 12.46454 15.3704 9.123989 0.3548048 0.2373786 0.4826093 0.3237565 2.484095 2.878527 4.181255 0.5426196 0.1755712 3.212176 2.108643 22.76908 4.411979 529.8244 8.431799 4.571341 5.187921 14.62888 19.84265 15.56979 1.307713 14.56396 0.2791189 22.56317 3.582288 1.081277 40.79616 6.65415 0.8858423 52.59917 2.607569 2.939735 4.239197 7.10117 207.0197 12.46486 0.5287144 54.20948 1.6421 12.82905 291.6601 \n",
      "Training error:  29.14637 29.34328 1.165463 15.60776 9.503404 10.02295 36.36021 22.70923 0.41078 0.1360115 0.1651318 1.558934 3.172465 4.499778 5.519652 0.5527823 0.39375 1.218313 0.5682803 37.89486 6.341859 98.05153 6.666467 2.837056 5.936465 50.73093 22.42776 4.046224 1.507325 22.40071 0.6289365 12.88625 14.70348 0.3765403 61.16754 5.790197 0.4394528 3.161811 2.374106 4.638067 8.885034 5.380878 163.0778 113.5789 1.094448 11.7707 3.473884 13.7496 307.2869 \n",
      "Testing error:  45.25812 26.99607 0.9237649 5.137148 4.005755 12.26202 15.53627 9.111 0.3240297 0.2326207 0.483991 0.3237565 2.484095 2.875143 3.070159 0.5447485 0.1755712 3.227447 2.108643 22.90576 5.23673 527.2314 8.432908 4.569862 5.187921 14.57382 19.89163 15.57325 1.309862 14.45019 0.2791189 22.54815 3.582288 1.081277 40.69985 5.057347 0.8813741 52.59917 2.683452 3.011292 4.239197 7.096535 205.9608 12.58152 0.6628285 54.4253 1.644895 12.84008 257.7247 \n",
      "total training time:  30.65117 \n",
      "lambda1= 28.05171 ; lambda2= 7.01691 ; avg tune err= 29.87842 \n",
      "Training error:  28.87882 29.34328 1.165463 15.60776 9.503404 9.374623 36.70494 22.76706 0.3316709 0.06738794 0.1582307 1.558934 3.172465 4.509295 2.389083 0.5474183 0.39375 1.118146 0.5682803 37.95562 2.266948 74.19962 6.667793 2.837609 5.936465 50.75302 22.48083 4.053685 1.496289 22.59818 0.6289365 12.89844 14.70348 0.3765403 61.25714 3.104439 0.4260634 3.161811 2.078396 4.479111 8.885034 5.38151 164.8159 113.9272 0.6440923 11.01404 3.476769 13.75879 233.9451 \n",
      "Testing error:  45.10205 26.99607 0.9237649 5.137148 4.005755 12.46454 15.3704 9.123989 0.3548048 0.2373786 0.4826093 0.3237565 2.484095 2.878527 4.181255 0.5426196 0.1755712 3.212176 2.108643 22.76908 4.411979 529.8244 8.431799 4.571341 5.187921 14.62888 19.84265 15.56979 1.307713 14.56396 0.2791189 22.56317 3.582288 1.081277 40.79616 6.65415 0.8858423 52.59917 2.607569 2.939735 4.239197 7.10117 207.0197 12.46486 0.5287144 54.20948 1.6421 12.82905 291.6601 \n",
      "Training error:  29.15941 29.34328 1.165463 15.60776 9.503404 10.02295 36.70494 22.76706 0.41078 0.1360115 0.1651318 1.558934 3.172465 4.509295 5.535968 0.5527823 0.39375 1.218313 0.5682803 37.95562 6.344876 117.142 6.667793 2.837609 5.936465 50.75302 22.48083 4.053685 1.509187 22.59818 0.6289365 12.89844 14.70348 0.3765403 61.25714 5.792808 0.4394528 3.161811 2.374106 4.640997 8.885034 5.38151 164.8159 113.9272 1.094448 11.7707 3.476769 13.75879 307.2869 \n",
      "Testing error:  45.30163 26.99607 0.9237649 5.137148 4.005755 12.26202 15.3704 9.123989 0.3240297 0.2326207 0.483991 0.3237565 2.484095 2.878527 3.070917 0.5447485 0.1755712 3.227447 2.108643 22.76908 5.235187 526.0967 8.431799 4.571341 5.187921 14.62888 19.84265 15.56979 1.308216 14.56396 0.2791189 22.56317 3.582288 1.081277 40.79616 5.053894 0.8813741 52.59917 2.683452 3.008624 4.239197 7.10117 207.0197 12.46486 0.6628285 54.4253 1.6421 12.82905 257.7247 \n",
      "total training time:  30.89567 \n",
      "Training error:  28.87882 29.34328 1.165463 15.60776 9.503404 9.374623 36.70494 22.76706 0.3316709 0.06738794 0.1582307 1.558934 3.172465 4.509295 2.389083 0.5474183 0.39375 1.118146 0.5682803 37.95562 2.266948 74.19962 6.667793 2.837609 5.936465 50.75302 22.48083 4.053685 1.496289 22.59818 0.6289365 12.89844 14.70348 0.3765403 61.25714 3.104439 0.4260634 3.161811 2.078396 4.479111 8.885034 5.38151 164.8159 113.9272 0.6440923 11.01404 3.476769 13.75879 233.9451 \n",
      "Testing error:  45.10205 26.99607 0.9237649 5.137148 4.005755 12.46454 15.3704 9.123989 0.3548048 0.2373786 0.4826093 0.3237565 2.484095 2.878527 4.181255 0.5426196 0.1755712 3.212176 2.108643 22.76908 4.411979 529.8244 8.431799 4.571341 5.187921 14.62888 19.84265 15.56979 1.307713 14.56396 0.2791189 22.56317 3.582288 1.081277 40.79616 6.65415 0.8858423 52.59917 2.607569 2.939735 4.239197 7.10117 207.0197 12.46486 0.5287144 54.20948 1.6421 12.82905 291.6601 \n",
      "Training error:  29.14637 29.34328 1.165463 15.60776 9.503404 10.02295 36.36021 22.70923 0.41078 0.1360115 0.1651318 1.558934 3.172465 4.499778 5.519652 0.5527823 0.39375 1.218313 0.5682803 37.89486 6.341859 98.05153 6.666467 2.837056 5.936465 50.73093 22.42776 4.046224 1.507325 22.40071 0.6289365 12.88625 14.70348 0.3765403 61.16754 5.790197 0.4394528 3.161811 2.374106 4.638067 8.885034 5.380878 163.0778 113.5789 1.094448 11.7707 3.473884 13.7496 307.2869 \n",
      "Testing error:  45.25812 26.99607 0.9237649 5.137148 4.005755 12.26202 15.53627 9.111 0.3240297 0.2326207 0.483991 0.3237565 2.484095 2.875143 3.070159 0.5447485 0.1755712 3.227447 2.108643 22.90576 5.23673 527.2314 8.432908 4.569862 5.187921 14.57382 19.89163 15.57325 1.309862 14.45019 0.2791189 22.54815 3.582288 1.081277 40.69985 5.057347 0.8813741 52.59917 2.683452 3.011292 4.239197 7.096535 205.9608 12.58152 0.6628285 54.4253 1.644895 12.84008 257.7247 \n",
      "total training time:  30.73709 \n",
      "Time difference of 22.19113 mins\n",
      "[1] \"Final Time Cost: \"\n",
      "Time difference of 1.8156 hours\n"
     ]
    }
   ],
   "source": [
    "#-------------------------- arguments ---------------------------#\n",
    "options(stringsAsFactors=F)\n",
    "library(glmnet)\n",
    "library(foreach)\n",
    "library(genio)\n",
    "library(Rcpp)\n",
    "\n",
    "# testing\n",
    "if (T) {\n",
    "    args <- c(\"21\", \"3\")\n",
    "} \n",
    "#args = commandArgs(trailingOnly = TRUE)\n",
    "chr = as.numeric(args[1])\n",
    "    \n",
    "\n",
    "#-------------------------- data import ---------------------------#  \n",
    "\n",
    "\n",
    "chr = \"21\"\n",
    "k = \"1\"\n",
    "genotype_dir <- \"/ysm-gpfs/project/wl382/GTEx_v8/genotype/cis_loc/\"\n",
    "gtex_dir <- \"/gpfs/loomis/project/zhao/zy92/GTEX/\" \n",
    "glist = dir(paste0(gtex_dir, \"adjusted_expr/chr\", chr))\n",
    "\n",
    "\n",
    "# idx \n",
    "\n",
    "chr_str <- paste0(\"chr\", chr, \"/\")\n",
    "gene_vec <- list.files(paste0(genotype_dir, chr_str))\n",
    "\n",
    "gene_idx = which(glist == \"ENSG00000184012\")\n",
    "dose_idx = which(startsWith(gene_vec, \"ENSG00000184012\"))\n",
    "\n",
    "gene_id <- gene_vec[dose_idx]\n",
    "g <- glist[gene_idx]\n",
    "dose_path = paste0(genotype_dir, chr_str, gene_id, \"/\", gene_id)\n",
    "Yt = dir(paste0(gtex_dir, \"adjusted_expr/\", chr_str, g, \"/\"))\n",
    "P = length(Yt)\n",
    "output_dir = \"/gpfs/loomis/project/zhao/zy92/GTEX/output/\" \n",
    "ntune = 5\n",
    "if_verbose = T\n",
    "\n",
    "bgt_origin = Sys.time()\n",
    "\n",
    "#-------------------------- functions ---------------------------#  \n",
    "\n",
    "grad_prep <- function(X, Y){\n",
    "\t## pre-calculate some metrics for gradient\n",
    "\tll = length(Y)\n",
    "\tP = ncol(X[[1]])\n",
    "\tXY = matrix(0,P,ll)\n",
    "\tfor(i in 1:ll){\n",
    "\t\tXY[,i] = t(X[[i]])%*%Y[[i]]/nrow(X[[i]])\n",
    "\t}\n",
    "\tXY\n",
    "}\n",
    "\n",
    "cv_helper <- function(N, fold){\n",
    "\tvalid_num = floor(N/fold)\n",
    "\tset.seed(123)\n",
    "\tperm = sample(1:N, size = N)\n",
    "\tidx1 = seq(1,N,valid_num)\n",
    "\tidx2 = c(idx1[-1]-1,N)\n",
    "\tlist(perm=perm, idx=cbind(idx1,idx2))\n",
    "}\n",
    "\n",
    "minmax_lambda <- function(lst){\n",
    "\tmax_lam = max(unlist(lapply(lst, function(x){max(x$lambda)})))\n",
    "\tmin_lam = min(unlist(lapply(lst, function(x){min(x$lambda)})))\n",
    "\tc(min_lam, max_lam)\n",
    "}\n",
    "\n",
    "elastic_net_mse <- function(lst, X_tune, Y_tune, X_test, Y_test){\n",
    "\tP = length(lst)\n",
    "\tM = ncol(X_tune[[1]])\n",
    "\tlam_V = rep(0, P)\n",
    "\ttest_res = list()\n",
    "\ttest_beta = matrix(0, M, P)\n",
    "\tfor(t in 1:P){\n",
    "\t\tncv = length(lst[[t]]$lambda)\n",
    "\t\ttmp_mse = rep(0, ncv)\n",
    "\t\tfor(k in 1:ncv){\n",
    "\t\t\ttmp_mse[k] = mean((Y_tune[[t]] - X_tune[[t]]%*%lst[[t]]$glmnet.fit$beta[,k])^2)\n",
    "\t\t}\n",
    "\t\tss = which.min(tmp_mse)\n",
    "\t\ttest_beta[,t] = lst[[t]]$glmnet.fit$beta[,ss]\n",
    "\t\tlam_V[t] = lst[[t]]$lambda[ss]\n",
    "\t\tpredicted = X_test[[t]]%*%lst[[t]]$glmnet.fit$beta[,ss]\n",
    "\t\ttest_res[[t]] = cbind(Y_test[[t]], predicted)\n",
    "\t}\n",
    "\tlist(lam = lam_V, mse = test_res, est = test_beta)\n",
    "}\n",
    "\n",
    "multi_mse <- function(theta_est, X_test, Y_test){\n",
    "\tanswer = list()\n",
    "\tP = ncol(theta_est)\n",
    "\tfor(t in 1:P){\n",
    "\t\tpredicted = X_test[[t]]%*%theta_est[,t]\n",
    "\t\tanswer[[t]] = cbind(Y_test[[t]], predicted)\n",
    "\t}\n",
    "\tanswer\n",
    "}\n",
    "\n",
    "avg_perm <- function(mse_lst){\n",
    "\tfd = length(mse_lst)\n",
    "\tP = length(mse_lst[[1]])\n",
    "\trsq = mse = adj_mse = matrix(0, fd, P)\n",
    "\tfor(f in 1:fd){\n",
    "\t\tfor(t in 1:P){\n",
    "\t\t\trsq[f,t] = (cor(mse_lst[[f]][[t]])[1,2])^2\n",
    "\t\t\tmse[f,t] = mean((mse_lst[[f]][[t]][,1]-mse_lst[[f]][[t]][,2])^2)\n",
    "\t\t\tadj_mse[f,t] = mse[f,t]/var(mse_lst[[f]][[t]][,1])\n",
    "\t\t}\n",
    "\t}\n",
    "\tcbind(apply(rsq, 2, mean), apply(mse, 2, mean), apply(adj_mse, 2, mean))\n",
    "\n",
    "\t#list(rsq = apply(rsq, 2, mean), mse = apply(mse, 2, mean), adj_mse = apply(adj_mse, 2, mean))\n",
    "}\n",
    "\n",
    "## group lasso on data with missing covariates, use validation data for stopping criterion\n",
    "glasso <- function(X, Y, X1, Y1, XX, XY, Xnorm, lambda1, lambda2, theta, stepsize = 1e-4, maxiter = 50, eps = 1e-3, verbose = FALSE){\n",
    "\tBgt = Sys.time()\n",
    "\tM = nrow(XY)\n",
    "\tP = length(X)\n",
    "\tNN = unlist(lapply(X, nrow))\n",
    "\told_objV1 = rep(0,P)\n",
    "\tfor(t in 1:P){\n",
    "\t\told_objV1[t] = 1/2*mean((Y[[t]]-X[[t]]%*%theta[,t])^2)\n",
    "\t}\n",
    "  if (verbose) {\n",
    "    cat(\"Training error: \", old_objV1, '\\n')\t\n",
    "  }\n",
    "\told_objV2 = rep(0,P)\n",
    "\tfor(t in 1:P){\n",
    "\t\told_objV2[t] = 1/2*mean((Y1[[t]]-X1[[t]]%*%theta[,t])^2)\n",
    "\t}\n",
    "\tif (verbose) {\n",
    "    cat(\"Testing error: \", old_objV2, '\\n')\n",
    "  }\n",
    "\tbeta_j_lasso = rep(0, P)\n",
    "\ttmp_XYj = 0\n",
    "\tif(!is.loaded(\"wrapper\")){\n",
    "\t\tdyn.load(\"/ysm-gpfs/pi/zhao/from_louise/yh367/GTEX/codes/glasso.so\") # change this to the abs path to optim.so\n",
    "\t}\n",
    "\tfor(i in 1:maxiter){\n",
    "\t\tbgt = Sys.time()\n",
    "\t\tres = .Call(\"wrapper\", XX, XY, theta, M, P, beta_j_lasso, lambda1, lambda2, Xnorm)\n",
    "\t\tedt = Sys.time()\n",
    "\t\t\n",
    "\t\tnew_objV1 = new_objV2 = rep(0,P)\n",
    "\t\tfor(t in 1:P){\n",
    "\t\t\tnew_objV1[t] = 1/2*mean((Y[[t]]-X[[t]]%*%theta[,t])^2)\n",
    "\t\t}\n",
    "\t\tif (verbose) {cat(\"Training error: \", new_objV1, '\\n')}\n",
    "\t\tfor(t in 1:P){\n",
    "\t\t\tnew_objV2[t] = 1/2*mean((Y1[[t]]-X1[[t]]%*%theta[,t])^2)\n",
    "\t\t}\n",
    "\t\tif (verbose) {cat(\"Testing error: \", new_objV2, '\\n')}\n",
    "\t\tif(mean(new_objV2) > mean(old_objV2)|mean(new_objV1) > mean(old_objV1)){\n",
    "\t\t\tbreak\n",
    "\t\t}else{\n",
    "\t\t\told_objV2 = new_objV2\n",
    "\t\t}\n",
    "\t\tif(max(abs(new_objV1-old_objV1)) < eps){\n",
    "\t\t\tbreak\n",
    "\t\t}else{\n",
    "\t\t\told_objV1 = new_objV1\n",
    "\t\t}\n",
    "\t}\n",
    "\tEdt = Sys.time()\n",
    "\tcat(\"total training time: \", Edt-Bgt, \"\\n\")\n",
    "\tlist(est = theta, avg_tune_err = mean(new_objV2), tune_err=new_objV2)\n",
    "}\n",
    "\n",
    "## simpler version of glasso, train model until converges \n",
    "glasso_no_early_stopping <- function(X, Y, XX, XY, Xnorm, lambda1, lambda2, theta, stepsize = 1e-4, maxiter = 50, eps = 1e-3, verbose = FALSE){\n",
    "  cat(\"running glasso_no_early_stopping\\n\")\n",
    "\tM = nrow(XY)\n",
    "\tP = length(X)\n",
    "\tNN = unlist(lapply(X, nrow))\n",
    "\told_objV1 = rep(0,P)\n",
    "\tfor(t in 1:P){\n",
    "\t\told_objV1[t] = 1/2*mean((Y[[t]]-X[[t]]%*%theta[,t])^2)\n",
    "\t}\n",
    "\tif (verbose) {cat(\"Training error: \", mean(old_objV1), '\\n')}\n",
    "\tbeta_j_lasso = rep(0, P)\n",
    "\ttmp_XYj = 0\n",
    "\tif(!is.loaded(\"wrapper\")){\n",
    "\t\tdyn.load(\"optim.so\")\n",
    "\t}\n",
    "\tfor(i in 1:maxiter){\n",
    "\t\tres = .Call(\"wrapper\", XX, XY, theta, M, P, beta_j_lasso, lambda1, lambda2, Xnorm)\n",
    "\t\tnew_objV1 = rep(0,P)\n",
    "\t\tfor(t in 1:P){\n",
    "\t\t\tnew_objV1[t] = 1/2*mean((Y[[t]]-X[[t]]%*%theta[,t])^2)\n",
    "\t\t}\n",
    "\t\tif (verbose) {cat(\"Training error: \", mean(new_objV1), '\\n')}\n",
    "\t\tif(max(abs(new_objV1-old_objV1)) < eps|mean(new_objV1) > mean(old_objV1)){\n",
    "\t\t\tbreak\n",
    "\t\t}else{\n",
    "\t\t\told_objV1 = new_objV1\n",
    "\t\t}\n",
    "\t}\n",
    "\tlist(est = theta, avg_train_err = mean(new_objV1), train_err = new_objV1)\n",
    "}\n",
    "\n",
    "extract_genotype <- function(gene_idx, chr_str, genotype_dir, gene_vec) {\n",
    "    gene_dir <- paste0(genotype_dir, chr_str, gene_vec[gene_idx], \"/\", gene_vec[gene_idx])\n",
    "    genotype_info <- read_plink(gene_dir)\n",
    "    \n",
    "    print(paste0(\"INFO: genotype matrix dimension:\", dim(genotype_info$X)[1], \" * \", dim(genotype_info$X)[2]))\n",
    "    return(genotype_info)\n",
    "}\n",
    "\n",
    "\n",
    "#-------------------------- main ---------------------------#  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# create dirs\n",
    "dir.create(paste0(output_dir, \"chr\", chr), showWarnings = FALSE)\n",
    "dir.create(paste0(output_dir, \"chr\", chr, \"/\", gene_id), showWarnings = FALSE)\n",
    "setwd(paste0(output_dir, \"chr\", chr, \"/\", gene_id))\n",
    "\n",
    "# expr files \n",
    "cat(\"INFO: loading expression files...\")\n",
    "Y = list()\n",
    "T_idx = c()\n",
    "for(t in 1:P){\n",
    "    tmp_exp = read.table(paste0(gtex_dir, \"adjusted_expr/chr\", chr, \"/\", g, \"/\", Yt[t]), header=F)\n",
    "    # check if the y is constant\n",
    "    if (!all(tmp_exp[,2] == tmp_exp[1,2])) {\n",
    "        T_idx = c(T_idx, t)\n",
    "        Y[[length(T_idx)]] = tmp_exp \n",
    "    }\n",
    "     \n",
    "}\n",
    "Yt = Yt[T_idx]\n",
    "ssize = unlist(lapply(Y, nrow))\n",
    "T_num = length(Yt)\n",
    "P = T_num\n",
    "\n",
    "# genotype files \n",
    "cat(\"INFO: loading genotype files...\")\t\n",
    "genotype_info <- read_plink(dose_path)\n",
    "dose = genotype_info$X  #[1:2000, ]\n",
    "dose_std = dose\n",
    "\n",
    "# center the dose\n",
    "for(j in 1:nrow(dose)){\n",
    "    dose_std[j, is.na(dose[j, ])] <- mean(dose[j, ], na.rm = T)\n",
    "    dose_std[j, ] <- dose_std[j, ] - mean(dose[j, ], na.rm = T)\n",
    "}\n",
    "\n",
    "# covariance matrix \n",
    "n_sample = ncol(dose_std)\n",
    "tmp = t(as.matrix(dose_std))\n",
    "XX = t(tmp)%*%as.matrix(tmp)/n_sample\n",
    "Xnorm = diag(XX)\n",
    "remove(tmp)\n",
    "remove(XX)\n",
    "sub_id = colnames(dose_std)\n",
    "\n",
    "# number of snps\n",
    "M = nrow(dose_std)\n",
    "sub_id_map = list()\n",
    "sub_id_map_exp = list()\n",
    "\n",
    "# sample matching \n",
    "for(t in 1:T_num){\n",
    "    #tmp = rep(0, nrow(Y[[t]]))\n",
    "    exp_id <- as.character(sapply(Y[[t]][,1], function(x) substr(x, 1, 10)))\n",
    "    # index of ids that have matched genotypes \n",
    "    # exp_id based on order of exp_id\n",
    "    match_id <- match(exp_id, sub_id)\n",
    "    sub_id_map[[t]] <- na.omit(match_id)\n",
    "    sub_id_map_exp[[t]] <- !is.na(match_id)  \n",
    "}\n",
    "\n",
    "# cv                                   \n",
    "fold = 5\n",
    "#IR                                \n",
    "IRdisplay::display_html(\"INFO: CV preparation\")\n",
    "cv_config = cv_helper(n_sample, fold)\n",
    "cv_perm = cv_config$perm\n",
    "cv_idx = cv_config$idx\n",
    "\n",
    "single_res_test = list()\n",
    "single_lam = matrix(0, fold, T_num)\n",
    "single_theta_est = list()\n",
    "\t\n",
    "multi_res_test = list()\n",
    "multi_lam = matrix(0, fold, 2)\n",
    "multi_theta_est = list()\n",
    "\n",
    "multi_res_test2 = list()\n",
    "multi_lam2 = array(0, dim=c(fold, T_num, 2))\n",
    "multi_theta_est2 = list()\n",
    "                                  \n",
    "# loading fast matrix operations \n",
    "#sourceCpp(\"/gpfs/project/zhao/zy92/utmost_update/CTIMP/MatrixMtp.cpp\") # call the C++ file and we have three functions as armaMatMult，eigenMatMult，eigenMapMatMult.\n",
    "sourceCpp(\"/gpfs/loomis/project/zhao/zy92/utmost_update/CTIMP/MatrixMtp.cpp\") # call the C++ file and we have three functions as armaMatMult，eigenMatMult，eigenMapMatMult.\n",
    "\n",
    "\n",
    "res_tune = list()\n",
    "rec_lamv = matrix(0, fold, ntune)                                 \n",
    "                                  \n",
    "for(f in 1:fold){\n",
    "#for(f in 1:1){\n",
    "    bgt = Sys.time()\n",
    "    IRdisplay::display_html(paste0(\"INFO: fold \", f))\n",
    "    test_index = cv_perm[cv_idx[f,1]:cv_idx[f,2]]\n",
    "    test_id = sub_id[test_index]\n",
    "\n",
    "    # move the tuning idx to another idx block\n",
    "    tuning_index = cv_perm[cv_idx[f%%fold+1,1]:cv_idx[f%%fold+1,2]]\n",
    "    tuning_id = sub_id[tuning_index]\n",
    "\n",
    "    # idx list\n",
    "    X_test = list()\n",
    "    Y_test = list()\n",
    "    X_tune = list()\n",
    "    Y_tune = list()\n",
    "    X_train = list()\n",
    "    Y_train = list()\n",
    "    for(t in 1:T_num){\n",
    "        # idx matching  \n",
    "        id_filter = !(sub_id_map[[t]]%in%c(test_index,tuning_index))\n",
    "        X_train_tmp = sub_id_map[[t]][id_filter]\n",
    "        Y_train_tmp = which(sub_id_map_exp[[t]] == T)[id_filter]\n",
    "        tuning_id_filter = sub_id_map[[t]]%in%tuning_index\n",
    "        X_tuning_tmp = sub_id_map[[t]][tuning_id_filter]\n",
    "        Y_tuning_tmp = which(sub_id_map_exp[[t]] == T)[tuning_id_filter]\n",
    "        testing_id_filter = sub_id_map[[t]]%in%test_index\n",
    "        X_test_tmp = sub_id_map[[t]][testing_id_filter]\n",
    "        Y_test_tmp = which(sub_id_map_exp[[t]] == T)[testing_id_filter]\n",
    "        # training data\n",
    "        X_train[[t]] = apply(as.matrix(dose_std[,X_train_tmp]),1,as.numeric)\n",
    "        Y_train[[t]] = Y[[t]][Y_train_tmp, 2]\n",
    "        X_tune[[t]] = apply(as.matrix(dose_std[,X_tuning_tmp]),1,as.numeric)\n",
    "        Y_tune[[t]] = Y[[t]][Y_tuning_tmp, 2]\n",
    "        X_test[[t]] = apply(as.matrix(dose_std[,X_test_tmp]),1,as.numeric)\n",
    "        Y_test[[t]] = Y[[t]][Y_test_tmp, 2]\n",
    "        #IRdisplay::display_html(length(X_train_tmp))\n",
    "        #IRdisplay::display_html(length(Y_train_tmp))\n",
    "        #IRdisplay::display_html(dim(X_train[[t]]))\n",
    "        #IRdisplay::display_html(length(Y_train[[t]]))\n",
    "    }\n",
    "\n",
    "    ## model training ##\t\n",
    "    ## train elastic net and used average lambda as tuning parameters ##\n",
    "    single_initial_est = matrix(0, ncol(X_train[[1]]), T_num)\n",
    "    single_summary = list()\n",
    "    for(t in 1:T_num) {\n",
    "        #print(Y_train[[t]])\n",
    "        #print(sum(is.na(X_train[[t]])))\n",
    "        if (t %% 5 == 0) {\n",
    "            IRdisplay::display_html(paste0(\"INFO: glmnet cv tissue\", t))\n",
    "        }\n",
    "        \n",
    "        \n",
    "        tt = cv.glmnet(X_train[[t]], Y_train[[t]], alpha = 0.5, nfolds = 5)\n",
    "        single_summary[[t]] = tt\n",
    "        single_initial_est[,t] = tt$glmnet.fit$beta[,which.min(tt$cvm)]\n",
    "    }\n",
    "    ## performance of Elastic net on tuning and testing data with various tuning parameters\n",
    "    els_output = elastic_net_mse(single_summary, X_tune, Y_tune, X_test, Y_test)\n",
    "    single_res_test[[f]] = els_output$mse\n",
    "    single_lam[f,] = els_output$lam\n",
    "    single_theta_est[[f]] = els_output$est\n",
    "    remove(els_output)\n",
    "\n",
    "    \n",
    "    ## use elastic net ests row norm as weights ##\n",
    "    lam_range = minmax_lambda(single_summary)\n",
    "    sig_norm = apply(single_initial_est, 1, function(x){sqrt(sum(x^2))})\n",
    "    sig_norm[sig_norm==0] = rep(min(sig_norm[sig_norm>0]), sum(sig_norm==0))/2\n",
    "    sig_norm = sig_norm/sum(sig_norm)\n",
    "    weights2 = 1/sig_norm; weights2 = weights2/sum(weights2);\n",
    "\n",
    "    tis_norm = apply(single_initial_est, 2, function(x){sum(abs(x))})\n",
    "    tis_norm[tis_norm==0] = rep(min(tis_norm[tis_norm>0]), sum(tis_norm==0))/2\n",
    "    tis_norm = tis_norm/sum(tis_norm)\n",
    "    weights1 = 1/tis_norm; weights1 = weights1/sum(weights1);\n",
    "    lam_V = seq(lam_range[1], lam_range[2], length.out = ntune)\n",
    "    \n",
    "    # initial value\n",
    "    initial_numeric = as.numeric(single_initial_est)\n",
    "\tremove(single_summary)\n",
    "    remove(single_initial_est)\n",
    "\n",
    "\n",
    "    #-------------------------- train - validate - test ---------------------------#\n",
    "    IRdisplay::display_html('starting train-validate-test\\n')\n",
    "    ## preparation\n",
    "    XY = grad_prep(X_train, Y_train)\n",
    "    #bgt = Sys.time()\n",
    "    XX_train = lapply(X_train, function(x) { eigenMatMult(t(x), x)/nrow(x)})\n",
    "    #edt = Sys.time()\n",
    "    #print(edt-bgt)\n",
    "    \n",
    "    spsz = unlist(lapply(X_train,nrow))\n",
    "    #res_tune = rep(0, ntune)\n",
    "    res_tune[[f]] = array(-1, dim=c(ntune, ntune, T_num))\n",
    "    #best.lam = 0\n",
    "    rec_lamv[f,] = lam_V\n",
    "    for(lam1 in 1:ntune){\n",
    "        for(lam2 in 1:ntune){\n",
    "            single_est = matrix(initial_numeric, M, T_num )\n",
    "            ans = glasso(X=X_train, Y=Y_train, X1=X_tune, Y1=Y_tune, XX=XX_train, XY=XY, Xnorm=Xnorm, lambda1=lam_V[lam1]/spsz, lambda2=lam_V[lam2], theta=single_est, verbose = if_verbose)\n",
    "            if(sum(ans$est!=0)>0){\n",
    "                res_tune[[f]][lam1,lam2, ] = ans$tune_err\n",
    "                if (if_verbose) { cat(\"lambda1=\",lam_V[lam1], \"; lambda2=\", lam_V[lam2], \"; avg tune err=\", ans$avg_tune_err, '\\n') }\n",
    "                remove(single_est); remove(ans);\n",
    "            }else{\n",
    "                remove(single_est); remove(ans);\n",
    "                break\n",
    "            }\t\t\t\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    \n",
    "    #-------------------------- save results on test set for evaluation ---------------------------#\n",
    "    IRdisplay::display_html(\"saving cross-validation results for evaluation and analysis\\n\")\n",
    "    avg_tune_res = apply(res_tune[[f]], c(1,2), mean)\n",
    "    best.lam = which(avg_tune_res == min(avg_tune_res[avg_tune_res>=0]), arr.ind = TRUE)[1,]\n",
    "    single_est = matrix(initial_numeric, M, P)\n",
    "    ans = glasso(X=X_train, Y=Y_train, X1=X_tune, Y1=Y_tune, XX=XX_train, XY=XY, Xnorm=Xnorm, lambda1=lam_V[best.lam[1]]/spsz, lambda2=lam_V[best.lam[2]], theta=single_est, verbose = if_verbose)\n",
    "    multi_res_test[[f]] = multi_mse(ans$est, X_test, Y_test)\n",
    "    multi_lam[f,] = lam_V[best.lam]\n",
    "    multi_theta_est[[f]] = ans$est\n",
    "    remove(single_est)\n",
    "    remove(ans)\n",
    "\n",
    "    \n",
    "    edt = Sys.time()\n",
    "    print(edt-bgt)\n",
    "} # end of cv loop\n",
    "edt_origin = Sys.time() \n",
    "print(\"Final Time Cost: \")                                 \n",
    "print(edt_origin-bgt_origin)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bgt = Sys.time()\n",
    "\n",
    "\n",
    "k = \"1\"\n",
    "X_all = list()\n",
    "Y_all = list()\n",
    "for(t in 1:T_num){\n",
    "    X_all_tmp = sub_id_map[[t]]\n",
    "    X_all[[t]] = apply(as.matrix(dose_std[,X_all_tmp]),1,as.numeric)\n",
    "    Y_all[[t]] = Y[[t]][which(sub_id_map_exp[[t]] == T), 2]\n",
    "}\n",
    "# initial values \n",
    "single_initial_est = matrix(0, ncol(X_train[[1]]), T_num)\n",
    "for(t in 1:T_num){\n",
    "    IRdisplay::display_html(paste0(\"Tissue \", t))\n",
    "    tt = cv.glmnet(X_all[[t]], Y_all[[t]], alpha = 0.5, nfolds = 5)\n",
    "    single_initial_est[,t] = tt$glmnet.fit$beta[,which.min(tt$cvm)]\n",
    "}\n",
    "\n",
    "sig_norm = apply(single_initial_est, 1, function(x){sqrt(sum(x^2))})\n",
    "sig_norm[sig_norm==0] = rep(min(sig_norm[sig_norm>0]), sum(sig_norm==0))/2\n",
    "sig_norm = sig_norm/sum(sig_norm)\n",
    "weights2 = 1/sig_norm; weights2 = weights2/sum(weights2);\n",
    "\n",
    "tis_norm = apply(single_initial_est, 2, function(x){sum(abs(x))})\n",
    "tis_norm[tis_norm==0] = rep(min(tis_norm[tis_norm>0]), sum(tis_norm==0))/2\n",
    "tis_norm = tis_norm/sum(tis_norm)\n",
    "weights1 = 1/tis_norm; weights1 = weights1/sum(weights1);\n",
    "\n",
    "spsz = unlist(lapply(X_all,nrow))\n",
    "initial_numeric = as.numeric(single_initial_est)\n",
    "#remove(single_initial_est)\n",
    "XY = grad_prep(X_all, Y_all)\n",
    "XX_all = lapply(X_all, function(x){t(x)%*%x/nrow(x)})\n",
    "tmp_res = rep(0, fold)\n",
    "for(f in 1:fold){\n",
    "    ans = glasso_no_early_stopping(X=X_all, Y=Y_all, XX=XX_all, XY=XY, Xnorm=Xnorm, lambda1=multi_lam[f,1]/spsz, lambda2=multi_lam[f,2], theta=matrix(initial_numeric,M,P), verbose = if_verbose)\n",
    "    tmp_res[f] = ans$avg_train_err\n",
    "}\n",
    "final.lam = multi_lam[which.min(tmp_res),]\n",
    "ans = glasso_no_early_stopping(X=X_all, Y=Y_all, XX=XX_all, XY=XY, Xnorm=Xnorm, lambda1=final.lam[1]/spsz, lambda2=final.lam[2], theta=matrix(initial_numeric,M,P), verbose = if_verbose)\n",
    "#info = read.table(info_path, header=T, sep='\\t')\n",
    "#downstream_est = data.frame(info[,1:3], ans$est)\n",
    "multi_all_res = multi_mse(ans$est, X_all, Y_all)\n",
    "single_all_res = multi_mse(single_initial_est, X_all, Y_all)\n",
    "\n",
    "#info = read.table(info_path, header=T, sep='\\t')\n",
    "info = genotype_info$bim\n",
    "#downstream_est = data.frame(info[1:2000, ], ans$est)\n",
    "downstream_est = data.frame(info, ans$est)\n",
    "edt = Sys.time()\n",
    "print(edt-bgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim(downstream_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A data.frame: 100 × 50</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>chr</th><th scope=col>id</th><th scope=col>posg</th><th scope=col>pos</th><th scope=col>ref</th><th scope=col>alt</th><th scope=col>X1</th><th scope=col>X2</th><th scope=col>X3</th><th scope=col>X4</th><th scope=col>⋯</th><th scope=col>X35</th><th scope=col>X36</th><th scope=col>X37</th><th scope=col>X38</th><th scope=col>X39</th><th scope=col>X40</th><th scope=col>X41</th><th scope=col>X42</th><th scope=col>X43</th><th scope=col>X44</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;chr&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>⋯</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>21</td><td>chr21_40464521_A_G_b38</td><td>0</td><td>40464521</td><td>G</td><td>A</td><td> 0.00000000</td><td> 0.000000000</td><td>0.0000000000</td><td>0.0000000</td><td>⋯</td><td> 0.51658051</td><td> 0.0000000000</td><td>0</td><td>0</td><td>0</td><td>0</td><td> 0.00000000</td><td>0.0000000</td><td>  1.74503598</td><td>-6.310849722</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>21</td><td>chr21_40464616_T_C_b38</td><td>0</td><td>40464616</td><td>C</td><td>T</td><td> 0.00000000</td><td> 0.000000000</td><td>0.0000000000</td><td>0.0000000</td><td>⋯</td><td> 0.00000000</td><td> 0.0000000000</td><td>0</td><td>0</td><td>0</td><td>0</td><td> 0.00000000</td><td>0.0000000</td><td>  0.00000000</td><td> 1.065904915</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>21</td><td>chr21_40464797_G_A_b38</td><td>0</td><td>40464797</td><td>A</td><td>G</td><td> 1.21408863</td><td> 0.412646335</td><td>0.0000000000</td><td>0.0000000</td><td>⋯</td><td> 0.00000000</td><td> 0.0000000000</td><td>0</td><td>0</td><td>0</td><td>0</td><td> 0.00000000</td><td>0.0000000</td><td> 13.20173550</td><td> 0.435323378</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>21</td><td>chr21_40464826_T_C_b38</td><td>0</td><td>40464826</td><td>C</td><td>T</td><td> 0.05894808</td><td> 0.093197033</td><td>0.0000000000</td><td>0.0000000</td><td>⋯</td><td> 0.61932982</td><td> 0.0000000000</td><td>0</td><td>0</td><td>0</td><td>0</td><td> 0.00000000</td><td>0.0000000</td><td> -0.44801787</td><td> 0.418455795</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>21</td><td>chr21_40464961_G_A_b38</td><td>0</td><td>40464961</td><td>A</td><td>G</td><td> 0.00000000</td><td> 0.000000000</td><td>0.0000000000</td><td>0.0000000</td><td>⋯</td><td> 0.00000000</td><td> 0.0000000000</td><td>0</td><td>0</td><td>0</td><td>0</td><td> 0.00000000</td><td>0.0000000</td><td>  2.37398822</td><td> 0.000000000</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>21</td><td>chr21_40465271_T_C_b38</td><td>0</td><td>40465271</td><td>T</td><td>C</td><td>-1.14559691</td><td>-0.621397534</td><td>0.0000000000</td><td>0.0000000</td><td>⋯</td><td> 0.00000000</td><td> 0.0000000000</td><td>0</td><td>0</td><td>0</td><td>0</td><td> 0.00000000</td><td>0.0000000</td><td> -0.95123888</td><td> 0.000000000</td></tr>\n",
       "\t<tr><th scope=row>7</th><td>21</td><td>chr21_40465366_C_T_b38</td><td>0</td><td>40465366</td><td>T</td><td>C</td><td> 0.14040855</td><td> 0.000000000</td><td>0.0000000000</td><td>0.0000000</td><td>⋯</td><td>-0.19192752</td><td> 0.0000000000</td><td>0</td><td>0</td><td>0</td><td>0</td><td> 0.00000000</td><td>0.0000000</td><td>  1.29474492</td><td>-1.586051395</td></tr>\n",
       "\t<tr><th scope=row>8</th><td>21</td><td>chr21_40466036_C_T_b38</td><td>0</td><td>40466036</td><td>T</td><td>C</td><td> 0.00000000</td><td> 0.000000000</td><td>0.0000000000</td><td>0.0000000</td><td>⋯</td><td> 0.00000000</td><td> 0.0000000000</td><td>0</td><td>0</td><td>0</td><td>0</td><td> 0.00000000</td><td>0.0000000</td><td>  0.00000000</td><td> 0.000000000</td></tr>\n",
       "\t<tr><th scope=row>9</th><td>21</td><td>chr21_40466218_C_A_b38</td><td>0</td><td>40466218</td><td>A</td><td>C</td><td>-1.01716370</td><td> 0.000000000</td><td>0.0000000000</td><td>0.1373444</td><td>⋯</td><td> 0.00000000</td><td> 0.0560647734</td><td>0</td><td>0</td><td>0</td><td>0</td><td>-0.21285198</td><td>0.0000000</td><td> -0.35263763</td><td> 0.000000000</td></tr>\n",
       "\t<tr><th scope=row>10</th><td>21</td><td>chr21_40466226_C_G_b38</td><td>0</td><td>40466226</td><td>G</td><td>C</td><td> 0.08874066</td><td> 0.407203323</td><td>0.0000000000</td><td>0.0000000</td><td>⋯</td><td> 0.00000000</td><td> 0.0000000000</td><td>0</td><td>0</td><td>0</td><td>0</td><td> 0.00000000</td><td>0.1133115</td><td> -0.16511971</td><td> 0.191428076</td></tr>\n",
       "\t<tr><th scope=row>11</th><td>21</td><td>chr21_40466298_C_T_b38</td><td>0</td><td>40466298</td><td>T</td><td>C</td><td> 0.00000000</td><td> 0.000000000</td><td>0.0000000000</td><td>0.0000000</td><td>⋯</td><td>-8.17513243</td><td> 0.0000000000</td><td>0</td><td>0</td><td>0</td><td>0</td><td> 0.00000000</td><td>0.0000000</td><td> -3.35203383</td><td> 0.000000000</td></tr>\n",
       "\t<tr><th scope=row>12</th><td>21</td><td>chr21_40466719_A_G_b38</td><td>0</td><td>40466719</td><td>G</td><td>A</td><td> 0.00000000</td><td> 0.000000000</td><td>0.0000000000</td><td>0.0000000</td><td>⋯</td><td> 0.00000000</td><td> 0.0000000000</td><td>0</td><td>0</td><td>0</td><td>0</td><td> 0.00000000</td><td>0.0000000</td><td>  0.00000000</td><td> 0.000000000</td></tr>\n",
       "\t<tr><th scope=row>13</th><td>21</td><td>chr21_40466817_T_G_b38</td><td>0</td><td>40466817</td><td>T</td><td>G</td><td>-0.18212471</td><td>-0.469490894</td><td>0.0000000000</td><td>0.0000000</td><td>⋯</td><td>-0.64980559</td><td> 0.0000000000</td><td>0</td><td>0</td><td>0</td><td>0</td><td> 0.00000000</td><td>0.0000000</td><td> -0.75534044</td><td>-0.310564480</td></tr>\n",
       "\t<tr><th scope=row>14</th><td>21</td><td>chr21_40466922_C_T_b38</td><td>0</td><td>40466922</td><td>T</td><td>C</td><td> 0.00000000</td><td> 0.000000000</td><td>0.0000000000</td><td>0.0000000</td><td>⋯</td><td> 0.00000000</td><td> 0.0000000000</td><td>0</td><td>0</td><td>0</td><td>0</td><td> 0.00000000</td><td>0.0000000</td><td>  0.00000000</td><td> 0.000000000</td></tr>\n",
       "\t<tr><th scope=row>15</th><td>21</td><td>chr21_40467145_C_A_b38</td><td>0</td><td>40467145</td><td>A</td><td>C</td><td> 0.00000000</td><td> 0.000000000</td><td>0.0000000000</td><td>0.0000000</td><td>⋯</td><td> 0.00000000</td><td> 0.0000000000</td><td>0</td><td>0</td><td>0</td><td>0</td><td> 0.00000000</td><td>0.0000000</td><td>  0.00000000</td><td> 0.000000000</td></tr>\n",
       "\t<tr><th scope=row>16</th><td>21</td><td>chr21_40467151_A_G_b38</td><td>0</td><td>40467151</td><td>G</td><td>A</td><td> 0.00000000</td><td> 0.000000000</td><td>0.0000000000</td><td>0.0000000</td><td>⋯</td><td> 0.00000000</td><td> 0.0000000000</td><td>0</td><td>0</td><td>0</td><td>0</td><td> 0.00000000</td><td>0.0000000</td><td>  0.00000000</td><td> 0.000000000</td></tr>\n",
       "\t<tr><th scope=row>17</th><td>21</td><td>chr21_40467257_C_A_b38</td><td>0</td><td>40467257</td><td>A</td><td>C</td><td> 0.01527602</td><td> 0.000000000</td><td>0.0000000000</td><td>0.0000000</td><td>⋯</td><td> 0.00000000</td><td> 0.0000000000</td><td>0</td><td>0</td><td>0</td><td>0</td><td> 0.00000000</td><td>0.0000000</td><td>  0.46456820</td><td>-0.224299209</td></tr>\n",
       "\t<tr><th scope=row>18</th><td>21</td><td>chr21_40467282_A_G_b38</td><td>0</td><td>40467282</td><td>A</td><td>G</td><td>-0.02039542</td><td>-0.007303370</td><td>0.0000000000</td><td>0.0000000</td><td>⋯</td><td>-0.07969421</td><td> 0.0000000000</td><td>0</td><td>0</td><td>0</td><td>0</td><td> 0.00000000</td><td>0.0000000</td><td>  0.00000000</td><td>-0.009024466</td></tr>\n",
       "\t<tr><th scope=row>19</th><td>21</td><td>chr21_40467492_G_A_b38</td><td>0</td><td>40467492</td><td>A</td><td>G</td><td> 0.00000000</td><td> 0.000000000</td><td>0.0000000000</td><td>0.0000000</td><td>⋯</td><td> 0.00000000</td><td> 0.0000000000</td><td>0</td><td>0</td><td>0</td><td>0</td><td> 0.00000000</td><td>0.0000000</td><td>  0.13205108</td><td> 0.000000000</td></tr>\n",
       "\t<tr><th scope=row>20</th><td>21</td><td>chr21_40467555_G_A_b38</td><td>0</td><td>40467555</td><td>A</td><td>G</td><td> 0.00000000</td><td> 0.000000000</td><td>0.0000000000</td><td>0.0000000</td><td>⋯</td><td> 0.00000000</td><td> 0.0000000000</td><td>0</td><td>0</td><td>0</td><td>0</td><td> 0.00000000</td><td>0.0000000</td><td>  0.00000000</td><td> 0.000000000</td></tr>\n",
       "\t<tr><th scope=row>21</th><td>21</td><td>chr21_40467631_T_C_b38</td><td>0</td><td>40467631</td><td>C</td><td>T</td><td> 0.00000000</td><td> 0.021268218</td><td>0.0000000000</td><td>0.0000000</td><td>⋯</td><td> 0.34510382</td><td> 0.0000000000</td><td>0</td><td>0</td><td>0</td><td>0</td><td> 0.00000000</td><td>0.1929455</td><td> -0.01154045</td><td> 0.067111371</td></tr>\n",
       "\t<tr><th scope=row>22</th><td>21</td><td>chr21_40468623_C_T_b38</td><td>0</td><td>40468623</td><td>T</td><td>C</td><td> 5.02118844</td><td> 0.000000000</td><td>0.0000000000</td><td>0.0000000</td><td>⋯</td><td>19.16293034</td><td> 0.0000000000</td><td>0</td><td>0</td><td>0</td><td>0</td><td> 2.51650014</td><td>0.0000000</td><td>109.77058013</td><td> 0.000000000</td></tr>\n",
       "\t<tr><th scope=row>23</th><td>21</td><td>chr21_40468895_C_T_b38</td><td>0</td><td>40468895</td><td>T</td><td>C</td><td> 0.50660974</td><td> 0.000000000</td><td>0.0000000000</td><td>0.0000000</td><td>⋯</td><td> 0.00000000</td><td> 0.0000000000</td><td>0</td><td>0</td><td>0</td><td>0</td><td> 0.06714634</td><td>0.0000000</td><td>-14.51747781</td><td> 0.000000000</td></tr>\n",
       "\t<tr><th scope=row>24</th><td>21</td><td>chr21_40469112_C_T_b38</td><td>0</td><td>40469112</td><td>T</td><td>C</td><td> 0.00000000</td><td> 0.000000000</td><td>0.0000000000</td><td>0.0000000</td><td>⋯</td><td> 0.11253863</td><td> 0.0000000000</td><td>0</td><td>0</td><td>0</td><td>0</td><td> 0.00000000</td><td>0.0000000</td><td> -4.55768053</td><td> 0.000000000</td></tr>\n",
       "\t<tr><th scope=row>25</th><td>21</td><td>chr21_40469113_G_A_b38</td><td>0</td><td>40469113</td><td>A</td><td>G</td><td> 0.18169368</td><td> 0.000000000</td><td>0.0322169691</td><td>0.0000000</td><td>⋯</td><td> 0.00000000</td><td> 0.0000000000</td><td>0</td><td>0</td><td>0</td><td>0</td><td> 0.00000000</td><td>0.0000000</td><td>  0.18074787</td><td> 0.988612892</td></tr>\n",
       "\t<tr><th scope=row>26</th><td>21</td><td>chr21_40469207_T_C_b38</td><td>0</td><td>40469207</td><td>C</td><td>T</td><td> 0.00000000</td><td> 0.000000000</td><td>0.0000000000</td><td>0.0304143</td><td>⋯</td><td> 0.00000000</td><td> 0.0000000000</td><td>0</td><td>0</td><td>0</td><td>0</td><td> 0.00000000</td><td>0.0000000</td><td>  0.26624012</td><td> 0.000000000</td></tr>\n",
       "\t<tr><th scope=row>27</th><td>21</td><td>chr21_40469541_T_C_b38</td><td>0</td><td>40469541</td><td>C</td><td>T</td><td> 0.00000000</td><td> 0.000000000</td><td>0.0000000000</td><td>0.0000000</td><td>⋯</td><td> 0.00000000</td><td> 0.0000000000</td><td>0</td><td>0</td><td>0</td><td>0</td><td> 0.00000000</td><td>0.0000000</td><td>  0.25634093</td><td> 0.000000000</td></tr>\n",
       "\t<tr><th scope=row>28</th><td>21</td><td>chr21_40469863_T_C_b38</td><td>0</td><td>40469863</td><td>C</td><td>T</td><td> 0.00000000</td><td> 0.304632504</td><td>0.0035584119</td><td>0.0000000</td><td>⋯</td><td> 0.11373859</td><td>-0.0289917302</td><td>0</td><td>0</td><td>0</td><td>0</td><td> 0.00000000</td><td>0.0000000</td><td>  1.52545457</td><td> 0.273573716</td></tr>\n",
       "\t<tr><th scope=row>29</th><td>21</td><td>chr21_40470033_T_C_b38</td><td>0</td><td>40470033</td><td>C</td><td>T</td><td> 0.00000000</td><td> 0.003691257</td><td>0.0001258493</td><td>0.0000000</td><td>⋯</td><td> 0.00000000</td><td>-0.0009422035</td><td>0</td><td>0</td><td>0</td><td>0</td><td> 0.00000000</td><td>0.0000000</td><td>  0.00000000</td><td> 0.000000000</td></tr>\n",
       "\t<tr><th scope=row>30</th><td>21</td><td>chr21_40470325_A_G_b38</td><td>0</td><td>40470325</td><td>G</td><td>A</td><td> 0.00000000</td><td> 0.000000000</td><td>0.0000000000</td><td>0.0000000</td><td>⋯</td><td> 0.00000000</td><td> 0.0000000000</td><td>0</td><td>0</td><td>0</td><td>0</td><td> 0.00000000</td><td>0.0000000</td><td>  0.00000000</td><td> 0.000000000</td></tr>\n",
       "\t<tr><th scope=row>⋮</th><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋱</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td></tr>\n",
       "\t<tr><th scope=row>71</th><td>21</td><td>chr21_40478709_T_C_b38 </td><td>0</td><td>40478709</td><td>C</td><td>T </td><td> 0.000000000</td><td> 0.01979913</td><td>-0.014249596</td><td>0.0000000</td><td>⋯</td><td> 0.00000000</td><td>0.0000000</td><td>0</td><td>0</td><td>0</td><td> 0.000000000</td><td>0.00000000</td><td> 0.00000000</td><td> 0.00000000</td><td> 0.00000000</td></tr>\n",
       "\t<tr><th scope=row>72</th><td>21</td><td>chr21_40478829_C_T_b38 </td><td>0</td><td>40478829</td><td>T</td><td>C </td><td> 0.000000000</td><td> 0.00000000</td><td> 0.000000000</td><td>0.0000000</td><td>⋯</td><td> 0.00000000</td><td>0.0000000</td><td>0</td><td>0</td><td>0</td><td> 0.000000000</td><td>0.00000000</td><td> 0.00000000</td><td> 0.00000000</td><td> 0.00000000</td></tr>\n",
       "\t<tr><th scope=row>73</th><td>21</td><td>chr21_40479135_G_C_b38 </td><td>0</td><td>40479135</td><td>G</td><td>C </td><td> 0.000000000</td><td> 0.00000000</td><td> 0.000000000</td><td>0.0000000</td><td>⋯</td><td> 0.00000000</td><td>0.0000000</td><td>0</td><td>0</td><td>0</td><td> 0.000000000</td><td>0.00000000</td><td> 0.00000000</td><td>-0.59261757</td><td>-0.52196570</td></tr>\n",
       "\t<tr><th scope=row>74</th><td>21</td><td>chr21_40479139_A_G_b38 </td><td>0</td><td>40479139</td><td>G</td><td>A </td><td> 0.000000000</td><td> 0.00000000</td><td> 0.000000000</td><td>0.0000000</td><td>⋯</td><td>-1.07234054</td><td>0.0000000</td><td>0</td><td>0</td><td>0</td><td> 0.000000000</td><td>0.00000000</td><td> 0.00000000</td><td> 0.00000000</td><td> 0.00000000</td></tr>\n",
       "\t<tr><th scope=row>75</th><td>21</td><td>chr21_40479151_A_C_b38 </td><td>0</td><td>40479151</td><td>C</td><td>A </td><td>-0.421219115</td><td>-0.73545884</td><td> 0.000000000</td><td>0.0000000</td><td>⋯</td><td> 0.00000000</td><td>0.0000000</td><td>0</td><td>0</td><td>0</td><td> 0.000000000</td><td>0.00000000</td><td> 0.00000000</td><td> 0.31245314</td><td> 0.00000000</td></tr>\n",
       "\t<tr><th scope=row>76</th><td>21</td><td>chr21_40479453_G_T_b38 </td><td>0</td><td>40479453</td><td>T</td><td>G </td><td> 0.426510786</td><td> 0.53923671</td><td> 0.000000000</td><td>0.0000000</td><td>⋯</td><td> 1.81908880</td><td>0.0000000</td><td>0</td><td>0</td><td>0</td><td> 0.308440557</td><td>0.05384545</td><td> 0.00000000</td><td> 0.00000000</td><td> 0.00000000</td></tr>\n",
       "\t<tr><th scope=row>77</th><td>21</td><td>chr21_40479524_T_C_b38 </td><td>0</td><td>40479524</td><td>C</td><td>T </td><td>-0.115133343</td><td> 0.00000000</td><td>-0.004932322</td><td>0.0000000</td><td>⋯</td><td> 0.07255690</td><td>0.0000000</td><td>0</td><td>0</td><td>0</td><td> 0.000000000</td><td>0.00000000</td><td> 0.00000000</td><td>-0.55362686</td><td> 0.00000000</td></tr>\n",
       "\t<tr><th scope=row>78</th><td>21</td><td>chr21_40479580_A_C_b38 </td><td>0</td><td>40479580</td><td>A</td><td>C </td><td> 0.000000000</td><td> 0.00000000</td><td> 0.000000000</td><td>0.0000000</td><td>⋯</td><td> 0.00000000</td><td>0.0000000</td><td>0</td><td>0</td><td>0</td><td> 0.000000000</td><td>0.00000000</td><td> 0.00000000</td><td> 1.23042511</td><td> 0.00000000</td></tr>\n",
       "\t<tr><th scope=row>79</th><td>21</td><td>chr21_40479820_GT_G_b38</td><td>0</td><td>40479820</td><td>G</td><td>GT</td><td> 0.000000000</td><td> 0.00000000</td><td> 0.000000000</td><td>0.0000000</td><td>⋯</td><td> 0.00000000</td><td>0.0000000</td><td>0</td><td>0</td><td>0</td><td> 0.000000000</td><td>0.00000000</td><td> 0.00000000</td><td>20.87866043</td><td> 0.00000000</td></tr>\n",
       "\t<tr><th scope=row>80</th><td>21</td><td>chr21_40479827_T_C_b38 </td><td>0</td><td>40479827</td><td>C</td><td>T </td><td> 0.000000000</td><td> 0.00000000</td><td> 0.000000000</td><td>0.0000000</td><td>⋯</td><td> 0.00000000</td><td>0.0000000</td><td>0</td><td>0</td><td>0</td><td> 0.000000000</td><td>0.00000000</td><td>-0.05397932</td><td>-0.11315679</td><td> 0.00000000</td></tr>\n",
       "\t<tr><th scope=row>81</th><td>21</td><td>chr21_40479904_G_A_b38 </td><td>0</td><td>40479904</td><td>A</td><td>G </td><td> 0.000000000</td><td> 0.00000000</td><td> 0.000000000</td><td>0.0000000</td><td>⋯</td><td> 0.00000000</td><td>0.0000000</td><td>0</td><td>0</td><td>0</td><td> 0.000000000</td><td>0.00000000</td><td> 0.00000000</td><td>-0.73695233</td><td> 0.00000000</td></tr>\n",
       "\t<tr><th scope=row>82</th><td>21</td><td>chr21_40479952_T_C_b38 </td><td>0</td><td>40479952</td><td>C</td><td>T </td><td> 0.000000000</td><td> 0.00000000</td><td> 0.000000000</td><td>0.0000000</td><td>⋯</td><td> 0.00000000</td><td>0.0000000</td><td>0</td><td>0</td><td>0</td><td> 0.000000000</td><td>0.00000000</td><td> 0.00000000</td><td> 0.00000000</td><td> 0.00000000</td></tr>\n",
       "\t<tr><th scope=row>83</th><td>21</td><td>chr21_40480353_C_T_b38 </td><td>0</td><td>40480353</td><td>T</td><td>C </td><td> 0.000000000</td><td> 0.00000000</td><td> 0.000000000</td><td>0.0000000</td><td>⋯</td><td> 0.00000000</td><td>0.0000000</td><td>0</td><td>0</td><td>0</td><td> 0.000000000</td><td>0.00000000</td><td> 0.00000000</td><td> 0.00000000</td><td>-0.04823205</td></tr>\n",
       "\t<tr><th scope=row>84</th><td>21</td><td>chr21_40480595_A_G_b38 </td><td>0</td><td>40480595</td><td>A</td><td>G </td><td> 0.260493178</td><td> 0.00000000</td><td>-0.004994653</td><td>0.0000000</td><td>⋯</td><td> 0.00000000</td><td>0.0000000</td><td>0</td><td>0</td><td>0</td><td> 0.000000000</td><td>0.00000000</td><td> 0.00000000</td><td> 0.00000000</td><td> 1.08324279</td></tr>\n",
       "\t<tr><th scope=row>85</th><td>21</td><td>chr21_40480646_G_A_b38 </td><td>0</td><td>40480646</td><td>A</td><td>G </td><td> 0.000000000</td><td> 0.00000000</td><td> 0.000000000</td><td>0.0000000</td><td>⋯</td><td>-0.05013099</td><td>0.0000000</td><td>0</td><td>0</td><td>0</td><td> 0.000000000</td><td>0.00000000</td><td> 0.55435711</td><td>-0.84886145</td><td> 0.00000000</td></tr>\n",
       "\t<tr><th scope=row>86</th><td>21</td><td>chr21_40481005_A_C_b38 </td><td>0</td><td>40481005</td><td>C</td><td>A </td><td> 0.000000000</td><td> 0.00000000</td><td> 0.000000000</td><td>0.0000000</td><td>⋯</td><td> 0.00000000</td><td>0.0000000</td><td>0</td><td>0</td><td>0</td><td> 0.000000000</td><td>0.00000000</td><td> 0.00000000</td><td> 0.76126602</td><td> 0.00000000</td></tr>\n",
       "\t<tr><th scope=row>87</th><td>21</td><td>chr21_40481014_A_C_b38 </td><td>0</td><td>40481014</td><td>C</td><td>A </td><td> 0.000000000</td><td> 0.00000000</td><td> 0.000000000</td><td>0.0000000</td><td>⋯</td><td> 0.00000000</td><td>0.0000000</td><td>0</td><td>0</td><td>0</td><td> 0.000000000</td><td>0.00000000</td><td> 0.00000000</td><td> 0.00000000</td><td> 0.00000000</td></tr>\n",
       "\t<tr><th scope=row>88</th><td>21</td><td>chr21_40481081_T_C_b38 </td><td>0</td><td>40481081</td><td>C</td><td>T </td><td> 0.000000000</td><td> 2.23411921</td><td> 0.000000000</td><td>0.0000000</td><td>⋯</td><td> 3.35008832</td><td>0.0000000</td><td>0</td><td>0</td><td>0</td><td> 0.366440949</td><td>0.25566563</td><td> 0.00000000</td><td> 0.00000000</td><td> 0.00000000</td></tr>\n",
       "\t<tr><th scope=row>89</th><td>21</td><td>chr21_40481108_C_T_b38 </td><td>0</td><td>40481108</td><td>T</td><td>C </td><td> 0.000000000</td><td> 0.00000000</td><td> 0.000000000</td><td>0.0000000</td><td>⋯</td><td> 0.00000000</td><td>0.0000000</td><td>0</td><td>0</td><td>0</td><td> 0.000000000</td><td>0.00000000</td><td> 0.00000000</td><td> 0.00000000</td><td> 0.00000000</td></tr>\n",
       "\t<tr><th scope=row>90</th><td>21</td><td>chr21_40481255_C_T_b38 </td><td>0</td><td>40481255</td><td>T</td><td>C </td><td>-0.005341046</td><td> 0.00000000</td><td> 0.000000000</td><td>0.0000000</td><td>⋯</td><td> 0.41933173</td><td>0.0000000</td><td>0</td><td>0</td><td>0</td><td> 0.000000000</td><td>0.00000000</td><td> 0.00000000</td><td> 1.47070230</td><td> 0.07180171</td></tr>\n",
       "\t<tr><th scope=row>91</th><td>21</td><td>chr21_40481263_A_G_b38 </td><td>0</td><td>40481263</td><td>A</td><td>G </td><td> 0.000000000</td><td> 0.00000000</td><td> 0.000000000</td><td>0.0000000</td><td>⋯</td><td> 0.00000000</td><td>0.0000000</td><td>0</td><td>0</td><td>0</td><td> 0.000000000</td><td>0.00000000</td><td> 0.00000000</td><td> 0.13765407</td><td>-0.99649439</td></tr>\n",
       "\t<tr><th scope=row>92</th><td>21</td><td>chr21_40481754_T_C_b38 </td><td>0</td><td>40481754</td><td>C</td><td>T </td><td> 0.000000000</td><td> 0.00000000</td><td> 0.000000000</td><td>0.0000000</td><td>⋯</td><td> 0.00000000</td><td>0.0000000</td><td>0</td><td>0</td><td>0</td><td> 0.000000000</td><td>0.00000000</td><td> 0.00000000</td><td> 0.03493883</td><td>-0.10447977</td></tr>\n",
       "\t<tr><th scope=row>93</th><td>21</td><td>chr21_40481833_T_A_b38 </td><td>0</td><td>40481833</td><td>A</td><td>T </td><td> 0.000000000</td><td> 0.00000000</td><td> 0.000000000</td><td>0.0000000</td><td>⋯</td><td> 0.00000000</td><td>0.0000000</td><td>0</td><td>0</td><td>0</td><td> 0.000000000</td><td>0.00000000</td><td> 0.00000000</td><td> 0.00000000</td><td>-0.61700718</td></tr>\n",
       "\t<tr><th scope=row>94</th><td>21</td><td>chr21_40481865_G_A_b38 </td><td>0</td><td>40481865</td><td>A</td><td>G </td><td> 0.000000000</td><td> 0.00000000</td><td> 0.000000000</td><td>0.0000000</td><td>⋯</td><td> 0.00000000</td><td>0.0000000</td><td>0</td><td>0</td><td>0</td><td> 0.000000000</td><td>0.00000000</td><td> 0.00000000</td><td> 0.00000000</td><td> 0.00000000</td></tr>\n",
       "\t<tr><th scope=row>95</th><td>21</td><td>chr21_40482614_T_C_b38 </td><td>0</td><td>40482614</td><td>C</td><td>T </td><td> 0.000000000</td><td> 0.15583846</td><td> 0.000000000</td><td>0.0000000</td><td>⋯</td><td> 0.00000000</td><td>0.0000000</td><td>0</td><td>0</td><td>0</td><td> 0.000000000</td><td>0.00000000</td><td> 0.00000000</td><td> 1.24963186</td><td> 0.07724665</td></tr>\n",
       "\t<tr><th scope=row>96</th><td>21</td><td>chr21_40482734_T_C_b38 </td><td>0</td><td>40482734</td><td>C</td><td>T </td><td> 0.000000000</td><td> 0.00000000</td><td> 0.000000000</td><td>0.0000000</td><td>⋯</td><td> 1.17492048</td><td>0.0000000</td><td>0</td><td>0</td><td>0</td><td> 0.000000000</td><td>0.00000000</td><td> 0.00000000</td><td> 0.06903606</td><td>-1.04923034</td></tr>\n",
       "\t<tr><th scope=row>97</th><td>21</td><td>chr21_40482755_C_T_b38 </td><td>0</td><td>40482755</td><td>T</td><td>C </td><td> 0.000000000</td><td> 0.00000000</td><td> 0.000000000</td><td>0.0000000</td><td>⋯</td><td> 0.00000000</td><td>0.0000000</td><td>0</td><td>0</td><td>0</td><td>-0.004079066</td><td>0.00000000</td><td>-0.01280556</td><td> 0.16703515</td><td> 0.00000000</td></tr>\n",
       "\t<tr><th scope=row>98</th><td>21</td><td>chr21_40482866_T_A_b38 </td><td>0</td><td>40482866</td><td>T</td><td>A </td><td> 0.000000000</td><td> 0.00000000</td><td> 0.000000000</td><td>0.0000000</td><td>⋯</td><td> 0.00000000</td><td>0.0000000</td><td>0</td><td>0</td><td>0</td><td> 0.000000000</td><td>0.00000000</td><td> 0.00000000</td><td>-0.75027046</td><td>-0.63448982</td></tr>\n",
       "\t<tr><th scope=row>99</th><td>21</td><td>chr21_40483213_G_C_b38 </td><td>0</td><td>40483213</td><td>G</td><td>C </td><td> 0.000000000</td><td> 0.00000000</td><td> 0.000000000</td><td>0.3064531</td><td>⋯</td><td>-0.04824842</td><td>0.4758991</td><td>0</td><td>0</td><td>0</td><td> 0.000000000</td><td>0.00000000</td><td> 0.00000000</td><td> 0.00000000</td><td>-0.06443647</td></tr>\n",
       "\t<tr><th scope=row>100</th><td>21</td><td>chr21_40483657_T_C_b38 </td><td>0</td><td>40483657</td><td>C</td><td>T </td><td> 0.000000000</td><td> 0.00000000</td><td> 0.000000000</td><td>0.0000000</td><td>⋯</td><td>-0.20988650</td><td>0.0000000</td><td>0</td><td>0</td><td>0</td><td> 0.000000000</td><td>0.00000000</td><td> 0.00000000</td><td> 0.00000000</td><td> 0.00000000</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 100 × 50\n",
       "\\begin{tabular}{r|lllllllllllllllllllll}\n",
       "  & chr & id & posg & pos & ref & alt & X1 & X2 & X3 & X4 & ⋯ & X35 & X36 & X37 & X38 & X39 & X40 & X41 & X42 & X43 & X44\\\\\n",
       "  & <chr> & <chr> & <dbl> & <int> & <chr> & <chr> & <dbl> & <dbl> & <dbl> & <dbl> & ⋯ & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n",
       "\\hline\n",
       "\t1 & 21 & chr21\\_40464521\\_A\\_G\\_b38 & 0 & 40464521 & G & A &  0.00000000 &  0.000000000 & 0.0000000000 & 0.0000000 & ⋯ &  0.51658051 &  0.0000000000 & 0 & 0 & 0 & 0 &  0.00000000 & 0.0000000 &   1.74503598 & -6.310849722\\\\\n",
       "\t2 & 21 & chr21\\_40464616\\_T\\_C\\_b38 & 0 & 40464616 & C & T &  0.00000000 &  0.000000000 & 0.0000000000 & 0.0000000 & ⋯ &  0.00000000 &  0.0000000000 & 0 & 0 & 0 & 0 &  0.00000000 & 0.0000000 &   0.00000000 &  1.065904915\\\\\n",
       "\t3 & 21 & chr21\\_40464797\\_G\\_A\\_b38 & 0 & 40464797 & A & G &  1.21408863 &  0.412646335 & 0.0000000000 & 0.0000000 & ⋯ &  0.00000000 &  0.0000000000 & 0 & 0 & 0 & 0 &  0.00000000 & 0.0000000 &  13.20173550 &  0.435323378\\\\\n",
       "\t4 & 21 & chr21\\_40464826\\_T\\_C\\_b38 & 0 & 40464826 & C & T &  0.05894808 &  0.093197033 & 0.0000000000 & 0.0000000 & ⋯ &  0.61932982 &  0.0000000000 & 0 & 0 & 0 & 0 &  0.00000000 & 0.0000000 &  -0.44801787 &  0.418455795\\\\\n",
       "\t5 & 21 & chr21\\_40464961\\_G\\_A\\_b38 & 0 & 40464961 & A & G &  0.00000000 &  0.000000000 & 0.0000000000 & 0.0000000 & ⋯ &  0.00000000 &  0.0000000000 & 0 & 0 & 0 & 0 &  0.00000000 & 0.0000000 &   2.37398822 &  0.000000000\\\\\n",
       "\t6 & 21 & chr21\\_40465271\\_T\\_C\\_b38 & 0 & 40465271 & T & C & -1.14559691 & -0.621397534 & 0.0000000000 & 0.0000000 & ⋯ &  0.00000000 &  0.0000000000 & 0 & 0 & 0 & 0 &  0.00000000 & 0.0000000 &  -0.95123888 &  0.000000000\\\\\n",
       "\t7 & 21 & chr21\\_40465366\\_C\\_T\\_b38 & 0 & 40465366 & T & C &  0.14040855 &  0.000000000 & 0.0000000000 & 0.0000000 & ⋯ & -0.19192752 &  0.0000000000 & 0 & 0 & 0 & 0 &  0.00000000 & 0.0000000 &   1.29474492 & -1.586051395\\\\\n",
       "\t8 & 21 & chr21\\_40466036\\_C\\_T\\_b38 & 0 & 40466036 & T & C &  0.00000000 &  0.000000000 & 0.0000000000 & 0.0000000 & ⋯ &  0.00000000 &  0.0000000000 & 0 & 0 & 0 & 0 &  0.00000000 & 0.0000000 &   0.00000000 &  0.000000000\\\\\n",
       "\t9 & 21 & chr21\\_40466218\\_C\\_A\\_b38 & 0 & 40466218 & A & C & -1.01716370 &  0.000000000 & 0.0000000000 & 0.1373444 & ⋯ &  0.00000000 &  0.0560647734 & 0 & 0 & 0 & 0 & -0.21285198 & 0.0000000 &  -0.35263763 &  0.000000000\\\\\n",
       "\t10 & 21 & chr21\\_40466226\\_C\\_G\\_b38 & 0 & 40466226 & G & C &  0.08874066 &  0.407203323 & 0.0000000000 & 0.0000000 & ⋯ &  0.00000000 &  0.0000000000 & 0 & 0 & 0 & 0 &  0.00000000 & 0.1133115 &  -0.16511971 &  0.191428076\\\\\n",
       "\t11 & 21 & chr21\\_40466298\\_C\\_T\\_b38 & 0 & 40466298 & T & C &  0.00000000 &  0.000000000 & 0.0000000000 & 0.0000000 & ⋯ & -8.17513243 &  0.0000000000 & 0 & 0 & 0 & 0 &  0.00000000 & 0.0000000 &  -3.35203383 &  0.000000000\\\\\n",
       "\t12 & 21 & chr21\\_40466719\\_A\\_G\\_b38 & 0 & 40466719 & G & A &  0.00000000 &  0.000000000 & 0.0000000000 & 0.0000000 & ⋯ &  0.00000000 &  0.0000000000 & 0 & 0 & 0 & 0 &  0.00000000 & 0.0000000 &   0.00000000 &  0.000000000\\\\\n",
       "\t13 & 21 & chr21\\_40466817\\_T\\_G\\_b38 & 0 & 40466817 & T & G & -0.18212471 & -0.469490894 & 0.0000000000 & 0.0000000 & ⋯ & -0.64980559 &  0.0000000000 & 0 & 0 & 0 & 0 &  0.00000000 & 0.0000000 &  -0.75534044 & -0.310564480\\\\\n",
       "\t14 & 21 & chr21\\_40466922\\_C\\_T\\_b38 & 0 & 40466922 & T & C &  0.00000000 &  0.000000000 & 0.0000000000 & 0.0000000 & ⋯ &  0.00000000 &  0.0000000000 & 0 & 0 & 0 & 0 &  0.00000000 & 0.0000000 &   0.00000000 &  0.000000000\\\\\n",
       "\t15 & 21 & chr21\\_40467145\\_C\\_A\\_b38 & 0 & 40467145 & A & C &  0.00000000 &  0.000000000 & 0.0000000000 & 0.0000000 & ⋯ &  0.00000000 &  0.0000000000 & 0 & 0 & 0 & 0 &  0.00000000 & 0.0000000 &   0.00000000 &  0.000000000\\\\\n",
       "\t16 & 21 & chr21\\_40467151\\_A\\_G\\_b38 & 0 & 40467151 & G & A &  0.00000000 &  0.000000000 & 0.0000000000 & 0.0000000 & ⋯ &  0.00000000 &  0.0000000000 & 0 & 0 & 0 & 0 &  0.00000000 & 0.0000000 &   0.00000000 &  0.000000000\\\\\n",
       "\t17 & 21 & chr21\\_40467257\\_C\\_A\\_b38 & 0 & 40467257 & A & C &  0.01527602 &  0.000000000 & 0.0000000000 & 0.0000000 & ⋯ &  0.00000000 &  0.0000000000 & 0 & 0 & 0 & 0 &  0.00000000 & 0.0000000 &   0.46456820 & -0.224299209\\\\\n",
       "\t18 & 21 & chr21\\_40467282\\_A\\_G\\_b38 & 0 & 40467282 & A & G & -0.02039542 & -0.007303370 & 0.0000000000 & 0.0000000 & ⋯ & -0.07969421 &  0.0000000000 & 0 & 0 & 0 & 0 &  0.00000000 & 0.0000000 &   0.00000000 & -0.009024466\\\\\n",
       "\t19 & 21 & chr21\\_40467492\\_G\\_A\\_b38 & 0 & 40467492 & A & G &  0.00000000 &  0.000000000 & 0.0000000000 & 0.0000000 & ⋯ &  0.00000000 &  0.0000000000 & 0 & 0 & 0 & 0 &  0.00000000 & 0.0000000 &   0.13205108 &  0.000000000\\\\\n",
       "\t20 & 21 & chr21\\_40467555\\_G\\_A\\_b38 & 0 & 40467555 & A & G &  0.00000000 &  0.000000000 & 0.0000000000 & 0.0000000 & ⋯ &  0.00000000 &  0.0000000000 & 0 & 0 & 0 & 0 &  0.00000000 & 0.0000000 &   0.00000000 &  0.000000000\\\\\n",
       "\t21 & 21 & chr21\\_40467631\\_T\\_C\\_b38 & 0 & 40467631 & C & T &  0.00000000 &  0.021268218 & 0.0000000000 & 0.0000000 & ⋯ &  0.34510382 &  0.0000000000 & 0 & 0 & 0 & 0 &  0.00000000 & 0.1929455 &  -0.01154045 &  0.067111371\\\\\n",
       "\t22 & 21 & chr21\\_40468623\\_C\\_T\\_b38 & 0 & 40468623 & T & C &  5.02118844 &  0.000000000 & 0.0000000000 & 0.0000000 & ⋯ & 19.16293034 &  0.0000000000 & 0 & 0 & 0 & 0 &  2.51650014 & 0.0000000 & 109.77058013 &  0.000000000\\\\\n",
       "\t23 & 21 & chr21\\_40468895\\_C\\_T\\_b38 & 0 & 40468895 & T & C &  0.50660974 &  0.000000000 & 0.0000000000 & 0.0000000 & ⋯ &  0.00000000 &  0.0000000000 & 0 & 0 & 0 & 0 &  0.06714634 & 0.0000000 & -14.51747781 &  0.000000000\\\\\n",
       "\t24 & 21 & chr21\\_40469112\\_C\\_T\\_b38 & 0 & 40469112 & T & C &  0.00000000 &  0.000000000 & 0.0000000000 & 0.0000000 & ⋯ &  0.11253863 &  0.0000000000 & 0 & 0 & 0 & 0 &  0.00000000 & 0.0000000 &  -4.55768053 &  0.000000000\\\\\n",
       "\t25 & 21 & chr21\\_40469113\\_G\\_A\\_b38 & 0 & 40469113 & A & G &  0.18169368 &  0.000000000 & 0.0322169691 & 0.0000000 & ⋯ &  0.00000000 &  0.0000000000 & 0 & 0 & 0 & 0 &  0.00000000 & 0.0000000 &   0.18074787 &  0.988612892\\\\\n",
       "\t26 & 21 & chr21\\_40469207\\_T\\_C\\_b38 & 0 & 40469207 & C & T &  0.00000000 &  0.000000000 & 0.0000000000 & 0.0304143 & ⋯ &  0.00000000 &  0.0000000000 & 0 & 0 & 0 & 0 &  0.00000000 & 0.0000000 &   0.26624012 &  0.000000000\\\\\n",
       "\t27 & 21 & chr21\\_40469541\\_T\\_C\\_b38 & 0 & 40469541 & C & T &  0.00000000 &  0.000000000 & 0.0000000000 & 0.0000000 & ⋯ &  0.00000000 &  0.0000000000 & 0 & 0 & 0 & 0 &  0.00000000 & 0.0000000 &   0.25634093 &  0.000000000\\\\\n",
       "\t28 & 21 & chr21\\_40469863\\_T\\_C\\_b38 & 0 & 40469863 & C & T &  0.00000000 &  0.304632504 & 0.0035584119 & 0.0000000 & ⋯ &  0.11373859 & -0.0289917302 & 0 & 0 & 0 & 0 &  0.00000000 & 0.0000000 &   1.52545457 &  0.273573716\\\\\n",
       "\t29 & 21 & chr21\\_40470033\\_T\\_C\\_b38 & 0 & 40470033 & C & T &  0.00000000 &  0.003691257 & 0.0001258493 & 0.0000000 & ⋯ &  0.00000000 & -0.0009422035 & 0 & 0 & 0 & 0 &  0.00000000 & 0.0000000 &   0.00000000 &  0.000000000\\\\\n",
       "\t30 & 21 & chr21\\_40470325\\_A\\_G\\_b38 & 0 & 40470325 & G & A &  0.00000000 &  0.000000000 & 0.0000000000 & 0.0000000 & ⋯ &  0.00000000 &  0.0000000000 & 0 & 0 & 0 & 0 &  0.00000000 & 0.0000000 &   0.00000000 &  0.000000000\\\\\n",
       "\t⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋱ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮ & ⋮\\\\\n",
       "\t71 & 21 & chr21\\_40478709\\_T\\_C\\_b38  & 0 & 40478709 & C & T  &  0.000000000 &  0.01979913 & -0.014249596 & 0.0000000 & ⋯ &  0.00000000 & 0.0000000 & 0 & 0 & 0 &  0.000000000 & 0.00000000 &  0.00000000 &  0.00000000 &  0.00000000\\\\\n",
       "\t72 & 21 & chr21\\_40478829\\_C\\_T\\_b38  & 0 & 40478829 & T & C  &  0.000000000 &  0.00000000 &  0.000000000 & 0.0000000 & ⋯ &  0.00000000 & 0.0000000 & 0 & 0 & 0 &  0.000000000 & 0.00000000 &  0.00000000 &  0.00000000 &  0.00000000\\\\\n",
       "\t73 & 21 & chr21\\_40479135\\_G\\_C\\_b38  & 0 & 40479135 & G & C  &  0.000000000 &  0.00000000 &  0.000000000 & 0.0000000 & ⋯ &  0.00000000 & 0.0000000 & 0 & 0 & 0 &  0.000000000 & 0.00000000 &  0.00000000 & -0.59261757 & -0.52196570\\\\\n",
       "\t74 & 21 & chr21\\_40479139\\_A\\_G\\_b38  & 0 & 40479139 & G & A  &  0.000000000 &  0.00000000 &  0.000000000 & 0.0000000 & ⋯ & -1.07234054 & 0.0000000 & 0 & 0 & 0 &  0.000000000 & 0.00000000 &  0.00000000 &  0.00000000 &  0.00000000\\\\\n",
       "\t75 & 21 & chr21\\_40479151\\_A\\_C\\_b38  & 0 & 40479151 & C & A  & -0.421219115 & -0.73545884 &  0.000000000 & 0.0000000 & ⋯ &  0.00000000 & 0.0000000 & 0 & 0 & 0 &  0.000000000 & 0.00000000 &  0.00000000 &  0.31245314 &  0.00000000\\\\\n",
       "\t76 & 21 & chr21\\_40479453\\_G\\_T\\_b38  & 0 & 40479453 & T & G  &  0.426510786 &  0.53923671 &  0.000000000 & 0.0000000 & ⋯ &  1.81908880 & 0.0000000 & 0 & 0 & 0 &  0.308440557 & 0.05384545 &  0.00000000 &  0.00000000 &  0.00000000\\\\\n",
       "\t77 & 21 & chr21\\_40479524\\_T\\_C\\_b38  & 0 & 40479524 & C & T  & -0.115133343 &  0.00000000 & -0.004932322 & 0.0000000 & ⋯ &  0.07255690 & 0.0000000 & 0 & 0 & 0 &  0.000000000 & 0.00000000 &  0.00000000 & -0.55362686 &  0.00000000\\\\\n",
       "\t78 & 21 & chr21\\_40479580\\_A\\_C\\_b38  & 0 & 40479580 & A & C  &  0.000000000 &  0.00000000 &  0.000000000 & 0.0000000 & ⋯ &  0.00000000 & 0.0000000 & 0 & 0 & 0 &  0.000000000 & 0.00000000 &  0.00000000 &  1.23042511 &  0.00000000\\\\\n",
       "\t79 & 21 & chr21\\_40479820\\_GT\\_G\\_b38 & 0 & 40479820 & G & GT &  0.000000000 &  0.00000000 &  0.000000000 & 0.0000000 & ⋯ &  0.00000000 & 0.0000000 & 0 & 0 & 0 &  0.000000000 & 0.00000000 &  0.00000000 & 20.87866043 &  0.00000000\\\\\n",
       "\t80 & 21 & chr21\\_40479827\\_T\\_C\\_b38  & 0 & 40479827 & C & T  &  0.000000000 &  0.00000000 &  0.000000000 & 0.0000000 & ⋯ &  0.00000000 & 0.0000000 & 0 & 0 & 0 &  0.000000000 & 0.00000000 & -0.05397932 & -0.11315679 &  0.00000000\\\\\n",
       "\t81 & 21 & chr21\\_40479904\\_G\\_A\\_b38  & 0 & 40479904 & A & G  &  0.000000000 &  0.00000000 &  0.000000000 & 0.0000000 & ⋯ &  0.00000000 & 0.0000000 & 0 & 0 & 0 &  0.000000000 & 0.00000000 &  0.00000000 & -0.73695233 &  0.00000000\\\\\n",
       "\t82 & 21 & chr21\\_40479952\\_T\\_C\\_b38  & 0 & 40479952 & C & T  &  0.000000000 &  0.00000000 &  0.000000000 & 0.0000000 & ⋯ &  0.00000000 & 0.0000000 & 0 & 0 & 0 &  0.000000000 & 0.00000000 &  0.00000000 &  0.00000000 &  0.00000000\\\\\n",
       "\t83 & 21 & chr21\\_40480353\\_C\\_T\\_b38  & 0 & 40480353 & T & C  &  0.000000000 &  0.00000000 &  0.000000000 & 0.0000000 & ⋯ &  0.00000000 & 0.0000000 & 0 & 0 & 0 &  0.000000000 & 0.00000000 &  0.00000000 &  0.00000000 & -0.04823205\\\\\n",
       "\t84 & 21 & chr21\\_40480595\\_A\\_G\\_b38  & 0 & 40480595 & A & G  &  0.260493178 &  0.00000000 & -0.004994653 & 0.0000000 & ⋯ &  0.00000000 & 0.0000000 & 0 & 0 & 0 &  0.000000000 & 0.00000000 &  0.00000000 &  0.00000000 &  1.08324279\\\\\n",
       "\t85 & 21 & chr21\\_40480646\\_G\\_A\\_b38  & 0 & 40480646 & A & G  &  0.000000000 &  0.00000000 &  0.000000000 & 0.0000000 & ⋯ & -0.05013099 & 0.0000000 & 0 & 0 & 0 &  0.000000000 & 0.00000000 &  0.55435711 & -0.84886145 &  0.00000000\\\\\n",
       "\t86 & 21 & chr21\\_40481005\\_A\\_C\\_b38  & 0 & 40481005 & C & A  &  0.000000000 &  0.00000000 &  0.000000000 & 0.0000000 & ⋯ &  0.00000000 & 0.0000000 & 0 & 0 & 0 &  0.000000000 & 0.00000000 &  0.00000000 &  0.76126602 &  0.00000000\\\\\n",
       "\t87 & 21 & chr21\\_40481014\\_A\\_C\\_b38  & 0 & 40481014 & C & A  &  0.000000000 &  0.00000000 &  0.000000000 & 0.0000000 & ⋯ &  0.00000000 & 0.0000000 & 0 & 0 & 0 &  0.000000000 & 0.00000000 &  0.00000000 &  0.00000000 &  0.00000000\\\\\n",
       "\t88 & 21 & chr21\\_40481081\\_T\\_C\\_b38  & 0 & 40481081 & C & T  &  0.000000000 &  2.23411921 &  0.000000000 & 0.0000000 & ⋯ &  3.35008832 & 0.0000000 & 0 & 0 & 0 &  0.366440949 & 0.25566563 &  0.00000000 &  0.00000000 &  0.00000000\\\\\n",
       "\t89 & 21 & chr21\\_40481108\\_C\\_T\\_b38  & 0 & 40481108 & T & C  &  0.000000000 &  0.00000000 &  0.000000000 & 0.0000000 & ⋯ &  0.00000000 & 0.0000000 & 0 & 0 & 0 &  0.000000000 & 0.00000000 &  0.00000000 &  0.00000000 &  0.00000000\\\\\n",
       "\t90 & 21 & chr21\\_40481255\\_C\\_T\\_b38  & 0 & 40481255 & T & C  & -0.005341046 &  0.00000000 &  0.000000000 & 0.0000000 & ⋯ &  0.41933173 & 0.0000000 & 0 & 0 & 0 &  0.000000000 & 0.00000000 &  0.00000000 &  1.47070230 &  0.07180171\\\\\n",
       "\t91 & 21 & chr21\\_40481263\\_A\\_G\\_b38  & 0 & 40481263 & A & G  &  0.000000000 &  0.00000000 &  0.000000000 & 0.0000000 & ⋯ &  0.00000000 & 0.0000000 & 0 & 0 & 0 &  0.000000000 & 0.00000000 &  0.00000000 &  0.13765407 & -0.99649439\\\\\n",
       "\t92 & 21 & chr21\\_40481754\\_T\\_C\\_b38  & 0 & 40481754 & C & T  &  0.000000000 &  0.00000000 &  0.000000000 & 0.0000000 & ⋯ &  0.00000000 & 0.0000000 & 0 & 0 & 0 &  0.000000000 & 0.00000000 &  0.00000000 &  0.03493883 & -0.10447977\\\\\n",
       "\t93 & 21 & chr21\\_40481833\\_T\\_A\\_b38  & 0 & 40481833 & A & T  &  0.000000000 &  0.00000000 &  0.000000000 & 0.0000000 & ⋯ &  0.00000000 & 0.0000000 & 0 & 0 & 0 &  0.000000000 & 0.00000000 &  0.00000000 &  0.00000000 & -0.61700718\\\\\n",
       "\t94 & 21 & chr21\\_40481865\\_G\\_A\\_b38  & 0 & 40481865 & A & G  &  0.000000000 &  0.00000000 &  0.000000000 & 0.0000000 & ⋯ &  0.00000000 & 0.0000000 & 0 & 0 & 0 &  0.000000000 & 0.00000000 &  0.00000000 &  0.00000000 &  0.00000000\\\\\n",
       "\t95 & 21 & chr21\\_40482614\\_T\\_C\\_b38  & 0 & 40482614 & C & T  &  0.000000000 &  0.15583846 &  0.000000000 & 0.0000000 & ⋯ &  0.00000000 & 0.0000000 & 0 & 0 & 0 &  0.000000000 & 0.00000000 &  0.00000000 &  1.24963186 &  0.07724665\\\\\n",
       "\t96 & 21 & chr21\\_40482734\\_T\\_C\\_b38  & 0 & 40482734 & C & T  &  0.000000000 &  0.00000000 &  0.000000000 & 0.0000000 & ⋯ &  1.17492048 & 0.0000000 & 0 & 0 & 0 &  0.000000000 & 0.00000000 &  0.00000000 &  0.06903606 & -1.04923034\\\\\n",
       "\t97 & 21 & chr21\\_40482755\\_C\\_T\\_b38  & 0 & 40482755 & T & C  &  0.000000000 &  0.00000000 &  0.000000000 & 0.0000000 & ⋯ &  0.00000000 & 0.0000000 & 0 & 0 & 0 & -0.004079066 & 0.00000000 & -0.01280556 &  0.16703515 &  0.00000000\\\\\n",
       "\t98 & 21 & chr21\\_40482866\\_T\\_A\\_b38  & 0 & 40482866 & T & A  &  0.000000000 &  0.00000000 &  0.000000000 & 0.0000000 & ⋯ &  0.00000000 & 0.0000000 & 0 & 0 & 0 &  0.000000000 & 0.00000000 &  0.00000000 & -0.75027046 & -0.63448982\\\\\n",
       "\t99 & 21 & chr21\\_40483213\\_G\\_C\\_b38  & 0 & 40483213 & G & C  &  0.000000000 &  0.00000000 &  0.000000000 & 0.3064531 & ⋯ & -0.04824842 & 0.4758991 & 0 & 0 & 0 &  0.000000000 & 0.00000000 &  0.00000000 &  0.00000000 & -0.06443647\\\\\n",
       "\t100 & 21 & chr21\\_40483657\\_T\\_C\\_b38  & 0 & 40483657 & C & T  &  0.000000000 &  0.00000000 &  0.000000000 & 0.0000000 & ⋯ & -0.20988650 & 0.0000000 & 0 & 0 & 0 &  0.000000000 & 0.00000000 &  0.00000000 &  0.00000000 &  0.00000000\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 100 × 50\n",
       "\n",
       "| <!--/--> | chr &lt;chr&gt; | id &lt;chr&gt; | posg &lt;dbl&gt; | pos &lt;int&gt; | ref &lt;chr&gt; | alt &lt;chr&gt; | X1 &lt;dbl&gt; | X2 &lt;dbl&gt; | X3 &lt;dbl&gt; | X4 &lt;dbl&gt; | ⋯ ⋯ | X35 &lt;dbl&gt; | X36 &lt;dbl&gt; | X37 &lt;dbl&gt; | X38 &lt;dbl&gt; | X39 &lt;dbl&gt; | X40 &lt;dbl&gt; | X41 &lt;dbl&gt; | X42 &lt;dbl&gt; | X43 &lt;dbl&gt; | X44 &lt;dbl&gt; |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 1 | 21 | chr21_40464521_A_G_b38 | 0 | 40464521 | G | A |  0.00000000 |  0.000000000 | 0.0000000000 | 0.0000000 | ⋯ |  0.51658051 |  0.0000000000 | 0 | 0 | 0 | 0 |  0.00000000 | 0.0000000 |   1.74503598 | -6.310849722 |\n",
       "| 2 | 21 | chr21_40464616_T_C_b38 | 0 | 40464616 | C | T |  0.00000000 |  0.000000000 | 0.0000000000 | 0.0000000 | ⋯ |  0.00000000 |  0.0000000000 | 0 | 0 | 0 | 0 |  0.00000000 | 0.0000000 |   0.00000000 |  1.065904915 |\n",
       "| 3 | 21 | chr21_40464797_G_A_b38 | 0 | 40464797 | A | G |  1.21408863 |  0.412646335 | 0.0000000000 | 0.0000000 | ⋯ |  0.00000000 |  0.0000000000 | 0 | 0 | 0 | 0 |  0.00000000 | 0.0000000 |  13.20173550 |  0.435323378 |\n",
       "| 4 | 21 | chr21_40464826_T_C_b38 | 0 | 40464826 | C | T |  0.05894808 |  0.093197033 | 0.0000000000 | 0.0000000 | ⋯ |  0.61932982 |  0.0000000000 | 0 | 0 | 0 | 0 |  0.00000000 | 0.0000000 |  -0.44801787 |  0.418455795 |\n",
       "| 5 | 21 | chr21_40464961_G_A_b38 | 0 | 40464961 | A | G |  0.00000000 |  0.000000000 | 0.0000000000 | 0.0000000 | ⋯ |  0.00000000 |  0.0000000000 | 0 | 0 | 0 | 0 |  0.00000000 | 0.0000000 |   2.37398822 |  0.000000000 |\n",
       "| 6 | 21 | chr21_40465271_T_C_b38 | 0 | 40465271 | T | C | -1.14559691 | -0.621397534 | 0.0000000000 | 0.0000000 | ⋯ |  0.00000000 |  0.0000000000 | 0 | 0 | 0 | 0 |  0.00000000 | 0.0000000 |  -0.95123888 |  0.000000000 |\n",
       "| 7 | 21 | chr21_40465366_C_T_b38 | 0 | 40465366 | T | C |  0.14040855 |  0.000000000 | 0.0000000000 | 0.0000000 | ⋯ | -0.19192752 |  0.0000000000 | 0 | 0 | 0 | 0 |  0.00000000 | 0.0000000 |   1.29474492 | -1.586051395 |\n",
       "| 8 | 21 | chr21_40466036_C_T_b38 | 0 | 40466036 | T | C |  0.00000000 |  0.000000000 | 0.0000000000 | 0.0000000 | ⋯ |  0.00000000 |  0.0000000000 | 0 | 0 | 0 | 0 |  0.00000000 | 0.0000000 |   0.00000000 |  0.000000000 |\n",
       "| 9 | 21 | chr21_40466218_C_A_b38 | 0 | 40466218 | A | C | -1.01716370 |  0.000000000 | 0.0000000000 | 0.1373444 | ⋯ |  0.00000000 |  0.0560647734 | 0 | 0 | 0 | 0 | -0.21285198 | 0.0000000 |  -0.35263763 |  0.000000000 |\n",
       "| 10 | 21 | chr21_40466226_C_G_b38 | 0 | 40466226 | G | C |  0.08874066 |  0.407203323 | 0.0000000000 | 0.0000000 | ⋯ |  0.00000000 |  0.0000000000 | 0 | 0 | 0 | 0 |  0.00000000 | 0.1133115 |  -0.16511971 |  0.191428076 |\n",
       "| 11 | 21 | chr21_40466298_C_T_b38 | 0 | 40466298 | T | C |  0.00000000 |  0.000000000 | 0.0000000000 | 0.0000000 | ⋯ | -8.17513243 |  0.0000000000 | 0 | 0 | 0 | 0 |  0.00000000 | 0.0000000 |  -3.35203383 |  0.000000000 |\n",
       "| 12 | 21 | chr21_40466719_A_G_b38 | 0 | 40466719 | G | A |  0.00000000 |  0.000000000 | 0.0000000000 | 0.0000000 | ⋯ |  0.00000000 |  0.0000000000 | 0 | 0 | 0 | 0 |  0.00000000 | 0.0000000 |   0.00000000 |  0.000000000 |\n",
       "| 13 | 21 | chr21_40466817_T_G_b38 | 0 | 40466817 | T | G | -0.18212471 | -0.469490894 | 0.0000000000 | 0.0000000 | ⋯ | -0.64980559 |  0.0000000000 | 0 | 0 | 0 | 0 |  0.00000000 | 0.0000000 |  -0.75534044 | -0.310564480 |\n",
       "| 14 | 21 | chr21_40466922_C_T_b38 | 0 | 40466922 | T | C |  0.00000000 |  0.000000000 | 0.0000000000 | 0.0000000 | ⋯ |  0.00000000 |  0.0000000000 | 0 | 0 | 0 | 0 |  0.00000000 | 0.0000000 |   0.00000000 |  0.000000000 |\n",
       "| 15 | 21 | chr21_40467145_C_A_b38 | 0 | 40467145 | A | C |  0.00000000 |  0.000000000 | 0.0000000000 | 0.0000000 | ⋯ |  0.00000000 |  0.0000000000 | 0 | 0 | 0 | 0 |  0.00000000 | 0.0000000 |   0.00000000 |  0.000000000 |\n",
       "| 16 | 21 | chr21_40467151_A_G_b38 | 0 | 40467151 | G | A |  0.00000000 |  0.000000000 | 0.0000000000 | 0.0000000 | ⋯ |  0.00000000 |  0.0000000000 | 0 | 0 | 0 | 0 |  0.00000000 | 0.0000000 |   0.00000000 |  0.000000000 |\n",
       "| 17 | 21 | chr21_40467257_C_A_b38 | 0 | 40467257 | A | C |  0.01527602 |  0.000000000 | 0.0000000000 | 0.0000000 | ⋯ |  0.00000000 |  0.0000000000 | 0 | 0 | 0 | 0 |  0.00000000 | 0.0000000 |   0.46456820 | -0.224299209 |\n",
       "| 18 | 21 | chr21_40467282_A_G_b38 | 0 | 40467282 | A | G | -0.02039542 | -0.007303370 | 0.0000000000 | 0.0000000 | ⋯ | -0.07969421 |  0.0000000000 | 0 | 0 | 0 | 0 |  0.00000000 | 0.0000000 |   0.00000000 | -0.009024466 |\n",
       "| 19 | 21 | chr21_40467492_G_A_b38 | 0 | 40467492 | A | G |  0.00000000 |  0.000000000 | 0.0000000000 | 0.0000000 | ⋯ |  0.00000000 |  0.0000000000 | 0 | 0 | 0 | 0 |  0.00000000 | 0.0000000 |   0.13205108 |  0.000000000 |\n",
       "| 20 | 21 | chr21_40467555_G_A_b38 | 0 | 40467555 | A | G |  0.00000000 |  0.000000000 | 0.0000000000 | 0.0000000 | ⋯ |  0.00000000 |  0.0000000000 | 0 | 0 | 0 | 0 |  0.00000000 | 0.0000000 |   0.00000000 |  0.000000000 |\n",
       "| 21 | 21 | chr21_40467631_T_C_b38 | 0 | 40467631 | C | T |  0.00000000 |  0.021268218 | 0.0000000000 | 0.0000000 | ⋯ |  0.34510382 |  0.0000000000 | 0 | 0 | 0 | 0 |  0.00000000 | 0.1929455 |  -0.01154045 |  0.067111371 |\n",
       "| 22 | 21 | chr21_40468623_C_T_b38 | 0 | 40468623 | T | C |  5.02118844 |  0.000000000 | 0.0000000000 | 0.0000000 | ⋯ | 19.16293034 |  0.0000000000 | 0 | 0 | 0 | 0 |  2.51650014 | 0.0000000 | 109.77058013 |  0.000000000 |\n",
       "| 23 | 21 | chr21_40468895_C_T_b38 | 0 | 40468895 | T | C |  0.50660974 |  0.000000000 | 0.0000000000 | 0.0000000 | ⋯ |  0.00000000 |  0.0000000000 | 0 | 0 | 0 | 0 |  0.06714634 | 0.0000000 | -14.51747781 |  0.000000000 |\n",
       "| 24 | 21 | chr21_40469112_C_T_b38 | 0 | 40469112 | T | C |  0.00000000 |  0.000000000 | 0.0000000000 | 0.0000000 | ⋯ |  0.11253863 |  0.0000000000 | 0 | 0 | 0 | 0 |  0.00000000 | 0.0000000 |  -4.55768053 |  0.000000000 |\n",
       "| 25 | 21 | chr21_40469113_G_A_b38 | 0 | 40469113 | A | G |  0.18169368 |  0.000000000 | 0.0322169691 | 0.0000000 | ⋯ |  0.00000000 |  0.0000000000 | 0 | 0 | 0 | 0 |  0.00000000 | 0.0000000 |   0.18074787 |  0.988612892 |\n",
       "| 26 | 21 | chr21_40469207_T_C_b38 | 0 | 40469207 | C | T |  0.00000000 |  0.000000000 | 0.0000000000 | 0.0304143 | ⋯ |  0.00000000 |  0.0000000000 | 0 | 0 | 0 | 0 |  0.00000000 | 0.0000000 |   0.26624012 |  0.000000000 |\n",
       "| 27 | 21 | chr21_40469541_T_C_b38 | 0 | 40469541 | C | T |  0.00000000 |  0.000000000 | 0.0000000000 | 0.0000000 | ⋯ |  0.00000000 |  0.0000000000 | 0 | 0 | 0 | 0 |  0.00000000 | 0.0000000 |   0.25634093 |  0.000000000 |\n",
       "| 28 | 21 | chr21_40469863_T_C_b38 | 0 | 40469863 | C | T |  0.00000000 |  0.304632504 | 0.0035584119 | 0.0000000 | ⋯ |  0.11373859 | -0.0289917302 | 0 | 0 | 0 | 0 |  0.00000000 | 0.0000000 |   1.52545457 |  0.273573716 |\n",
       "| 29 | 21 | chr21_40470033_T_C_b38 | 0 | 40470033 | C | T |  0.00000000 |  0.003691257 | 0.0001258493 | 0.0000000 | ⋯ |  0.00000000 | -0.0009422035 | 0 | 0 | 0 | 0 |  0.00000000 | 0.0000000 |   0.00000000 |  0.000000000 |\n",
       "| 30 | 21 | chr21_40470325_A_G_b38 | 0 | 40470325 | G | A |  0.00000000 |  0.000000000 | 0.0000000000 | 0.0000000 | ⋯ |  0.00000000 |  0.0000000000 | 0 | 0 | 0 | 0 |  0.00000000 | 0.0000000 |   0.00000000 |  0.000000000 |\n",
       "| ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋱ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ | ⋮ |\n",
       "| 71 | 21 | chr21_40478709_T_C_b38  | 0 | 40478709 | C | T  |  0.000000000 |  0.01979913 | -0.014249596 | 0.0000000 | ⋯ |  0.00000000 | 0.0000000 | 0 | 0 | 0 |  0.000000000 | 0.00000000 |  0.00000000 |  0.00000000 |  0.00000000 |\n",
       "| 72 | 21 | chr21_40478829_C_T_b38  | 0 | 40478829 | T | C  |  0.000000000 |  0.00000000 |  0.000000000 | 0.0000000 | ⋯ |  0.00000000 | 0.0000000 | 0 | 0 | 0 |  0.000000000 | 0.00000000 |  0.00000000 |  0.00000000 |  0.00000000 |\n",
       "| 73 | 21 | chr21_40479135_G_C_b38  | 0 | 40479135 | G | C  |  0.000000000 |  0.00000000 |  0.000000000 | 0.0000000 | ⋯ |  0.00000000 | 0.0000000 | 0 | 0 | 0 |  0.000000000 | 0.00000000 |  0.00000000 | -0.59261757 | -0.52196570 |\n",
       "| 74 | 21 | chr21_40479139_A_G_b38  | 0 | 40479139 | G | A  |  0.000000000 |  0.00000000 |  0.000000000 | 0.0000000 | ⋯ | -1.07234054 | 0.0000000 | 0 | 0 | 0 |  0.000000000 | 0.00000000 |  0.00000000 |  0.00000000 |  0.00000000 |\n",
       "| 75 | 21 | chr21_40479151_A_C_b38  | 0 | 40479151 | C | A  | -0.421219115 | -0.73545884 |  0.000000000 | 0.0000000 | ⋯ |  0.00000000 | 0.0000000 | 0 | 0 | 0 |  0.000000000 | 0.00000000 |  0.00000000 |  0.31245314 |  0.00000000 |\n",
       "| 76 | 21 | chr21_40479453_G_T_b38  | 0 | 40479453 | T | G  |  0.426510786 |  0.53923671 |  0.000000000 | 0.0000000 | ⋯ |  1.81908880 | 0.0000000 | 0 | 0 | 0 |  0.308440557 | 0.05384545 |  0.00000000 |  0.00000000 |  0.00000000 |\n",
       "| 77 | 21 | chr21_40479524_T_C_b38  | 0 | 40479524 | C | T  | -0.115133343 |  0.00000000 | -0.004932322 | 0.0000000 | ⋯ |  0.07255690 | 0.0000000 | 0 | 0 | 0 |  0.000000000 | 0.00000000 |  0.00000000 | -0.55362686 |  0.00000000 |\n",
       "| 78 | 21 | chr21_40479580_A_C_b38  | 0 | 40479580 | A | C  |  0.000000000 |  0.00000000 |  0.000000000 | 0.0000000 | ⋯ |  0.00000000 | 0.0000000 | 0 | 0 | 0 |  0.000000000 | 0.00000000 |  0.00000000 |  1.23042511 |  0.00000000 |\n",
       "| 79 | 21 | chr21_40479820_GT_G_b38 | 0 | 40479820 | G | GT |  0.000000000 |  0.00000000 |  0.000000000 | 0.0000000 | ⋯ |  0.00000000 | 0.0000000 | 0 | 0 | 0 |  0.000000000 | 0.00000000 |  0.00000000 | 20.87866043 |  0.00000000 |\n",
       "| 80 | 21 | chr21_40479827_T_C_b38  | 0 | 40479827 | C | T  |  0.000000000 |  0.00000000 |  0.000000000 | 0.0000000 | ⋯ |  0.00000000 | 0.0000000 | 0 | 0 | 0 |  0.000000000 | 0.00000000 | -0.05397932 | -0.11315679 |  0.00000000 |\n",
       "| 81 | 21 | chr21_40479904_G_A_b38  | 0 | 40479904 | A | G  |  0.000000000 |  0.00000000 |  0.000000000 | 0.0000000 | ⋯ |  0.00000000 | 0.0000000 | 0 | 0 | 0 |  0.000000000 | 0.00000000 |  0.00000000 | -0.73695233 |  0.00000000 |\n",
       "| 82 | 21 | chr21_40479952_T_C_b38  | 0 | 40479952 | C | T  |  0.000000000 |  0.00000000 |  0.000000000 | 0.0000000 | ⋯ |  0.00000000 | 0.0000000 | 0 | 0 | 0 |  0.000000000 | 0.00000000 |  0.00000000 |  0.00000000 |  0.00000000 |\n",
       "| 83 | 21 | chr21_40480353_C_T_b38  | 0 | 40480353 | T | C  |  0.000000000 |  0.00000000 |  0.000000000 | 0.0000000 | ⋯ |  0.00000000 | 0.0000000 | 0 | 0 | 0 |  0.000000000 | 0.00000000 |  0.00000000 |  0.00000000 | -0.04823205 |\n",
       "| 84 | 21 | chr21_40480595_A_G_b38  | 0 | 40480595 | A | G  |  0.260493178 |  0.00000000 | -0.004994653 | 0.0000000 | ⋯ |  0.00000000 | 0.0000000 | 0 | 0 | 0 |  0.000000000 | 0.00000000 |  0.00000000 |  0.00000000 |  1.08324279 |\n",
       "| 85 | 21 | chr21_40480646_G_A_b38  | 0 | 40480646 | A | G  |  0.000000000 |  0.00000000 |  0.000000000 | 0.0000000 | ⋯ | -0.05013099 | 0.0000000 | 0 | 0 | 0 |  0.000000000 | 0.00000000 |  0.55435711 | -0.84886145 |  0.00000000 |\n",
       "| 86 | 21 | chr21_40481005_A_C_b38  | 0 | 40481005 | C | A  |  0.000000000 |  0.00000000 |  0.000000000 | 0.0000000 | ⋯ |  0.00000000 | 0.0000000 | 0 | 0 | 0 |  0.000000000 | 0.00000000 |  0.00000000 |  0.76126602 |  0.00000000 |\n",
       "| 87 | 21 | chr21_40481014_A_C_b38  | 0 | 40481014 | C | A  |  0.000000000 |  0.00000000 |  0.000000000 | 0.0000000 | ⋯ |  0.00000000 | 0.0000000 | 0 | 0 | 0 |  0.000000000 | 0.00000000 |  0.00000000 |  0.00000000 |  0.00000000 |\n",
       "| 88 | 21 | chr21_40481081_T_C_b38  | 0 | 40481081 | C | T  |  0.000000000 |  2.23411921 |  0.000000000 | 0.0000000 | ⋯ |  3.35008832 | 0.0000000 | 0 | 0 | 0 |  0.366440949 | 0.25566563 |  0.00000000 |  0.00000000 |  0.00000000 |\n",
       "| 89 | 21 | chr21_40481108_C_T_b38  | 0 | 40481108 | T | C  |  0.000000000 |  0.00000000 |  0.000000000 | 0.0000000 | ⋯ |  0.00000000 | 0.0000000 | 0 | 0 | 0 |  0.000000000 | 0.00000000 |  0.00000000 |  0.00000000 |  0.00000000 |\n",
       "| 90 | 21 | chr21_40481255_C_T_b38  | 0 | 40481255 | T | C  | -0.005341046 |  0.00000000 |  0.000000000 | 0.0000000 | ⋯ |  0.41933173 | 0.0000000 | 0 | 0 | 0 |  0.000000000 | 0.00000000 |  0.00000000 |  1.47070230 |  0.07180171 |\n",
       "| 91 | 21 | chr21_40481263_A_G_b38  | 0 | 40481263 | A | G  |  0.000000000 |  0.00000000 |  0.000000000 | 0.0000000 | ⋯ |  0.00000000 | 0.0000000 | 0 | 0 | 0 |  0.000000000 | 0.00000000 |  0.00000000 |  0.13765407 | -0.99649439 |\n",
       "| 92 | 21 | chr21_40481754_T_C_b38  | 0 | 40481754 | C | T  |  0.000000000 |  0.00000000 |  0.000000000 | 0.0000000 | ⋯ |  0.00000000 | 0.0000000 | 0 | 0 | 0 |  0.000000000 | 0.00000000 |  0.00000000 |  0.03493883 | -0.10447977 |\n",
       "| 93 | 21 | chr21_40481833_T_A_b38  | 0 | 40481833 | A | T  |  0.000000000 |  0.00000000 |  0.000000000 | 0.0000000 | ⋯ |  0.00000000 | 0.0000000 | 0 | 0 | 0 |  0.000000000 | 0.00000000 |  0.00000000 |  0.00000000 | -0.61700718 |\n",
       "| 94 | 21 | chr21_40481865_G_A_b38  | 0 | 40481865 | A | G  |  0.000000000 |  0.00000000 |  0.000000000 | 0.0000000 | ⋯ |  0.00000000 | 0.0000000 | 0 | 0 | 0 |  0.000000000 | 0.00000000 |  0.00000000 |  0.00000000 |  0.00000000 |\n",
       "| 95 | 21 | chr21_40482614_T_C_b38  | 0 | 40482614 | C | T  |  0.000000000 |  0.15583846 |  0.000000000 | 0.0000000 | ⋯ |  0.00000000 | 0.0000000 | 0 | 0 | 0 |  0.000000000 | 0.00000000 |  0.00000000 |  1.24963186 |  0.07724665 |\n",
       "| 96 | 21 | chr21_40482734_T_C_b38  | 0 | 40482734 | C | T  |  0.000000000 |  0.00000000 |  0.000000000 | 0.0000000 | ⋯ |  1.17492048 | 0.0000000 | 0 | 0 | 0 |  0.000000000 | 0.00000000 |  0.00000000 |  0.06903606 | -1.04923034 |\n",
       "| 97 | 21 | chr21_40482755_C_T_b38  | 0 | 40482755 | T | C  |  0.000000000 |  0.00000000 |  0.000000000 | 0.0000000 | ⋯ |  0.00000000 | 0.0000000 | 0 | 0 | 0 | -0.004079066 | 0.00000000 | -0.01280556 |  0.16703515 |  0.00000000 |\n",
       "| 98 | 21 | chr21_40482866_T_A_b38  | 0 | 40482866 | T | A  |  0.000000000 |  0.00000000 |  0.000000000 | 0.0000000 | ⋯ |  0.00000000 | 0.0000000 | 0 | 0 | 0 |  0.000000000 | 0.00000000 |  0.00000000 | -0.75027046 | -0.63448982 |\n",
       "| 99 | 21 | chr21_40483213_G_C_b38  | 0 | 40483213 | G | C  |  0.000000000 |  0.00000000 |  0.000000000 | 0.3064531 | ⋯ | -0.04824842 | 0.4758991 | 0 | 0 | 0 |  0.000000000 | 0.00000000 |  0.00000000 |  0.00000000 | -0.06443647 |\n",
       "| 100 | 21 | chr21_40483657_T_C_b38  | 0 | 40483657 | C | T  |  0.000000000 |  0.00000000 |  0.000000000 | 0.0000000 | ⋯ | -0.20988650 | 0.0000000 | 0 | 0 | 0 |  0.000000000 | 0.00000000 |  0.00000000 |  0.00000000 |  0.00000000 |\n",
       "\n"
      ],
      "text/plain": [
       "    chr id                      posg pos      ref alt X1           X2          \n",
       "1   21  chr21_40464521_A_G_b38  0    40464521 G   A    0.00000000   0.000000000\n",
       "2   21  chr21_40464616_T_C_b38  0    40464616 C   T    0.00000000   0.000000000\n",
       "3   21  chr21_40464797_G_A_b38  0    40464797 A   G    1.21408863   0.412646335\n",
       "4   21  chr21_40464826_T_C_b38  0    40464826 C   T    0.05894808   0.093197033\n",
       "5   21  chr21_40464961_G_A_b38  0    40464961 A   G    0.00000000   0.000000000\n",
       "6   21  chr21_40465271_T_C_b38  0    40465271 T   C   -1.14559691  -0.621397534\n",
       "7   21  chr21_40465366_C_T_b38  0    40465366 T   C    0.14040855   0.000000000\n",
       "8   21  chr21_40466036_C_T_b38  0    40466036 T   C    0.00000000   0.000000000\n",
       "9   21  chr21_40466218_C_A_b38  0    40466218 A   C   -1.01716370   0.000000000\n",
       "10  21  chr21_40466226_C_G_b38  0    40466226 G   C    0.08874066   0.407203323\n",
       "11  21  chr21_40466298_C_T_b38  0    40466298 T   C    0.00000000   0.000000000\n",
       "12  21  chr21_40466719_A_G_b38  0    40466719 G   A    0.00000000   0.000000000\n",
       "13  21  chr21_40466817_T_G_b38  0    40466817 T   G   -0.18212471  -0.469490894\n",
       "14  21  chr21_40466922_C_T_b38  0    40466922 T   C    0.00000000   0.000000000\n",
       "15  21  chr21_40467145_C_A_b38  0    40467145 A   C    0.00000000   0.000000000\n",
       "16  21  chr21_40467151_A_G_b38  0    40467151 G   A    0.00000000   0.000000000\n",
       "17  21  chr21_40467257_C_A_b38  0    40467257 A   C    0.01527602   0.000000000\n",
       "18  21  chr21_40467282_A_G_b38  0    40467282 A   G   -0.02039542  -0.007303370\n",
       "19  21  chr21_40467492_G_A_b38  0    40467492 A   G    0.00000000   0.000000000\n",
       "20  21  chr21_40467555_G_A_b38  0    40467555 A   G    0.00000000   0.000000000\n",
       "21  21  chr21_40467631_T_C_b38  0    40467631 C   T    0.00000000   0.021268218\n",
       "22  21  chr21_40468623_C_T_b38  0    40468623 T   C    5.02118844   0.000000000\n",
       "23  21  chr21_40468895_C_T_b38  0    40468895 T   C    0.50660974   0.000000000\n",
       "24  21  chr21_40469112_C_T_b38  0    40469112 T   C    0.00000000   0.000000000\n",
       "25  21  chr21_40469113_G_A_b38  0    40469113 A   G    0.18169368   0.000000000\n",
       "26  21  chr21_40469207_T_C_b38  0    40469207 C   T    0.00000000   0.000000000\n",
       "27  21  chr21_40469541_T_C_b38  0    40469541 C   T    0.00000000   0.000000000\n",
       "28  21  chr21_40469863_T_C_b38  0    40469863 C   T    0.00000000   0.304632504\n",
       "29  21  chr21_40470033_T_C_b38  0    40470033 C   T    0.00000000   0.003691257\n",
       "30  21  chr21_40470325_A_G_b38  0    40470325 G   A    0.00000000   0.000000000\n",
       "⋮   ⋮   ⋮                       ⋮    ⋮        ⋮   ⋮   ⋮            ⋮           \n",
       "71  21  chr21_40478709_T_C_b38  0    40478709 C   T    0.000000000  0.01979913 \n",
       "72  21  chr21_40478829_C_T_b38  0    40478829 T   C    0.000000000  0.00000000 \n",
       "73  21  chr21_40479135_G_C_b38  0    40479135 G   C    0.000000000  0.00000000 \n",
       "74  21  chr21_40479139_A_G_b38  0    40479139 G   A    0.000000000  0.00000000 \n",
       "75  21  chr21_40479151_A_C_b38  0    40479151 C   A   -0.421219115 -0.73545884 \n",
       "76  21  chr21_40479453_G_T_b38  0    40479453 T   G    0.426510786  0.53923671 \n",
       "77  21  chr21_40479524_T_C_b38  0    40479524 C   T   -0.115133343  0.00000000 \n",
       "78  21  chr21_40479580_A_C_b38  0    40479580 A   C    0.000000000  0.00000000 \n",
       "79  21  chr21_40479820_GT_G_b38 0    40479820 G   GT   0.000000000  0.00000000 \n",
       "80  21  chr21_40479827_T_C_b38  0    40479827 C   T    0.000000000  0.00000000 \n",
       "81  21  chr21_40479904_G_A_b38  0    40479904 A   G    0.000000000  0.00000000 \n",
       "82  21  chr21_40479952_T_C_b38  0    40479952 C   T    0.000000000  0.00000000 \n",
       "83  21  chr21_40480353_C_T_b38  0    40480353 T   C    0.000000000  0.00000000 \n",
       "84  21  chr21_40480595_A_G_b38  0    40480595 A   G    0.260493178  0.00000000 \n",
       "85  21  chr21_40480646_G_A_b38  0    40480646 A   G    0.000000000  0.00000000 \n",
       "86  21  chr21_40481005_A_C_b38  0    40481005 C   A    0.000000000  0.00000000 \n",
       "87  21  chr21_40481014_A_C_b38  0    40481014 C   A    0.000000000  0.00000000 \n",
       "88  21  chr21_40481081_T_C_b38  0    40481081 C   T    0.000000000  2.23411921 \n",
       "89  21  chr21_40481108_C_T_b38  0    40481108 T   C    0.000000000  0.00000000 \n",
       "90  21  chr21_40481255_C_T_b38  0    40481255 T   C   -0.005341046  0.00000000 \n",
       "91  21  chr21_40481263_A_G_b38  0    40481263 A   G    0.000000000  0.00000000 \n",
       "92  21  chr21_40481754_T_C_b38  0    40481754 C   T    0.000000000  0.00000000 \n",
       "93  21  chr21_40481833_T_A_b38  0    40481833 A   T    0.000000000  0.00000000 \n",
       "94  21  chr21_40481865_G_A_b38  0    40481865 A   G    0.000000000  0.00000000 \n",
       "95  21  chr21_40482614_T_C_b38  0    40482614 C   T    0.000000000  0.15583846 \n",
       "96  21  chr21_40482734_T_C_b38  0    40482734 C   T    0.000000000  0.00000000 \n",
       "97  21  chr21_40482755_C_T_b38  0    40482755 T   C    0.000000000  0.00000000 \n",
       "98  21  chr21_40482866_T_A_b38  0    40482866 T   A    0.000000000  0.00000000 \n",
       "99  21  chr21_40483213_G_C_b38  0    40483213 G   C    0.000000000  0.00000000 \n",
       "100 21  chr21_40483657_T_C_b38  0    40483657 C   T    0.000000000  0.00000000 \n",
       "    X3           X4        ⋯ X35         X36           X37 X38 X39 X40         \n",
       "1   0.0000000000 0.0000000 ⋯  0.51658051  0.0000000000 0   0   0   0           \n",
       "2   0.0000000000 0.0000000 ⋯  0.00000000  0.0000000000 0   0   0   0           \n",
       "3   0.0000000000 0.0000000 ⋯  0.00000000  0.0000000000 0   0   0   0           \n",
       "4   0.0000000000 0.0000000 ⋯  0.61932982  0.0000000000 0   0   0   0           \n",
       "5   0.0000000000 0.0000000 ⋯  0.00000000  0.0000000000 0   0   0   0           \n",
       "6   0.0000000000 0.0000000 ⋯  0.00000000  0.0000000000 0   0   0   0           \n",
       "7   0.0000000000 0.0000000 ⋯ -0.19192752  0.0000000000 0   0   0   0           \n",
       "8   0.0000000000 0.0000000 ⋯  0.00000000  0.0000000000 0   0   0   0           \n",
       "9   0.0000000000 0.1373444 ⋯  0.00000000  0.0560647734 0   0   0   0           \n",
       "10  0.0000000000 0.0000000 ⋯  0.00000000  0.0000000000 0   0   0   0           \n",
       "11  0.0000000000 0.0000000 ⋯ -8.17513243  0.0000000000 0   0   0   0           \n",
       "12  0.0000000000 0.0000000 ⋯  0.00000000  0.0000000000 0   0   0   0           \n",
       "13  0.0000000000 0.0000000 ⋯ -0.64980559  0.0000000000 0   0   0   0           \n",
       "14  0.0000000000 0.0000000 ⋯  0.00000000  0.0000000000 0   0   0   0           \n",
       "15  0.0000000000 0.0000000 ⋯  0.00000000  0.0000000000 0   0   0   0           \n",
       "16  0.0000000000 0.0000000 ⋯  0.00000000  0.0000000000 0   0   0   0           \n",
       "17  0.0000000000 0.0000000 ⋯  0.00000000  0.0000000000 0   0   0   0           \n",
       "18  0.0000000000 0.0000000 ⋯ -0.07969421  0.0000000000 0   0   0   0           \n",
       "19  0.0000000000 0.0000000 ⋯  0.00000000  0.0000000000 0   0   0   0           \n",
       "20  0.0000000000 0.0000000 ⋯  0.00000000  0.0000000000 0   0   0   0           \n",
       "21  0.0000000000 0.0000000 ⋯  0.34510382  0.0000000000 0   0   0   0           \n",
       "22  0.0000000000 0.0000000 ⋯ 19.16293034  0.0000000000 0   0   0   0           \n",
       "23  0.0000000000 0.0000000 ⋯  0.00000000  0.0000000000 0   0   0   0           \n",
       "24  0.0000000000 0.0000000 ⋯  0.11253863  0.0000000000 0   0   0   0           \n",
       "25  0.0322169691 0.0000000 ⋯  0.00000000  0.0000000000 0   0   0   0           \n",
       "26  0.0000000000 0.0304143 ⋯  0.00000000  0.0000000000 0   0   0   0           \n",
       "27  0.0000000000 0.0000000 ⋯  0.00000000  0.0000000000 0   0   0   0           \n",
       "28  0.0035584119 0.0000000 ⋯  0.11373859 -0.0289917302 0   0   0   0           \n",
       "29  0.0001258493 0.0000000 ⋯  0.00000000 -0.0009422035 0   0   0   0           \n",
       "30  0.0000000000 0.0000000 ⋯  0.00000000  0.0000000000 0   0   0   0           \n",
       "⋮   ⋮            ⋮         ⋱ ⋮           ⋮             ⋮   ⋮   ⋮   ⋮           \n",
       "71  -0.014249596 0.0000000 ⋯  0.00000000 0.0000000     0   0   0    0.000000000\n",
       "72   0.000000000 0.0000000 ⋯  0.00000000 0.0000000     0   0   0    0.000000000\n",
       "73   0.000000000 0.0000000 ⋯  0.00000000 0.0000000     0   0   0    0.000000000\n",
       "74   0.000000000 0.0000000 ⋯ -1.07234054 0.0000000     0   0   0    0.000000000\n",
       "75   0.000000000 0.0000000 ⋯  0.00000000 0.0000000     0   0   0    0.000000000\n",
       "76   0.000000000 0.0000000 ⋯  1.81908880 0.0000000     0   0   0    0.308440557\n",
       "77  -0.004932322 0.0000000 ⋯  0.07255690 0.0000000     0   0   0    0.000000000\n",
       "78   0.000000000 0.0000000 ⋯  0.00000000 0.0000000     0   0   0    0.000000000\n",
       "79   0.000000000 0.0000000 ⋯  0.00000000 0.0000000     0   0   0    0.000000000\n",
       "80   0.000000000 0.0000000 ⋯  0.00000000 0.0000000     0   0   0    0.000000000\n",
       "81   0.000000000 0.0000000 ⋯  0.00000000 0.0000000     0   0   0    0.000000000\n",
       "82   0.000000000 0.0000000 ⋯  0.00000000 0.0000000     0   0   0    0.000000000\n",
       "83   0.000000000 0.0000000 ⋯  0.00000000 0.0000000     0   0   0    0.000000000\n",
       "84  -0.004994653 0.0000000 ⋯  0.00000000 0.0000000     0   0   0    0.000000000\n",
       "85   0.000000000 0.0000000 ⋯ -0.05013099 0.0000000     0   0   0    0.000000000\n",
       "86   0.000000000 0.0000000 ⋯  0.00000000 0.0000000     0   0   0    0.000000000\n",
       "87   0.000000000 0.0000000 ⋯  0.00000000 0.0000000     0   0   0    0.000000000\n",
       "88   0.000000000 0.0000000 ⋯  3.35008832 0.0000000     0   0   0    0.366440949\n",
       "89   0.000000000 0.0000000 ⋯  0.00000000 0.0000000     0   0   0    0.000000000\n",
       "90   0.000000000 0.0000000 ⋯  0.41933173 0.0000000     0   0   0    0.000000000\n",
       "91   0.000000000 0.0000000 ⋯  0.00000000 0.0000000     0   0   0    0.000000000\n",
       "92   0.000000000 0.0000000 ⋯  0.00000000 0.0000000     0   0   0    0.000000000\n",
       "93   0.000000000 0.0000000 ⋯  0.00000000 0.0000000     0   0   0    0.000000000\n",
       "94   0.000000000 0.0000000 ⋯  0.00000000 0.0000000     0   0   0    0.000000000\n",
       "95   0.000000000 0.0000000 ⋯  0.00000000 0.0000000     0   0   0    0.000000000\n",
       "96   0.000000000 0.0000000 ⋯  1.17492048 0.0000000     0   0   0    0.000000000\n",
       "97   0.000000000 0.0000000 ⋯  0.00000000 0.0000000     0   0   0   -0.004079066\n",
       "98   0.000000000 0.0000000 ⋯  0.00000000 0.0000000     0   0   0    0.000000000\n",
       "99   0.000000000 0.3064531 ⋯ -0.04824842 0.4758991     0   0   0    0.000000000\n",
       "100  0.000000000 0.0000000 ⋯ -0.20988650 0.0000000     0   0   0    0.000000000\n",
       "    X41         X42         X43          X44         \n",
       "1    0.00000000 0.0000000     1.74503598 -6.310849722\n",
       "2    0.00000000 0.0000000     0.00000000  1.065904915\n",
       "3    0.00000000 0.0000000    13.20173550  0.435323378\n",
       "4    0.00000000 0.0000000    -0.44801787  0.418455795\n",
       "5    0.00000000 0.0000000     2.37398822  0.000000000\n",
       "6    0.00000000 0.0000000    -0.95123888  0.000000000\n",
       "7    0.00000000 0.0000000     1.29474492 -1.586051395\n",
       "8    0.00000000 0.0000000     0.00000000  0.000000000\n",
       "9   -0.21285198 0.0000000    -0.35263763  0.000000000\n",
       "10   0.00000000 0.1133115    -0.16511971  0.191428076\n",
       "11   0.00000000 0.0000000    -3.35203383  0.000000000\n",
       "12   0.00000000 0.0000000     0.00000000  0.000000000\n",
       "13   0.00000000 0.0000000    -0.75534044 -0.310564480\n",
       "14   0.00000000 0.0000000     0.00000000  0.000000000\n",
       "15   0.00000000 0.0000000     0.00000000  0.000000000\n",
       "16   0.00000000 0.0000000     0.00000000  0.000000000\n",
       "17   0.00000000 0.0000000     0.46456820 -0.224299209\n",
       "18   0.00000000 0.0000000     0.00000000 -0.009024466\n",
       "19   0.00000000 0.0000000     0.13205108  0.000000000\n",
       "20   0.00000000 0.0000000     0.00000000  0.000000000\n",
       "21   0.00000000 0.1929455    -0.01154045  0.067111371\n",
       "22   2.51650014 0.0000000   109.77058013  0.000000000\n",
       "23   0.06714634 0.0000000   -14.51747781  0.000000000\n",
       "24   0.00000000 0.0000000    -4.55768053  0.000000000\n",
       "25   0.00000000 0.0000000     0.18074787  0.988612892\n",
       "26   0.00000000 0.0000000     0.26624012  0.000000000\n",
       "27   0.00000000 0.0000000     0.25634093  0.000000000\n",
       "28   0.00000000 0.0000000     1.52545457  0.273573716\n",
       "29   0.00000000 0.0000000     0.00000000  0.000000000\n",
       "30   0.00000000 0.0000000     0.00000000  0.000000000\n",
       "⋮   ⋮           ⋮           ⋮            ⋮           \n",
       "71  0.00000000   0.00000000  0.00000000   0.00000000 \n",
       "72  0.00000000   0.00000000  0.00000000   0.00000000 \n",
       "73  0.00000000   0.00000000 -0.59261757  -0.52196570 \n",
       "74  0.00000000   0.00000000  0.00000000   0.00000000 \n",
       "75  0.00000000   0.00000000  0.31245314   0.00000000 \n",
       "76  0.05384545   0.00000000  0.00000000   0.00000000 \n",
       "77  0.00000000   0.00000000 -0.55362686   0.00000000 \n",
       "78  0.00000000   0.00000000  1.23042511   0.00000000 \n",
       "79  0.00000000   0.00000000 20.87866043   0.00000000 \n",
       "80  0.00000000  -0.05397932 -0.11315679   0.00000000 \n",
       "81  0.00000000   0.00000000 -0.73695233   0.00000000 \n",
       "82  0.00000000   0.00000000  0.00000000   0.00000000 \n",
       "83  0.00000000   0.00000000  0.00000000  -0.04823205 \n",
       "84  0.00000000   0.00000000  0.00000000   1.08324279 \n",
       "85  0.00000000   0.55435711 -0.84886145   0.00000000 \n",
       "86  0.00000000   0.00000000  0.76126602   0.00000000 \n",
       "87  0.00000000   0.00000000  0.00000000   0.00000000 \n",
       "88  0.25566563   0.00000000  0.00000000   0.00000000 \n",
       "89  0.00000000   0.00000000  0.00000000   0.00000000 \n",
       "90  0.00000000   0.00000000  1.47070230   0.07180171 \n",
       "91  0.00000000   0.00000000  0.13765407  -0.99649439 \n",
       "92  0.00000000   0.00000000  0.03493883  -0.10447977 \n",
       "93  0.00000000   0.00000000  0.00000000  -0.61700718 \n",
       "94  0.00000000   0.00000000  0.00000000   0.00000000 \n",
       "95  0.00000000   0.00000000  1.24963186   0.07724665 \n",
       "96  0.00000000   0.00000000  0.06903606  -1.04923034 \n",
       "97  0.00000000  -0.01280556  0.16703515   0.00000000 \n",
       "98  0.00000000   0.00000000 -0.75027046  -0.63448982 \n",
       "99  0.00000000   0.00000000  0.00000000  -0.06443647 \n",
       "100 0.00000000   0.00000000  0.00000000   0.00000000 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "downstream_est[1:100, 1:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error: 2306.63991985794\n",
      "Training error: 2370.47650114781\n",
      "Training error: 2306.63991985794\n",
      "Training error: 2370.47650114781\n",
      "Training error: 2306.63991985794\n",
      "Training error: 2370.47650114781\n",
      "Training error: 2306.63991985794\n",
      "Training error: 2370.47650114781\n",
      "Training error: 2306.63991985794\n",
      "Training error: 2370.47650114781\n",
      "Training error: 2306.63991985794\n",
      "Training error: 2370.47650114781\n"
     ]
    }
   ],
   "source": [
    "save(single_res_test, single_lam, single_theta_est, multi_res_test, multi_lam, multi_theta_est, file = paste0('chr', chr, '.', k, '.', gene_id, \".RData\"))\n",
    "\t#res_single = avg_perm(single_res_test)\n",
    "\t#res_multi = avg_perm(multi_res_test)\n",
    "\t#cat(\"Elastic net average testing error (all): \", apply(res_single, 2, mean), '\\n')\n",
    "\t#cat(\"glasso averge testing error (all): \", apply(res_multi, 2, mean), '\\n')\n",
    "\t#cat(\"Number of all zero tissues in elastic net is \", sum(is.na(res_single[,1])), '\\n')\n",
    "\t#cat(\"Number of all zero tissues in glasso is \", sum(is.na(res_multi[,1])), '\\n')\n",
    "\t#cat(\"Elastic net average testing error (non-zero): \", apply(res_single[!is.na(res_multi[,1]),], 2, mean), '\\n')\n",
    "\t#cat(\"glasso averge testing error (non-zero): \", apply(res_multi[!is.na(res_multi[,1]),], 2, mean), '\\n')\n",
    "\n",
    "\t## generate an estimate with whole data ##\n",
    "\t\n",
    "\t#write.table(downstream_est, paste0(gene_id, \".est\"), quote=F, row.names=F, col.names=c(\"SNP\", \"REF.0.\", \"ALT.1.\", Yt))\n",
    "    write.table(downstream_est, paste0(gene_id, \".est\"), quote=F, row.names=F, col.names=c(colnames(genotype_info$bim), Yt))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "523663"
      ],
      "text/latex": [
       "523663"
      ],
      "text/markdown": [
       "523663"
      ],
      "text/plain": [
       "[1] 523663"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "length(ans$est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "523429"
      ],
      "text/latex": [
       "523429"
      ],
      "text/markdown": [
       "523429"
      ],
      "text/plain": [
       "[1] 523429"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sum(initial_numeric == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "523663"
      ],
      "text/latex": [
       "523663"
      ],
      "text/markdown": [
       "523663"
      ],
      "text/plain": [
       "[1] 523663"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "length(initial_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (obj in ls()) { \n",
    "    message(obj)\n",
    "    print(object.size(get(obj)), units='auto') \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "49"
      ],
      "text/latex": [
       "49"
      ],
      "text/markdown": [
       "49"
      ],
      "text/plain": [
       "[1] 49"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "length(XX_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>10687</li><li>10687</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 10687\n",
       "\\item 10687\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 10687\n",
       "2. 10687\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 10687 10687"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dim(XX_train[[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mtx <- as.big.matrix(XX_train[[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "871.4 Mb\n"
     ]
    }
   ],
   "source": [
    "print(object.size(XX_train[[1]]), units='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mtx <- matrix(1.57, 10000, 10000 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "762.9 Mb\n"
     ]
    }
   ],
   "source": [
    "print(object.size(test_mtx), units = \"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim(test_mtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A matrix: 6 × 6 of type dbl</caption>\n",
       "<tbody>\n",
       "\t<tr><td>1.57</td><td>1.57</td><td>1.57</td><td>1.57</td><td>1.57</td><td>1.57</td></tr>\n",
       "\t<tr><td>1.57</td><td>1.57</td><td>1.57</td><td>1.57</td><td>1.57</td><td>1.57</td></tr>\n",
       "\t<tr><td>1.57</td><td>1.57</td><td>1.57</td><td>1.57</td><td>1.57</td><td>1.57</td></tr>\n",
       "\t<tr><td>1.57</td><td>1.57</td><td>1.57</td><td>1.57</td><td>1.57</td><td>1.57</td></tr>\n",
       "\t<tr><td>1.57</td><td>1.57</td><td>1.57</td><td>1.57</td><td>1.57</td><td>1.57</td></tr>\n",
       "\t<tr><td>1.57</td><td>1.57</td><td>1.57</td><td>1.57</td><td>1.57</td><td>1.57</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A matrix: 6 × 6 of type dbl\n",
       "\\begin{tabular}{llllll}\n",
       "\t 1.57 & 1.57 & 1.57 & 1.57 & 1.57 & 1.57\\\\\n",
       "\t 1.57 & 1.57 & 1.57 & 1.57 & 1.57 & 1.57\\\\\n",
       "\t 1.57 & 1.57 & 1.57 & 1.57 & 1.57 & 1.57\\\\\n",
       "\t 1.57 & 1.57 & 1.57 & 1.57 & 1.57 & 1.57\\\\\n",
       "\t 1.57 & 1.57 & 1.57 & 1.57 & 1.57 & 1.57\\\\\n",
       "\t 1.57 & 1.57 & 1.57 & 1.57 & 1.57 & 1.57\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A matrix: 6 × 6 of type dbl\n",
       "\n",
       "| 1.57 | 1.57 | 1.57 | 1.57 | 1.57 | 1.57 |\n",
       "| 1.57 | 1.57 | 1.57 | 1.57 | 1.57 | 1.57 |\n",
       "| 1.57 | 1.57 | 1.57 | 1.57 | 1.57 | 1.57 |\n",
       "| 1.57 | 1.57 | 1.57 | 1.57 | 1.57 | 1.57 |\n",
       "| 1.57 | 1.57 | 1.57 | 1.57 | 1.57 | 1.57 |\n",
       "| 1.57 | 1.57 | 1.57 | 1.57 | 1.57 | 1.57 |\n",
       "\n"
      ],
      "text/plain": [
       "     [,1] [,2] [,3] [,4] [,5] [,6]\n",
       "[1,] 1.57 1.57 1.57 1.57 1.57 1.57\n",
       "[2,] 1.57 1.57 1.57 1.57 1.57 1.57\n",
       "[3,] 1.57 1.57 1.57 1.57 1.57 1.57\n",
       "[4,] 1.57 1.57 1.57 1.57 1.57 1.57\n",
       "[5,] 1.57 1.57 1.57 1.57 1.57 1.57\n",
       "[6,] 1.57 1.57 1.57 1.57 1.57 1.57"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_mtx[1:6, 1:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<caption>A matrix: 6 × 6 of type dbl</caption>\n",
       "<tbody>\n",
       "\t<tr><td> 0.11700682</td><td> 0.11700682</td><td>-0.01664066</td><td> 0.09849660</td><td> 0.10877685</td><td>-0.04667148</td></tr>\n",
       "\t<tr><td> 0.11700682</td><td> 0.11700682</td><td>-0.01664066</td><td> 0.09849660</td><td> 0.10877685</td><td>-0.04667148</td></tr>\n",
       "\t<tr><td>-0.01664066</td><td>-0.01664066</td><td> 0.21854696</td><td> 0.19724280</td><td>-0.01430092</td><td>-0.09354227</td></tr>\n",
       "\t<tr><td> 0.09849660</td><td> 0.09849660</td><td> 0.19724280</td><td> 0.30699923</td><td> 0.09279519</td><td>-0.15061144</td></tr>\n",
       "\t<tr><td> 0.10877685</td><td> 0.10877685</td><td>-0.01430092</td><td> 0.09279519</td><td> 0.10954468</td><td>-0.04445851</td></tr>\n",
       "\t<tr><td>-0.04667148</td><td>-0.04667148</td><td>-0.09354227</td><td>-0.15061144</td><td>-0.04445851</td><td> 0.48489715</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A matrix: 6 × 6 of type dbl\n",
       "\\begin{tabular}{llllll}\n",
       "\t  0.11700682 &  0.11700682 & -0.01664066 &  0.09849660 &  0.10877685 & -0.04667148\\\\\n",
       "\t  0.11700682 &  0.11700682 & -0.01664066 &  0.09849660 &  0.10877685 & -0.04667148\\\\\n",
       "\t -0.01664066 & -0.01664066 &  0.21854696 &  0.19724280 & -0.01430092 & -0.09354227\\\\\n",
       "\t  0.09849660 &  0.09849660 &  0.19724280 &  0.30699923 &  0.09279519 & -0.15061144\\\\\n",
       "\t  0.10877685 &  0.10877685 & -0.01430092 &  0.09279519 &  0.10954468 & -0.04445851\\\\\n",
       "\t -0.04667148 & -0.04667148 & -0.09354227 & -0.15061144 & -0.04445851 &  0.48489715\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A matrix: 6 × 6 of type dbl\n",
       "\n",
       "|  0.11700682 |  0.11700682 | -0.01664066 |  0.09849660 |  0.10877685 | -0.04667148 |\n",
       "|  0.11700682 |  0.11700682 | -0.01664066 |  0.09849660 |  0.10877685 | -0.04667148 |\n",
       "| -0.01664066 | -0.01664066 |  0.21854696 |  0.19724280 | -0.01430092 | -0.09354227 |\n",
       "|  0.09849660 |  0.09849660 |  0.19724280 |  0.30699923 |  0.09279519 | -0.15061144 |\n",
       "|  0.10877685 |  0.10877685 | -0.01430092 |  0.09279519 |  0.10954468 | -0.04445851 |\n",
       "| -0.04667148 | -0.04667148 | -0.09354227 | -0.15061144 | -0.04445851 |  0.48489715 |\n",
       "\n"
      ],
      "text/plain": [
       "     [,1]        [,2]        [,3]        [,4]        [,5]        [,6]       \n",
       "[1,]  0.11700682  0.11700682 -0.01664066  0.09849660  0.10877685 -0.04667148\n",
       "[2,]  0.11700682  0.11700682 -0.01664066  0.09849660  0.10877685 -0.04667148\n",
       "[3,] -0.01664066 -0.01664066  0.21854696  0.19724280 -0.01430092 -0.09354227\n",
       "[4,]  0.09849660  0.09849660  0.19724280  0.30699923  0.09279519 -0.15061144\n",
       "[5,]  0.10877685  0.10877685 -0.01430092  0.09279519  0.10954468 -0.04445851\n",
       "[6,] -0.04667148 -0.04667148 -0.09354227 -0.15061144 -0.04445851  0.48489715"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "XX_train[[1]][1:6, 1:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n"
     ]
    }
   ],
   "source": [
    "res_single = avg_perm(single_res_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n",
      "Warning message in cor(mse_lst[[f]][[t]]):\n",
      "“the standard deviation is zero”\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elastic net average testing error (all):  NA 99.70318 1.093741 \n",
      "glasso averge testing error (all):  NA 96.68083 1.008235 \n",
      "Number of all zero tissues in elastic net is  47 \n",
      "Number of all zero tissues in glasso is  49 \n",
      "Elastic net average testing error (non-zero):  NaN NaN NaN \n",
      "glasso averge testing error (non-zero):  NaN NaN NaN \n",
      "training a model on entire data with parameters chosen from cv\n",
      "Training error: 2306.63991985794\n"
     ]
    },
    {
     "ename": "ERROR",
     "evalue": "Error in multi_lam[f, 1]: incorrect number of dimensions\n",
     "output_type": "error",
     "traceback": [
      "Error in multi_lam[f, 1]: incorrect number of dimensions\nTraceback:\n",
      "1. glasso_no_early_stopping(X = X_all, Y = Y_all, XX = XX_all, XY = XY, \n .     Xnorm = Xnorm, lambda1 = multi_lam[f, 1]/spsz, lambda2 = multi_lam[f, \n .         2], theta = matrix(initial_numeric, M, P))"
     ]
    }
   ],
   "source": [
    "#save(single_res_test, single_lam, single_theta_est, multi_res_test, multi_lam, multi_theta_est, res_tune, rec_lamv, file = paste0(output_dir, '/', gene_id, \".cv.evaluation.RData\"))\n",
    "save(single_res_test, single_lam, single_theta_est, multi_res_test, multi_lam, multi_theta_est, res_tune, file = paste0(output_dir, '/', gene_id, \".cv.evaluation.RData\"))\n",
    "                                  \n",
    "\tres_single = avg_perm(single_res_test)\n",
    "\tres_multi = avg_perm(multi_res_test)\n",
    "\tcat(\"Elastic net average testing error (all): \", apply(res_single, 2, mean), '\\n')\n",
    "\tcat(\"glasso averge testing error (all): \", apply(res_multi, 2, mean), '\\n')\n",
    "\tcat(\"Number of all zero tissues in elastic net is \", sum(is.na(res_single[,1])), '\\n')\n",
    "\tcat(\"Number of all zero tissues in glasso is \", sum(is.na(res_multi[,1])), '\\n')\n",
    "\tcat(\"Elastic net average testing error (non-zero): \", apply(res_single[!is.na(res_multi[,1]),], 2, mean), '\\n')\n",
    "\tcat(\"glasso averge testing error (non-zero): \", apply(res_multi[!is.na(res_multi[,1]),], 2, mean), '\\n')\n",
    "\n",
    "  #------------ use tuning parameter chosen above to train model on entire dataset -------------#\n",
    "\t## generate an estimate with whole data ##\n",
    "  cat('training a model on entire data with parameters chosen from cv\\n')\n",
    "\tX_all = list()\n",
    "\tY_all = list()\n",
    "\tfor(t in 1:T_num){\n",
    "\t\tX_all_tmp = sub_id_map[[t]]\n",
    "        X_all[[t]] = apply(as.matrix(dose_std[,X_all_tmp]),1,as.numeric)\n",
    "\t\t#X_all[[t]] = apply(as.matrix(dose[X_all_tmp,-c(1)]),2,as.numeric)\n",
    "\t\tY_all[[t]] = Y[[t]][which(sub_id_map_exp[[t]] == T), 2]\n",
    "\t}\n",
    "\t# initial values \n",
    "\tsingle_initial_est = matrix(0, ncol(X_train[[1]]), T_num)\n",
    "\tfor(t in 1:T_num){\n",
    "\t\ttt = cv.glmnet(X_all[[t]], Y_all[[t]], alpha = 0.5, nfolds = 5)\n",
    "\t\tsingle_initial_est[,t] = tt$glmnet.fit$beta[,which.min(tt$cvm)]\n",
    "\t}\n",
    "\n",
    "\tsig_norm = apply(single_initial_est, 1, function(x){sqrt(sum(x^2))})\n",
    "\tsig_norm[sig_norm==0] = rep(min(sig_norm[sig_norm>0]), sum(sig_norm==0))/2\n",
    "\tsig_norm = sig_norm/sum(sig_norm)\n",
    "\tweights2 = 1/sig_norm; weights2 = weights2/sum(weights2);\n",
    "\n",
    "\ttis_norm = apply(single_initial_est, 2, function(x){sum(abs(x))})\n",
    "\ttis_norm[tis_norm==0] = rep(min(tis_norm[tis_norm>0]), sum(tis_norm==0))/2\n",
    "\ttis_norm = tis_norm/sum(tis_norm)\n",
    "\tweights1 = 1/tis_norm; weights1 = weights1/sum(weights1);\n",
    "\n",
    "\tspsz = unlist(lapply(X_all,nrow))\n",
    "\tinitial_numeric = as.numeric(single_initial_est)\n",
    "\t#remove(single_initial_est)\n",
    "\tXY = grad_prep(X_all, Y_all)\n",
    "\tXX_all = lapply(X_all, function(x){t(x)%*%x/nrow(x)})\n",
    "\ttmp_res = rep(0, fold)\n",
    "\tfor (f in 1:fold){\n",
    "\t\t#ans = glasso_no_early_stopping(X=X_all, Y=Y_all, XX=XX_all, XY=XY, Xnorm=Xnorm, lambda1=multi_lam[f,1]/spsz, lambda2=multi_lam[f,2], theta=matrix(initial_numeric,M,P))#, verbose = if_verbose)\n",
    "\t\tans = glasso_no_early_stopping(X=X_all, Y=Y_all, XX=XX_all, XY=XY, Xnorm=Xnorm, lambda1=multi_lam[f]/spsz, lambda2=multi_lam[f], theta=matrix(initial_numeric,M,P))#, verbose = if_verbose)\n",
    "        tmp_res[f] = ans$avg_train_err\n",
    "\t}\n",
    "\tfinal.lam = multi_lam[which.min(tmp_res),]\n",
    "\tans = glasso_no_early_stopping(X=X_all, Y=Y_all, XX=XX_all, XY=XY, Xnorm=Xnorm, lambda1=final.lam[1]/spsz, lambda2=final.lam[2], theta=matrix(initial_numeric,M,P))#, verbose = if_verbose)\n",
    "\tinfo = read.table(info_path, header=T, sep='\\t')\n",
    "    downstream_est = data.frame(info[,1:3], ans$est)\n",
    "\tmulti_all_res = multi_mse(ans$est, X_all, Y_all)\n",
    "\tsingle_all_res = multi_mse(single_initial_est, X_all, Y_all)\n",
    "  cat('writing final estimates\\n')\n",
    "\twrite.table(downstream_est, paste0(output_dir, '/', gene_id, \".est\"), quote = F, row.names = F, col.names = c(\"SNP\", \"REF.0.\", \"ALT.1.\", Yt))\n",
    "\n",
    "  cat('saving the prediction on all data for future analysis\\n')\n",
    "\tsave(multi_all_res, single_all_res, final.lam, ans, file = paste0(output_dir, '/', gene_id, \".prediction_on_all_data.RData\"))\n",
    "  cat('done!\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
